# gridbot_websocket_server.py
# Central hub for communication, data processing, and command execution
# Updates: Aligns with long_term_ml.py's 14-feature trade format, supports group-specific optimization,
# enhances wscat command translation, adds detailed logging, and optimizes database operations
# Fixes: Renames 'group' column to '_group' to avoid SQLite reserved keyword, enhances migration error handling

import os
import json
import logging
import time
import uuid
import sqlite3
from websocket_server import WebsocketServer
from collections import deque, defaultdict
from datetime import datetime, timezone
import hashlib
import config

# Use absolute path for log file
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
SERVER_LOG_FILE = os.path.join(BASE_DIR, 'gridbot_websocket_server.log')

# Configure logging
try:
    logging.basicConfig(
        level=getattr(logging, config.LOG_LEVEL),
        format="%(asctime)s - %(levelname)s - [%(module)s:%(funcName)s:%(lineno)d] - %(message)s",
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler(SERVER_LOG_FILE, mode='a')
        ],
        force=True
    )
    logger = logging.getLogger(__name__)
    logger.info(f"Logging initialized to {SERVER_LOG_FILE}")
except Exception as e:
    print(f"Failed to initialize logging: {e}")
    raise

# Database for persistent storage
DB_FILE = os.path.join(BASE_DIR, 'gridbot_websocket_server.db')

def initialize_database():
    """Initialize SQLite database with all required tables and indexes."""
    try:
        start_time = time.time()
        conn = sqlite3.connect(DB_FILE)
        c = conn.cursor()
        # Clients table
        c.execute('''CREATE TABLE IF NOT EXISTS clients
                     (client_id TEXT PRIMARY KEY, session_id TEXT, last_seen INTEGER, registered_at INTEGER)''')
        c.execute('''CREATE INDEX IF NOT EXISTS idx_clients_last_seen ON clients(last_seen)''')
        # Messages table
        c.execute('''CREATE TABLE IF NOT EXISTS messages
                     (id INTEGER PRIMARY KEY AUTOINCREMENT, timestamp INTEGER, client_id TEXT, message TEXT, direction TEXT)''')
        c.execute('''CREATE INDEX IF NOT EXISTS idx_messages_timestamp ON messages(timestamp)''')
        # Trades table with 14-feature alignment
        c.execute('''CREATE TABLE IF NOT EXISTS trades
                     (id INTEGER PRIMARY KEY AUTOINCREMENT, timestamp INTEGER, client_id TEXT, type TEXT, order_id TEXT,
                      status TEXT, side TEXT, price REAL, volume REAL, rsi REAL, ema REAL, volatility REAL, macd REAL,
                      macd_signal REAL, bollinger_upper REAL, bollinger_lower REAL, momentum REAL, volume_trend REAL,
                      predicted_price REAL, grid_level INTEGER, source TEXT)''')
        c.execute('''CREATE INDEX IF NOT EXISTS idx_trades_timestamp ON trades(timestamp)''')
        # Parameters table with group-specific fields
        c.execute('''CREATE TABLE IF NOT EXISTS parameters
                     (id INTEGER PRIMARY KEY AUTOINCREMENT, timestamp INTEGER, _group TEXT,
                      grid_size REAL, position_size REAL, max_order_range REAL, stagnation_timeout INTEGER,
                      num_buy_grid_lines INTEGER, num_sell_grid_lines INTEGER, lookback INTEGER,
                      ml_trend_weight REAL, ml_confidence_threshold REAL, pytorch_learning_rate REAL,
                      pytorch_dropout REAL, pytorch_batch_size INTEGER, sklearn_n_estimators INTEGER,
                      pytorch_hidden_size INTEGER, pytorch_num_epochs INTEGER, volatility_window INTEGER,
                      websocket_ping_interval INTEGER)''')
        c.execute('''CREATE INDEX IF NOT EXISTS idx_parameters_timestamp ON parameters(timestamp)''')
        # Predictions table
        c.execute('''CREATE TABLE IF NOT EXISTS predictions
                     (id INTEGER PRIMARY KEY AUTOINCREMENT, timestamp INTEGER, client_id TEXT, sklearn_predicted REAL,
                      pytorch_predicted REAL, mse REAL, confidence REAL)''')
        c.execute('''CREATE INDEX IF NOT EXISTS idx_predictions_timestamp ON predictions(timestamp)''')
        # PL updates table
        c.execute('''CREATE TABLE IF NOT EXISTS pl_updates
                     (id INTEGER PRIMARY KEY AUTOINCREMENT, timestamp INTEGER, client_id TEXT, total_pl REAL)''')
        c.execute('''CREATE INDEX IF NOT EXISTS idx_pl_updates_timestamp ON pl_updates(timestamp)''')
        conn.commit()
        elapsed = time.time() - start_time
        logger.info(f"Initialized database with all tables and indexes: {DB_FILE}, Time={elapsed:.2f}s")
    except Exception as e:
        logger.error(f"Failed to initialize database: {e}")
        raise
    finally:
        conn.close()

# Optimized code
import sqlite3
from typing import List, Tuple
DB_FILE = 'database.db'
# Optimized code
import sqlite3
from typing import List
def migrate_database() -> None:
    """Migrate existing database to ensure all required tables and columns exist."""
    try:
        start_time = time.time()
        # Establish a connection to the database
        conn = sqlite3.connect(DB_FILE)
        c = conn.cursor()
        # Ensure all tables exist
        tables = ['clients', 'messages', 'trades', 'parameters', 'predictions', 'pl_updates']
        existing_tables = 0
        for table in tables:
            c.execute(f"SELECT name FROM sqlite_master WHERE type='table' AND name='{table}'")
            if c.fetchone():
                existing_tables += 1
            else:
                logger.info(f"Creating missing table: {table}")
                initialize_database()  # Re-run initialization
                break
        # Migrate trades table
        trade_columns = [('volatility', 'REAL', '0.0'),
                         ('predicted_price', 'REAL', '0.0'),
                         ('grid_level', 'INTEGER', '0'),
                         ('source', 'TEXT', "'unknown'")]
        trade_migrations = 0
        for col_name, col_type, default in trade_columns:
            if col_name not in ['volatility', 'predicted_price', 'grid_level']:
                try:
                    c.execute(f"ALTER TABLE trades ADD COLUMN {col_name} {col_type} DEFAULT {default}")
                    logger.info(f"Added {col_name} column to trades table")
                    trade_migrations += 1
                except Exception as e:
                    logger.error(f"Failed to add {col_name} to trades table: {e}")
        # Migrate parameters table
        param_columns = [('_group', 'TEXT', "'unknown'"),
                         ('ml_trend_weight', 'REAL', str(config.ML_TREND_WEIGHT)),
                         ('ml_confidence_threshold', 'REAL', str(config.ML_CONFIDENCE_THRESHOLD)),
                         ('pytorch_learning_rate', 'REAL', str(config.PYTORCH_LEARNING_RATE)),
                         ('pytorch_dropout', 'REAL', str(config.PYTORCH_DROPOUT)),
                         ('pytorch_batch_size', 'INTEGER', str(config.PYTORCH_BATCH_SIZE)),
                         ('sklearn_n_estimators', 'INTEGER', str(config.SKLEARN_N_ESTIMATORS)),
                         ('pytorch_hidden_size', 'INTEGER', str(config.PYTORCH_HIDDEN_SIZE)),
                         ('pytorch_num_epochs', 'INTEGER', str(config.PYTORCH_NUM_EPOCHS)),
                         ('volatility_window', 'INTEGER', str(config.VOLATILITY_WINDOW)),
                         ('websocket_ping_interval', 'INTEGER', str(config.WEBSOCKET_PING_INTERVAL))]
        param_migrations = 0
        for col_name, col_type, default in param_columns:
            if col_name not in ['ml_trend_weight', 'ml_confidence_threshold',
                                 'pytorch_learning_rate', 'pytorch_dropout',
                                 'pytorch_batch_size', 'sklearn_n_estimators']:
                try:
                    c.execute(f"ALTER TABLE parameters ADD COLUMN {col_name} {col_type} DEFAULT {default}")
                    logger.info(f"Added {col_name} column to parameters table")
                    param_migrations += 1
                except Exception as e:
                    logger.error(f"Failed to add {col_name} to parameters table: {e}")
        # Migrate predictions table
        pred_columns = [('confidence', 'REAL', '0.5')]
        pred_migrations = 0
        if 'confidence' not in ['volatility', 'predicted_price']:
            try:
                c.execute("ALTER TABLE predictions ADD COLUMN confidence REAL DEFAULT 0.5")
                logger.info("Added confidence column to predictions table")
                pred_migrations += 1
            except Exception as e:
                logger.error(f"Failed to add confidence to predictions table: {e}")
        conn.commit()
        elapsed = time.time() - start_time
        logger.info(f"Database migration completed: {existing_tables}/{len(tables)} tables existed, "
                    f"{trade_migrations}/{len(trade_columns)} trade columns added, "
                    f"{param_migrations}/{len(param_columns)} parameter columns added, "
                    f"{pred_migrations}/1 prediction column added, Time={elapsed:.2f}s")
    except Exception as e:
        logger.error(f"Failed to migrate database: {e}")
        raise
    finally:
        conn.close()

def store_client(client_id, session_id):
    """Store or update client identity in the database."""
    try:
        start_time = time.time()
        conn = sqlite3.connect(DB_FILE)
        c = conn.cursor()
        timestamp = int(time.time() * 1000)
        c.execute("INSERT OR REPLACE INTO clients (client_id, session_id, last_seen, registered_at) VALUES (?, ?, ?, ?)",
                  (client_id, session_id, timestamp, timestamp))
        conn.commit()
        elapsed = time.time() - start_time
        logger.debug(f"Stored client: client_id={client_id}, session_id={session_id}, Time={elapsed:.2f}s")
    except Exception as e:
        logger.error(f"Failed to store client {client_id}: {e}")
    finally:
        conn.close()

def get_client(client_id):
    """Retrieve client details from the database."""
    try:
        start_time = time.time()
        conn = sqlite3.connect(DB_FILE)
        c = conn.cursor()
        c.execute("SELECT session_id, last_seen, registered_at FROM clients WHERE client_id = ?", (client_id,))
        result = c.fetchone()
        elapsed = time.time() - start_time
        if result:
            logger.debug(f"Retrieved client {client_id}: session_id={result[0]}, Time={elapsed:.2f}s")
            return {"session_id": result[0], "last_seen": result[1], "registered_at": result[2]}
        logger.debug(f"No client found for {client_id}, Time={elapsed:.2f}s")
        return None
    except Exception as e:
        logger.error(f"Failed to retrieve client {client_id}: {e}")
        return None
    finally:
        conn.close()

def log_message(client_id, message, direction):
    """Log a message to the database."""
    try:
        start_time = time.time()
        conn = sqlite3.connect(DB_FILE)
        c = conn.cursor()
        timestamp = int(time.time() * 1000)
        c.execute("INSERT INTO messages (timestamp, client_id, message, direction) VALUES (?, ?, ?, ?)",
                  (timestamp, client_id, str(message), direction))
        conn.commit()
        elapsed = time.time() - start_time
        logger.debug(f"Logged {direction} message for client_id={client_id}: {message}, Time={elapsed:.2f}s")
    except Exception as e:
        logger.error(f"Failed to log message for client_id={client_id}: {e}")
    finally:
        conn.close()

def store_trade(client_id, trade_data):
    """Store trade or order data in the database."""
    try:
        start_time = time.time()
        conn = sqlite3.connect(DB_FILE)
        c = conn.cursor()
        timestamp = trade_data.get('timestamp', int(time.time() * 1000))
        c.execute('''INSERT INTO trades (timestamp, client_id, type, order_id, status, side, price, volume, rsi, ema,
                     volatility, macd, macd_signal, bollinger_upper, bollinger_lower, momentum, volume_trend,
                     predicted_price, grid_level, source)
                     VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)''',
                  (timestamp, client_id, trade_data.get('type', 'unknown'), trade_data.get('id', ''),
                   trade_data.get('status', 'unknown'), trade_data.get('side', 'unknown'),
                   float(trade_data.get('price', 0.0)), float(trade_data.get('volume', 0.0)),
                   float(trade_data.get('rsi', 50.0)), float(trade_data.get('ema', trade_data.get('price', 0.0))),
                   float(trade_data.get('volatility', 0.0)), float(trade_data.get('macd', 0.0)),
                   float(trade_data.get('macd_signal', 0.0)), float(trade_data.get('bollinger_upper', trade_data.get('price', 0.0))),
                   float(trade_data.get('bollinger_lower', trade_data.get('price', 0.0))),
                   float(trade_data.get('momentum', 0.0)), float(trade_data.get('volume_trend', 0.0)),
                   float(trade_data.get('predicted_price', trade_data.get('price', 0.0))),
                   int(trade_data.get('grid_level', 0)), trade_data.get('source', 'unknown')))
        conn.commit()
        elapsed = time.time() - start_time
        logger.debug(f"Stored trade/order: client_id={client_id}, id={trade_data.get('id')}, type={trade_data.get('type')}, Time={elapsed:.2f}s")
    except Exception as e:
        logger.error(f"Failed to store trade for client_id={client_id}: {e}")
    finally:
        conn.close()

def store_parameters(params, group):
    """Store optimized parameters in the database with group metadata."""
    try:
        start_time = time.time()
        conn = sqlite3.connect(DB_FILE)
        c = conn.cursor()
        timestamp = int(time.time() * 1000)
        c.execute('''INSERT INTO parameters (timestamp, _group, grid_size, position_size, max_order_range, stagnation_timeout,
                     num_buy_grid_lines, num_sell_grid_lines, lookback, ml_trend_weight, ml_confidence_threshold,
                     pytorch_learning_rate, pytorch_dropout, pytorch_batch_size, sklearn_n_estimators,
                     pytorch_hidden_size, pytorch_num_epochs, volatility_window, websocket_ping_interval)
                     VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)''',
                  (timestamp, group,
                   params.get('grid_size', config.GRID_SIZE),
                   params.get('position_size', config.POSITION_SIZE),
                   params.get('max_order_range', config.MAX_ORDER_RANGE),
                   params.get('stagnation_timeout', config.STAGNATION_TIMEOUT),
                   params.get('num_buy_grid_lines', config.NUM_BUY_GRID_LINES),
                   params.get('num_sell_grid_lines', config.NUM_SELL_GRID_LINES),
                   params.get('lookback', config.LOOKBACK),
                   params.get('ml_trend_weight', config.ML_TREND_WEIGHT),
                   params.get('ml_confidence_threshold', config.ML_CONFIDENCE_THRESHOLD),
                   params.get('pytorch_learning_rate', config.PYTORCH_LEARNING_RATE),
                   params.get('pytorch_dropout', config.PYTORCH_DROPOUT),
                   params.get('pytorch_batch_size', config.PYTORCH_BATCH_SIZE),
                   params.get('sklearn_n_estimators', config.SKLEARN_N_ESTIMATORS),
                   params.get('pytorch_hidden_size', config.PYTORCH_HIDDEN_SIZE),
                   params.get('pytorch_num_epochs', config.PYTORCH_NUM_EPOCHS),
                   params.get('volatility_window', config.VOLATILITY_WINDOW),
                   params.get('websocket_ping_interval', config.WEBSOCKET_PING_INTERVAL)))
        conn.commit()
        elapsed = time.time() - start_time
        logger.debug(f"Stored parameters for _group={group}: {params}, Time={elapsed:.2f}s")
    except Exception as e:
        logger.error(f"Failed to store parameters for _group={group}: {e}")
    finally:
        conn.close()

def store_predictions(client_id, prediction_data):
    """Store prediction data in the database."""
    try:
        start_time = time.time()
        conn = sqlite3.connect(DB_FILE)
        c = conn.cursor()
        timestamp = prediction_data.get('timestamp', int(time.time() * 1000))
        mse = prediction_data.get('mse', 0.0)
        confidence = prediction_data.get('confidence', 0.5)
        c.execute('''INSERT INTO predictions (timestamp, client_id, sklearn_predicted, pytorch_predicted, mse, confidence)
                     VALUES (?, ?, ?, ?, ?, ?)''',
                  (timestamp, client_id, float(prediction_data.get('sklearn_predicted', 0.0)),
                   float(prediction_data.get('pytorch_predicted', 0.0)), float(mse), float(confidence)))
        conn.commit()
        elapsed = time.time() - start_time
        logger.debug(f"Stored predictions: client_id={client_id}, timestamp={timestamp}, mse={mse}, confidence={confidence}, Time={elapsed:.2f}s")
    except Exception as e:
        logger.error(f"Failed to store predictions for client_id={client_id}: {e}")
    finally:
        conn.close()

def store_pl_update(client_id, pl_data):
    """Store profit/loss update in the database."""
    try:
        start_time = time.time()
        conn = sqlite3.connect(DB_FILE)
        c = conn.cursor()
        timestamp = pl_data.get('timestamp', int(time.time() * 1000))
        c.execute('''INSERT INTO pl_updates (timestamp, client_id, total_pl)
                     VALUES (?, ?, ?)''',
                  (timestamp, client_id, float(pl_data.get('total_pl', 0.0))))
        conn.commit()
        elapsed = time.time() - start_time
        logger.debug(f"Stored pl_update: client_id={client_id}, total_pl={pl_data.get('total_pl')}, Time={elapsed:.2f}s")
    except Exception as e:
        logger.error(f"Failed to store pl_update for client_id={client_id}: {e}")
    finally:
        conn.close()

import sqlite3
from typing import Dict
def get_latest_parameters(group=None):
    """Retrieve the latest optimized parameters from the database by group."""
    try:
        start_time = time.time()
        conn = sqlite3.connect(DB_FILE)
        c = conn.cursor()
        if group:
            query = '''SELECT grid_size, position_size, max_order_range, stagnation_timeout, num_buy_grid_lines,
                         num_sell_grid_lines, lookback, ml_trend_weight, ml_confidence_threshold, pytorch_learning_rate,
                         pytorch_dropout, pytorch_batch_size, sklearn_n_estimators, pytorch_hidden_size,
                         pytorch_num_epochs, volatility_window, websocket_ping_interval
                     FROM parameters WHERE _group = ? ORDER BY timestamp DESC LIMIT 1'''
            c.execute(query, (group,))
        else:
            query = '''SELECT grid_size, position_size, max_order_range, stagnation_timeout, num_buy_grid_lines,
                         num_sell_grid_lines, lookback, ml_trend_weight, ml_confidence_threshold, pytorch_learning_rate,
                         pytorch_dropout, pytorch_batch_size, sklearn_n_estimators, pytorch_hidden_size,
                         pytorch_num_epochs, volatility_window, websocket_ping_interval
                     FROM parameters ORDER BY timestamp DESC LIMIT 1'''
            c.execute(query)
        result = c.fetchone()
        elapsed = time.time() - start_time
        if result:
            params = {
                "grid_size": result[0],
                "position_size": result[1],
                "max_order_range": result[2],
                "stagnation_timeout": result[3],
                "num_buy_grid_lines": result[4],
                "num_sell_grid_lines": result[5],
                "lookback": result[6],
                "ml_trend_weight": result[7],
                "ml_confidence_threshold": result[8],
                "pytorch_learning_rate": result[9],
                "pytorch_dropout": result[10],
                "pytorch_batch_size": result[11],
                "sklearn_n_estimators": result[12],
                "pytorch_hidden_size": result[13],
                "pytorch_num_epochs": result[14],
                "volatility_window": result[15],
                "websocket_ping_interval": result[16]
            }
            logger.debug(f"Retrieved latest parameters for _group={group}: {params}, Time={elapsed:.2f}s")
            return params
        else:
            defaults = {
                "grid_size": config.GRID_SIZE,
                "position_size": config.POSITION_SIZE,
                "max_order_range": config.MAX_ORDER_RANGE,
                "stagnation_timeout": config.STAGNATION_TIMEOUT,
                "num_buy_grid_lines": config.NUM_BUY_GRID_LINES,
                "num_sell_grid_lines": config.NUM_SELL_GRID_LINES,
                "lookback": config.LOOKBACK,
                "ml_trend_weight": config.ML_TREND_WEIGHT,
                "ml_confidence_threshold": config.ML_CONFIDENCE_THRESHOLD,
                "pytorch_learning_rate": config.PYTORCH_LEARNING_RATE,
                "pytorch_dropout": config.PYTORCH_DROPOUT,
                "pytorch_batch_size": config.PYTORCH_BATCH_SIZE,
                "sklearn_n_estimators": config.SKLEARN_N_ESTIMATORS,
                "pytorch_hidden_size": config.PYTORCH_HIDDEN_SIZE,
                "pytorch_num_epochs": config.PYTORCH_NUM_EPOCHS,
                "volatility_window": config.VOLATILITY_WINDOW,
                "websocket_ping_interval": config.WEBSOCKET_PING_INTERVAL
            }
            logger.debug(f"No parameters found for _group={group}, returning defaults: {defaults}, Time={elapsed:.2f}s")
            return defaults
    except Exception as e:
        logger.error(f"Failed to retrieve parameters for _group={group}: {e}")
        return {
            "grid_size": config.GRID_SIZE,
            "position_size": config.POSITION_SIZE,
            "max_order_range": config.MAX_ORDER_RANGE,
            "stagnation_timeout": config.STAGNATION_TIMEOUT,
            "num_buy_grid_lines": config.NUM_BUY_GRID_LINES,
            "num_sell_grid_lines": config.NUM_SELL_GRID_LINES,
            "lookback": config.LOOKBACK,
            "ml_trend_weight": config.ML_TREND_WEIGHT,
            "ml_confidence_threshold": config.ML_CONFIDENCE_THRESHOLD,
            "pytorch_learning_rate": config.PYTORCH_LEARNING_RATE,
            "pytorch_dropout": config.PYTORCH_DROPOUT,
            "pytorch_batch_size": config.PYTORCH_BATCH_SIZE,
            "sklearn_n_estimators": config.SKLEARN_N_ESTIMATORS,
            "pytorch_hidden_size": config.PYTORCH_HIDDEN_SIZE,
            "pytorch_num_epochs": config.PYTORCH_NUM_EPOCHS,
            "volatility_window": config.VOLATILITY_WINDOW,
            "websocket_ping_interval": config.WEBSOCKET_PING_INTERVAL
        }
    finally:
        conn.close()

import time
from datetime import datetime, timedelta
import logging
def translate_message_for_ml(message):
    start_time = time.time()
    msg_type = message.get('type')
    timestamp = message.get('timestamp', int(time.time() * 1000))
    if isinstance(timestamp, int):
        timestamp = datetime.fromtimestamp(timestamp / 1000, tz=timezone.utc).isoformat()
    required_features = ['price', 'volume', 'trades', 'rsi', 'ema', 'volatility', 'macd', 'macd_signal',
                        'bollinger_upper', 'bollinger_lower', 'momentum', 'volume_trend', 'predicted_price', 'grid_level']
    if not all(feature in message for feature in required_features):
        logger.warning(f"Incomplete trade features for ML: {message}")
        return None
    elapsed = time.time() - start_time
    logger.debug(f"Translated grid_trade message for ML: {message}, Time={elapsed:.2f}s")
    return {'timestamp': timestamp, 'price': float(message['price']), 'volume': float(message['volume']),
            'trades': int(message['trades']), 'rsi': float(message['rsi']), 'ema': float(message['ema']),
            'volatility': float(message['volatility']), 'macd': float(message['macd']), 'macd_signal': float(message['macd_signal']),
            'bollinger_upper': float(message['bollinger_upper']), 'bollinger_lower': float(message['bollinger_lower']),
            'momentum': float(message['momentum']), 'volume_trend': float(message['volume_trend']),
            'predicted_price': float(message['predicted_price']), 'grid_level': int(message['grid_level'])}

class WebSocketServerManager:
    def __init__(self, port=9001):
        try:
            self.server = WebsocketServer(port=port)
            self.clients = {}  # Maps client IDs to client objects
            self.client_sessions = {}  # Maps client IDs to session IDs
            self.message_queue = deque()  # Queue for Long Term ML messages
            self.recent_updates = defaultdict(list)  # Store recent config_update messages
            self.last_update_time = {}  # Track last config_update timestamp per client and group
            self.server.set_fn_new_client(self.new_client)
            self.server.set_fn_message_received(self.message_received)
            self.server.set_fn_client_left(self.client_left)
            if os.path.exists(DB_FILE):
                migrate_database()  # Migrate existing database
            initialize_database()  # Ensure all tables exist
            logger.info(f"Initialized WebSocket server on port {port}")
        except Exception as e:
            logger.error(f"Failed to initialize WebSocket server: {e}")
            raise

    def request_optimization(self, client_id, group=None):
        """Send an optimization request to Long Term ML."""
        try:
            start_time = time.time()
            client_target = self.clients.get("Long Term ML")
            if client_target:
                request_id = str(uuid.uuid4())
                opt_request = {
                    "command": "optimize",
                    "request_id": request_id,
                    "reason": f"Periodic optimization request for _group {group}" if group else "Periodic optimization request",
                    "_group": group if group else "unknown",
                    "timestamp": int(time.time() * 1000)
                }
                self.server.send_message(client_target, json.dumps(opt_request))
                log_message("Long Term ML", opt_request, "out")
                elapsed = time.time() - start_time
                logger.info(f"Sent optimization request to Long Term ML: ID={request_id}, _Group={group}, Time={elapsed:.2f}s")
            else:
                logger.warning("Long Term ML not connected, cannot request optimization")
        except Exception as e:
            logger.error(f"Failed to request optimization for _group={group}: {e}")

    def new_client(self, client, server):
        """Handle new client connections."""
        start_time = time.time()
        session_id = str(uuid.uuid4())
        client['session_id'] = session_id
        client['identified'] = False
        logger.info(f"New client connected: Session ID {session_id}")
        identify_msg = {
            "action": "identify",
            "session_id": session_id,
            "timestamp": int(time.time() * 1000)
        }
        server.send_message(client, json.dumps(identify_msg))
        log_message("unknown", identify_msg, "out")
        elapsed = time.time() - start_time
        logger.debug(f"Sent identification request to client {session_id}, Time={elapsed:.2f}s")

    def client_left(self, client, server):
        """Handle client disconnection."""
        start_time = time.time()
        session_id = client.get('session_id', 'unknown')
        client_id = client.get('client_id', 'unknown')
        if client_id in self.clients:
            del self.clients[client_id]
            del self.client_sessions[client_id]
            self.last_update_time.pop(client_id, None)
            self.recent_updates.pop(client_id, None)
            logger.info(f"Client disconnected: ID={client_id}, Session ID={session_id}")
        else:
            logger.info(f"Unknown client disconnected: Session ID={session_id}")
        elapsed = time.time() - start_time
        logger.debug(f"Current clients: {list(self.clients.keys())}, Time={elapsed:.2f}s")

    def flush_queue(self, client, server):
        """Send queued messages to Long Term ML."""
        start_time = time.time()
        while self.message_queue:
            message = self.message_queue.popleft()
            server.send_message(client, json.dumps(message))
            log_message("Long Term ML", message, "out")
            logger.debug(f"Flushed queued message to Long Term ML: {message}")
        elapsed = time.time() - start_time
        logger.info(f"Flushed message queue to Long Term ML, Time={elapsed:.2f}s")

    def message_received(self, client, server, message):
        start_time = time.time()
        session_id = client.get('session_id', 'unknown')
        client_id = client.get('client_id', 'unknown')
        logger.debug(f"Received message from client {session_id} (ID={client_id}, Identified={client.get('identified', False)}): {message}")
        try:
            data = json.loads(message)

            # Handle identification
            if isinstance(data, dict) and data.get("action") == "identify":
                received_client_id = data.get("client_id")
                if not received_client_id:
                    logger.error(f"Missing client_id from client {session_id}")
                    server.send_message(client, json.dumps({
                        "action": "identify",
                        "status": "error",
                        "reason": "Missing client_id"
                    }))
                    log_message(client_id, {"action": "identify", "status": "error", "reason": "Missing client_id"}, "out")
                    return

                received_session_id = data.get("session_id", session_id)
                valid_session_ids = ["Client", "Long Term ML", session_id]
                if not received_session_id or received_session_id in ["None", ""]:
                    logger.warning(f"Invalid session_id from client {received_client_id}, using server-assigned {session_id}")
                    received_session_id = session_id
                elif received_session_id not in valid_session_ids:
                    logger.warning(f"Received session_id {received_session_id} not in valid list, using server-assigned {session_id}")
                    received_session_id = session_id

                if client.get('identified', False) and self.client_sessions.get(received_client_id) == received_session_id:
                    logger.debug(f"Ignoring redundant identification request from client {received_client_id}, session {received_session_id}")
                    server.send_message(client, json.dumps({
                        "action": "identify",
                        "status": "success",
                        "session_id": received_session_id,
                        "timestamp": int(time.time() * 1000)
                    }))
                    return

                store_client(received_client_id, received_session_id)
                logger.info(f"Client identified: ID={received_client_id}, Session ID={received_session_id}")

                if received_client_id in self.clients and self.client_sessions.get(received_client_id) != received_session_id:
                    logger.warning(f"Client {received_client_id} reconnected with new session {received_session_id}")
                    self.clients[received_client_id] = client
                    self.client_sessions[received_client_id] = received_session_id
                self.clients[received_client_id] = client
                self.client_sessions[received_client_id] = received_session_id
                client['client_id'] = received_client_id
                client['identified'] = True

                server.send_message(client, json.dumps({
                    "action": "identify",
                    "status": "success",
                    "session_id": received_session_id,
                    "timestamp": int(time.time() * 1000)
                }))
                log_message(received_client_id, {"action": "identify", "status": "success", "session_id": received_session_id}, "out")

                # Flush queued messages if Long Term ML or Wscat
                if received_client_id in ["Long Term ML", "Wscat"]:
                    self.flush_queue(client, server)

                elapsed = time.time() - start_time
                logger.debug(f"Processed identification for client {received_client_id}, Current clients: {list(self.clients.keys())}, Time={elapsed:.2f}s")
                return

            if not client.get('identified', False):
                logger.warning(f"Message from unidentified client {session_id}: {message}")
                server.send_message(client, json.dumps({
                    "action": "identify",
                    "status": "error",
                    "reason": "Client not identified"
                }))
                log_message(client_id, {"action": "identify", "status": "error", "reason": "Client not identified"}, "out")
                return

            # Log message type for debugging
            msg_type = data.get('type') if isinstance(data, dict) else 'list'
            logger.debug(f"Processing message type={msg_type} from client {client_id}")

            # Handle list messages
            if isinstance(data, list):
                for item in data:
                    if not isinstance(item, dict):
                        logger.warning(f"Invalid item in list from client {client_id}: {item}")
                        continue
                    item['client_id'] = client_id
                    self.process_message_item(client_id, item, server)
                elapsed = time.time() - start_time
                logger.info(f"Processed list message with {len(data)} items from client {client_id}, Time={elapsed:.2f}s")
                return

            # Handle dictionary messages
            if isinstance(data, dict):
                data['client_id'] = client_id
                self.process_message_item(client_id, data, server)
            else:
                logger.warning(f"Invalid message format from client {client_id}: {message}")

            elapsed = time.time() - start_time
            logger.info(f"Processed message from client {client_id}, Time={elapsed:.2f}s")

        except json.JSONDecodeError as e:
            logger.error(f"JSON decode error from client {session_id}: {e}, Message: {message}")
        except Exception as e:
            logger.error(f"Error processing message from client {session_id}: {e}")

    def process_message_item(self, client_id, item, server):
        """Process individual message item."""
        start_time = time.time()
        msg_type = item.get("type")
        command = item.get("command")
        request_id = item.get("request_id", "unknown")
        group = item.get("group", "unknown")

        # Handle commands
        if command:
            if command == "optimize":
                if not request_id:
                    logger.warning(f"Missing request_id in optimize command from client {client_id}")
                    server.send_message(self.clients[client_id], json.dumps({
                        "command": "optimize",
                        "status": "error",
                        "reason": "Missing request_id"
                    }))
                    log_message(client_id, {"command": "optimize", "status": "error", "reason": "Missing request_id"}, "out")
                    return
                if client_id in ["Long Term ML", "Wscat"]:  # Allow Long Term ML and Wscat
                    client_target = self.clients.get("Client")
                    if client_target:
                        server.send_message(client_target, json.dumps(item))
                        logger.info(f"Forwarded optimize command from {client_id} to Client: ID={request_id}, _Group={group}")
                        log_message("Client", item, "out")
                        server.send_message(self.clients[client_id], json.dumps({
                            "command": "optimize",
                            "status": "received",
                            "request_id": request_id,
                            "_group": group
                        }))
                        log_message(client_id, {"command": "optimize", "status": "received", "request_id": request_id, "_group": group}, "out")
                    else:
                        logger.warning(f"Client not found for optimize command from {client_id}")
                        server.send_message(self.clients[client_id], json.dumps({
                            "command": "optimize",
                            "status": "error",
                            "reason": "Client not found",
                            "request_id": request_id
                        }))
                        log_message(client_id, {"command": "optimize", "status": "error", "reason": "Client not found"}, "out")
                else:
                    logger.debug(f"Skipping optimize command from {client_id}: not forwarding to self")
                elapsed = time.time() - start_time
                logger.info(f"Processed optimize command from {client_id}, Time={elapsed:.2f}s")
                return

            elif command == "config_update":
                if item.get("status") == "applied":
                    reason = item.get("reason", "No reason provided")
                    logger.info(f"Received config_update response from {client_id}: ID={request_id}, _Group={group}, Reason={reason}")
                    client_target = self.clients.get("Long Term ML")
                    if client_target:
                        server.send_message(client_target, json.dumps(item))
                        logger.info(f"Forwarded config_update response to Long Term ML: ID={request_id}, _Group={group}")
                        log_message("Long Term ML", item, "out")
                    elapsed = time.time() - start_time
                    logger.info(f"Processed config_update response from {client_id}, Time={elapsed:.2f}s")
                    return
                if client_id not in ["Long Term ML", "Wscat"]:
                    logger.warning(f"Unauthorized config_update command from client {client_id}")
                    server.send_message(self.clients[client_id], json.dumps({
                        "command": "config_update",
                        "status": "error",
                        "reason": "Unauthorized client",
                        "request_id": request_id,
                        "_group": group
                    }))
                    log_message(client_id, {"command": "config_update", "status": "error", "reason": "Unauthorized client"}, "out")
                    return
                params = item.get("parameters")
                if not params:
                    logger.warning(f"Missing parameters in config_update from client {client_id}")
                    server.send_message(self.clients[client_id], json.dumps({
                        "command": "config_update",
                        "status": "error",
                        "reason": "Missing parameters",
                        "request_id": request_id,
                        "_group": group
                    }))
                    log_message(client_id, {"command": "config_update", "status": "error", "reason": "Missing parameters"}, "out")
                    self.request_optimization(client_id, group)
                    return

                # Group-specific cooldowns
                current_time = time.time()
                COOLDOWNS = {'grid': 10.0, 'ml': 15.0, 'strategy': 20.0}
                cooldown = COOLDOWNS.get(group, 10.0)
                client_group_key = f"{client_id}:{group}"
                last_update = self.last_update_time.get(client_group_key, 0)
                if client_id == "Wscat" or current_time - last_update >= cooldown:
                    param_hash = hashlib.sha256(json.dumps(params, sort_keys=True).encode()).hexdigest()
                    recent = self.recent_updates[client_group_key]
                    recent = [u for u in recent if current_time - u['time'] < cooldown]
                    self.recent_updates[client_group_key] = recent

                    if any(u['hash'] == param_hash for u in recent):
                        logger.debug(f"Ignoring duplicate config_update: ID={request_id}, _Group={group}, Parameters={params}")
                        return

                    self.recent_updates[client_group_key].append({
                        'hash': param_hash,
                        'time': current_time,
                        'parameters': params
                    })

                    store_parameters(params, group)
                    self.last_update_time[client_group_key] = current_time

                    client_target = self.clients.get("Client")
                    if client_target:
                        config_update = {
                            "command": "config_update",
                            "request_id": request_id,
                            "parameters": params,
                            "reason": item.get("reason", "Parameter update from ML"),
                            "_group": group,
                            "timestamp": int(time.time() * 1000)
                        }
                        server.send_message(client_target, json.dumps(config_update))
                        logger.info(f"Forwarded config_update to Client: ID={request_id}, _Group={group}, Parameters={params}, Reason={config_update['reason']}")
                        log_message("Client", config_update, "out")
                        server.send_message(self.clients[client_id], json.dumps({
                            "command": "config_update",
                            "status": "success",
                            "request_id": request_id,
                            "_group": group,
                            "timestamp": int(time.time() * 1000)
                        }))
                        log_message(client_id, {"command": "config_update", "status": "success", "request_id": request_id, "_group": group}, "out")
                    else:
                        logger.warning(f"Client not found for config_update from {client_id}")
                        server.send_message(self.clients[client_id], json.dumps({
                            "command": "config_update",
                            "status": "error",
                            "reason": "Client not found",
                            "request_id": request_id,
                            "_group": group
                        }))
                        log_message(client_id, {"command": "config_update", "status": "error", "reason": "Client not found"}, "out")
                else:
                    logger.debug(f"Skipping config_update due to cooldown: ID={request_id}, _Group={group}, Time since last={current_time - last_update:.2f}s")
                elapsed = time.time() - start_time
                logger.info(f"Processed config_update from {client_id} for _group={group}, Time={elapsed:.2f}s")
                return

            elif command in ["status", "pause", "resume"] and client_id in ["Long Term ML", "Wscat"]:
                client_target = self.clients.get("Client")
                if client_target:
                    server.send_message(client_target, json.dumps(item))
                    logger.info(f"Forwarded {command} command from {client_id} to Client")
                    log_message("Client", item, "out")
                    server.send_message(self.clients[client_id], json.dumps({
                        "command": command,
                        "status": "received",
                        "request_id": request_id,
                        "_group": group
                    }))
                    log_message(client_id, {"command": command, "status": "received", "request_id": request_id, "_group": group}, "out")
                else:
                    logger.warning(f"Client not found for {command} command")
                elapsed = time.time() - start_time
                logger.info(f"Processed {command} command from {client_id}, Time={elapsed:.2f}s")
                return

            else:
                logger.warning(f"Unknown command from client {client_id}: {item}")
                server.send_message(self.clients[client_id], json.dumps({
                    "command": command,
                    "status": "error",
                    "reason": "Unknown command",
                    "request_id": request_id,
                    "_group": group
                }))
                log_message(client_id, {"command": command, "status": "error", "reason": "Unknown command"}, "out")
                elapsed = time.time() - start_time
                logger.info(f"Processed unknown command from {client_id}, Time={elapsed:.2f}s")
                return

        # Handle message types
        if msg_type in ["order", "predictions", "pl_update", "status", "heartbeat"]:
            if msg_type == "order" or msg_type == "trade":
                store_trade(client_id, item)
            elif msg_type == "predictions":
                store_predictions(client_id, item)
            elif msg_type == "pl_update":
                store_pl_update(client_id, item)

            translated_msg = translate_message_for_ml(item)
            if translated_msg:
                client_target = self.clients.get("Long Term ML")
                if client_target:
                    server.send_message(client_target, json.dumps(translated_msg))
                    logger.info(f"Forwarded {msg_type} message to Long Term ML: ID={translated_msg.get('request_id', 'unknown')}, _Group={group}")
                    log_message("Long Term ML", translated_msg, "out")
                else:
                    logger.warning(f"Long Term ML not found for {msg_type} message, queuing")
                    self.message_queue.appendleft(translated_msg)  # Prioritize critical messages
            elif msg_type not in ["order", "predictions", "pl_update", "heartbeat"]:
                logger.warning(f"Failed to translate {msg_type} message for Long Term ML: {item}")
        else:
            logger.warning(f"Unknown message type from client {client_id}: {item}")
        elapsed = time.time() - start_time
        logger.info(f"Processed message item type={msg_type} from {client_id}, Time={elapsed:.2f}s")

    def run(self):
        """Start the WebSocket server."""
        try:
            logger.info(f"Starting WebSocket server on port {self.server.port}")
            self.server.run_forever()
        except Exception as e:
            logger.error(f"Server runtime error: {e}")
            raise

if __name__ == "__main__":
    try:
        server_manager = WebSocketServerManager(port=9001)
        server_manager.run()
    except Exception as e:
        logger.error(f"Server initialization failed: {e}")
        raise