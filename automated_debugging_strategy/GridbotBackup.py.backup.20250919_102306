import csv
from threading import Lock
from unittest import result
import ccxt
import config
import time
import sys
import websocket
import json
import urllib3
import urllib3.util.connection as urllib3_connection
import socket
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
import torch
import torch.nn as nn
import torch.optim as optim
import logging
import threading
import backoff
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import GradientBoostingRegressor
import cmd
import queue
import collections
import xgboost as xgb
import joblib
import os
import math
from datetime import datetime, timezone
import logging
import logging.handlers
from concurrent_log_handler import ConcurrentRotatingFileHandler
from sklearn.linear_model import SGDRegressor
from uuid import uuid4
import ssl

feature_trades_csv = os.path.join(os.path.dirname(__file__), "feature_trades.csv")
csv_lock = Lock()

# --- ADDED: GridBot ML Log File ---
gridbot_ml_log = os.path.join(os.path.dirname(__file__), "gridbot_ml.log")
ml_log_lock = Lock()

# --- ADDED: Breakout State File ---
BREAKOUT_STATE_FILE = os.path.join(os.path.dirname(__file__), "breakout_state.json")
breakout_state_lock = Lock()

def write_breakout_state(state):
    """Writes the breakout state to a JSON file."""
    with breakout_state_lock:
        with open(BREAKOUT_STATE_FILE, 'w') as f:
            json.dump(state, f, indent=4)
    logger.info(f"Updated breakout state file: {state}")

# --- ADDED: Supertrend State Awareness ---
SUPERTREND_STATE_FILE = os.path.join(os.path.dirname(__file__), "supertrend_state.json")

def is_supertrend_in_position():
    """Reads the supertrend state file to determine if it's in a position."""
    try:
        if os.path.exists(SUPERTREND_STATE_FILE):
            with open(SUPERTREND_STATE_FILE, 'r') as f:
                state = json.load(f)
                in_position = state.get("in_position", False)
                logger.debug(f"Supertrend state check: in_position = {in_position}")
                return in_position
    except (FileNotFoundError, json.JSONDecodeError, Exception) as e:
        logger.warning(f"Could not read supertrend state file: {e}")
    return False

# Global trade cache and lock
trades_cache = []
trade_lock = threading.Lock()

buy_prices = {}

# Global bot state dictionary
bot_state = {
    "paused": False,
    "current_price": 0.0,
    "buy_orders": [],
    "sell_orders": [],
    "total_pl": 0.0,
    "eth_balance": 0.0,
    "usd_balance": 0.0,
    "initial_buy_price": 0.0,
    "initial_eth": 0.0,
    "initial_usd": 0.0,
    "last_reset_time": 0,
    "needs_reset": False,
    "last_sklearn_prediction": 0.0,
    "last_pytorch_prediction": 0.0,
    "last_xgb_prediction": 0.0,
    "feature_cache": pd.DataFrame(),  # NEW: Cache for real-time features
    "grid_paused": False, # ADDED: To pause grid logic during breakout
    "needs_grid_reposition": False, # ADDED: To trigger grid repositioning after breakout
    # --- ADDED: Breakout State ---
    "breakout_position": {
        "active": False,
        "side": None,       # 'buy' or 'sell'
        "entry_price": 0.0,
        "peak_price": 0.0,  # For 'buy' positions
        "trough_price": 0.0, # For 'sell' positions
        "stop_price": 0.0,
        "order_id": None,
        "size": 0.0
    },
    "last_breakout_time": 0,
    "last_retrain_time": 0,  # NEW: Track last retrain timestamp
    "recent_meta_outputs": collections.deque(maxlen=5), # For meta-model output variance check
}

ohlcv_df = pd.DataFrame()  # Global DataFrame for OHLCV data
raw_trades_for_ohlcv_df = pd.DataFrame()  # Temp storage for WebSocket trades to build OHLCV

# Global shutdown signal
shutdown_event = threading.Event()

# Enhanced logging with rotation, date/time-based filenames, and external directory
log_formatter = logging.Formatter(
    "%(asctime)s - %(levelname)s - [%(module)s:%(funcName)s:%(lineno)d] - %(message)s"
)
stream_handler = logging.StreamHandler()
stream_handler.setFormatter(log_formatter)
file_handler = ConcurrentRotatingFileHandler(
    config.LOG_FILE, maxBytes=10*1024*1024, backupCount=5
)
file_handler.setFormatter(log_formatter)
logging.basicConfig(
    level=getattr(logging, getattr(config, 'LOG_LEVEL', 'INFO')),
    handlers=[stream_handler, file_handler]
)
logger = logging.getLogger(__name__)

# Store original gai_family for restoration
original_gai_family = urllib3_connection.allowed_gai_family

def force_ipv4():
    urllib3_connection.allowed_gai_family = lambda: socket.AF_INET
    return socket.AF_INET

class BotCLI(cmd.Cmd):
    prompt = "(bot) "

    def do_status(self, arg):
        """Show current bot status, balances, open orders, and last retrain time. Usage: status"""
        logger.info("Fetching bot status")
        print(
            f"Paused: {bot_state['paused']}, Price: {bot_state['current_price']:.2f}, "
            f"Buy Orders: {len(bot_state['buy_orders'])}, Sell Orders: {len(bot_state['sell_orders'])}, "
            f"P/L: {config.TOTAL_PL:.4f}, ETH: {bot_state['eth_balance']:.6f}, USD: {bot_state['usd_balance']:.2f}, "
            f"Feature Cache: {len(bot_state['feature_cache'])} rows, "
            f"Last Retrain: {datetime.fromtimestamp(bot_state['last_retrain_time']).strftime('%Y-%m-%d %H:%M:%S') if bot_state['last_retrain_time'] else 'Never'}"
        )

    def do_pause(self, arg):
        """Pause the bot (halts trading and order placement). Usage: pause"""
        logger.info("Pausing bot via CLI")
        bot_state["paused"] = True
        print("Bot paused")

    def do_resume(self, arg):
        """Resume the bot if paused. Usage: resume"""
        logger.info("Resuming bot via CLI")
        bot_state["paused"] = False
        print("Bot resumed")

    def do_exit(self, arg):
        """Shutdown the bot and exit the CLI. Usage: exit"""
        logger.info("Initiating shutdown via CLI")
        shutdown_event.set()
        return True

    def do_reset(self, arg):
        """Reset the bot: clears orders, resets state, and reinitializes balances. Usage: reset"""
        logger.info("Resetting bot via CLI command.")
        bot_state["buy_orders"] = []
        bot_state["sell_orders"] = []
        bot_state["total_pl"] = 0.0
        bot_state["eth_balance"] = 0.0
        bot_state["usd_balance"] = 0.0
        bot_state["initial_buy_price"] = 0.0
        bot_state["initial_eth"] = 0.0
        bot_state["initial_usd"] = 0.0
        bot_state["last_reset_time"] = time.time()
        bot_state["needs_reset"] = False
        bot_state["last_sklearn_prediction"] = 0.0
        bot_state["last_pytorch_prediction"] = 0.0
        bot_state["last_xgb_prediction"] = 0.0
        bot_state["feature_cache"] = pd.DataFrame()
        bot_state["last_retrain_time"] = 0
        bot_state["recent_meta_outputs"] = collections.deque(maxlen=5)
        logger.info("Bot state reset. Orders cleared, balances and predictions reset.")
        print("Bot state has been reset. All orders cleared and state reinitialized.")

    def do_position(self, arg):
        """View or set position size. Usage: position [size]"""
        if not arg.strip():
            print(f"Current POSITION_SIZE: {getattr(config, 'POSITION_SIZE', 0.0018)}")
            return
        try:
            new_size = float(arg)
            if new_size < config.MIN_POSITION_SIZE or new_size > config.MAX_POSITION_SIZE:
                print(f"Position size must be between {config.MIN_POSITION_SIZE} and {config.MAX_POSITION_SIZE}.")
                return
            logger.info(f"Setting POSITION_SIZE from {getattr(config, 'POSITION_SIZE', 0.0018)} to {new_size}")
            config.POSITION_SIZE = new_size
            print(f"POSITION_SIZE set to {config.POSITION_SIZE}")
        except Exception as e:
            logger.error(f"Invalid position size input: {e}")
            print("Invalid position size.")

    def do_allocation(self, arg):
        """View or set ETH/USD allocation targets. Usage: allocation [eth_target] [usd_target]"""
        args = arg.strip().split()
        if not args:
            eth = getattr(config, 'ETH_TARGET', None)
            usd = getattr(config, 'USD_TARGET', None)
            print(f"ETH_TARGET: {eth}, USD_TARGET: {usd}")
            return
        if len(args) != 2:
            print("Usage: allocation [eth_target] [usd_target]")
            return
        try:
            eth_target = float(args[0])
            usd_target = float(args[1])
            if eth_target < 0 or usd_target < 0:
                print("Targets must be non-negative.")
                return
            logger.info(f"Setting ETH_TARGET to {eth_target}, USD_TARGET to {usd_target}")
            config.ETH_TARGET = eth_target
            config.USD_TARGET = usd_target
            print(f"ETH_TARGET set to {eth_target}, USD_TARGET set to {usd_target}")
        except Exception as e:
            logger.error(f"Invalid allocation input: {e}")
            print("Invalid allocation values.")

    def do_grid(self, arg):
        """View or set grid parameters. Usage: grid [param] [value]"""
        args = arg.strip().split()
        if not args:
            print(f"GRID_SIZE: {getattr(config, 'GRID_SIZE', None)}")
            print(f"NUM_BUY_GRID_LINES: {getattr(config, 'NUM_BUY_GRID_LINES', None)}")
            print(f"NUM_SELL_GRID_LINES: {getattr(config, 'NUM_SELL_GRID_LINES', None)}")
            print(f"FEATURE_ORDER_CAP: {getattr(config, 'FEATURE_ORDER_CAP', None)}")
            print(f"SOFT_FEATURE_ORDER_CAP: {getattr(config, 'SOFT_FEATURE_ORDER_CAP', None)}")
            print(f"MAX_TOTAL_ORDERS: {getattr(config, 'MAX_TOTAL_ORDERS', None)}")
            print(f"MAX_ORDER_RANGE: {getattr(config, 'MAX_ORDER_RANGE', None)}")
            print(f"MIN_ORDER_RANGE: {getattr(config, 'MIN_ORDER_RANGE', None)}")
            print(f"PRICE_DRIFT_THRESHOLD: {getattr(config, 'PRICE_DRIFT_THRESHOLD', None)}")
            return
        if len(args) != 2:
            print("Usage: grid [param] [value]")
            return
        param, value = args
        param = param.upper()
        try:
            if param in ["GRID_SIZE", "MAX_ORDER_RANGE", "MIN_ORDER_RANGE", "PRICE_DRIFT_THRESHOLD"]:
                val = float(value)
                min_val = getattr(config, f"MIN_{param}", 0)
                max_val = getattr(config, f"MAX_{param}", float('inf'))
                if val < min_val or val > max_val:
                    print(f"{param} must be between {min_val} and {max_val}.")
                    return
            elif param in ["NUM_BUY_GRID_LINES", "NUM_SELL_GRID_LINES", "FEATURE_ORDER_CAP", "SOFT_FEATURE_ORDER_CAP", "MAX_TOTAL_ORDERS"]:
                val = int(value)
                if param in ["NUM_BUY_GRID_LINES", "NUM_SELL_GRID_LINES"]:
                    min_val = config.MIN_NUM_GRID_LINES
                    max_val = config.MAX_NUM_GRID_LINES
                    if val < min_val or val > max_val:
                        print(f"{param} must be between {min_val} and {max_val}.")
                        return
            else:
                print(f"Unknown grid parameter: {param}")
                return
            logger.info(f"Setting {param} to {val}")
            setattr(config, param, val)
            print(f"{param} set to {val}")
            bot_state["needs_reset"] = True
        except Exception as e:
            logger.error(f"Invalid grid parameter input: {e}")
            print("Invalid grid parameter or value.")

    def do_grid_size(self, arg):
        """Set the grid size (with validation). Usage: grid_size [value]"""
        if not arg.strip():
            print(f"Current GRID_SIZE: {getattr(config, 'GRID_SIZE', 14.0)}")
            return
        try:
            new_size = float(arg)
            new_size = max(config.MIN_GRID_SIZE, min(config.MAX_GRID_SIZE, new_size))
            logger.info(f"Adjusting GRID_SIZE from {config.GRID_SIZE} to {new_size}")
            config.GRID_SIZE = new_size
            bot_state["needs_reset"] = True
            print(f"GRID_SIZE set to {config.GRID_SIZE}")
        except ValueError:
            logger.error("Invalid grid size input")
            print("Invalid grid size")

    def do_check_order(self, arg):
        """View or set order check parameters. Usage: check_order [param] [value]"""
        args = arg.strip().split()
        if not args:
            print(f"CHECK_ORDER_FREQUENCY: {getattr(config, 'CHECK_ORDER_FREQUENCY', None)}")
            print(f"CHECK_ORDER_STATUS: {getattr(config, 'CHECK_ORDER_STATUS', None)}")
            return
        if len(args) != 2:
            print("Usage: check_order [param] [value]")
            return
        param, value = args
        param = param.upper()
        try:
            if param == "CHECK_ORDER_FREQUENCY":
                val = float(value)
                if val < config.MIN_CHECK_FREQUENCY or val > config.MAX_CHECK_FREQUENCY:
                    print(f"CHECK_ORDER_FREQUENCY must be between {config.MIN_CHECK_FREQUENCY} and {config.MAX_CHECK_FREQUENCY}.")
                    return
                logger.info(f"Setting {param} to {val}")
                setattr(config, param, val)
                print(f"{param} set to {val}")
            elif param == "CHECK_ORDER_STATUS":
                if value.lower() not in ['open', 'closed', 'canceled']:
                    print("CHECK_ORDER_STATUS must be 'open', 'closed', or 'canceled'.")
                    return
                logger.info(f"Setting {param} to {value}")
                setattr(config, param, value)
                print(f"{param} set to {value}")
            else:
                print(f"Unknown parameter: {param}")
        except Exception as e:
            logger.error(f"Invalid check order input: {e}")
            print("Invalid check order parameter or value.")

    def do_timeout(self, arg):
        """View or set timeout parameters. Usage: timeout [param] [value]"""
        args = arg.strip().split()
        if not args:
            print(f"STAGNATION_TIMEOUT: {getattr(config, 'STAGNATION_TIMEOUT', None)}")
            print(f"MIN_RESET_INTERVAL: {getattr(config, 'MIN_RESET_INTERVAL', None)}")
            return
        if len(args) != 2:
            print("Usage: timeout [param] [value]")
            return
        param, value = args
        param = param.upper()
        try:
            if param == "STAGNATION_TIMEOUT":
                val = int(value)
                if val < config.MIN_STAGNATION_TIMEOUT or val > config.MAX_STAGNATION_TIMEOUT:
                    print(f"STAGNATION_TIMEOUT must be between {config.MIN_STAGNATION_TIMEOUT} and {config.MAX_STAGNATION_TIMEOUT}.")
                    return
            elif param == "MIN_RESET_INTERVAL":
                val = int(value)
                if val < 0:
                    print("MIN_RESET_INTERVAL must be non-negative.")
                    return
            else:
                print(f"Unknown parameter: {param}")
                return
            logger.info(f"Setting {param} to {val}")
            setattr(config, param, val)
            print(f"{param} set to {val}")
        except Exception as e:
            logger.error(f"Invalid timeout input: {e}")
            print("Invalid timeout parameter or value.")

    def do_balance(self, arg):
        """View or set balance parameters. Usage: balance [param] [value]"""
        args = arg.strip().split()
        if not args:
            print(f"INITIAL_ETH_PERCENTAGE: {getattr(config, 'INITIAL_ETH_PERCENTAGE', None)}")
            print(f"TARGET_ETH_BUFFER: {getattr(config, 'TARGET_ETH_BUFFER', None)}")
            print(f"ETH_BALANCE_DIVISOR: {getattr(config, 'ETH_BALANCE_DIVISOR', None)}")
            print(f"MIN_USD_BALANCE: {getattr(config, 'MIN_USD_BALANCE', None)}")
            return
        if len(args) != 2:
            print("Usage: balance [param] [value]")
            return
        param, value = args
        param = param.upper()
        try:
            val = float(value)
            if param == "INITIAL_ETH_PERCENTAGE":
                if val < config.MIN_INITIAL_ETH_PERCENTAGE or val > config.MAX_INITIAL_ETH_PERCENTAGE:
                    print(f"INITIAL_ETH_PERCENTAGE must be between {config.MIN_INITIAL_ETH_PERCENTAGE} and {config.MAX_INITIAL_ETH_PERCENTAGE}.")
                    return
            elif param == "TARGET_ETH_BUFFER":
                if val < config.MIN_TARGET_ETH_BUFFER or val > config.MAX_TARGET_ETH_BUFFER:
                    print(f"TARGET_ETH_BUFFER must be between {config.MIN_TARGET_ETH_BUFFER} and {config.MAX_TARGET_ETH_BUFFER}.")
                    return
            elif param == "ETH_BALANCE_DIVISOR":
                if val <= 0:
                    print("ETH_BALANCE_DIVISOR must be positive.")
                    return
            elif param == "MIN_USD_BALANCE":
                if val < 0:
                    print("MIN_USD_BALANCE must be non-negative.")
                    return
            else:
                print(f"Unknown parameter: {param}")
                return
            logger.info(f"Setting {param} to {val}")
            setattr(config, param, val)
            print(f"{param} set to {val}")
        except Exception as e:
            logger.error(f"Invalid balance input: {e}")
            print("Invalid balance parameter or value.")

    def do_replenish(self, arg):
        """View or set ETH replenishment parameters. Usage: replenish [param] [value]"""
        args = arg.strip().split()
        if not args:
            print(f"REPLENISH_ETH_THRESHOLD: {getattr(config, 'REPLENISH_ETH_THRESHOLD', None)}")
            print(f"MIN_REPLENISH_AMOUNT: {getattr(config, 'MIN_REPLENISH_AMOUNT', None)}")
            print(f"MAX_REPLENISH_AMOUNT: {getattr(config, 'MAX_REPLENISH_AMOUNT', None)}")
            return
        if len(args) != 2:
            print("Usage: replenish [param] [value]")
            return
        param, value = args
        param = param.upper()
        try:
            val = float(value)
            if param == "REPLENISH_ETH_THRESHOLD":
                if val < config.MIN_REPLENISH_THRESHOLD or val > config.MAX_REPLENISH_THRESHOLD:
                    print(f"REPLENISH_ETH_THRESHOLD must be between {config.MIN_REPLENISH_THRESHOLD} and {config.MAX_REPLENISH_THRESHOLD}.")
                    return
            elif param == "MIN_REPLENISH_AMOUNT":
                if val < 0 or val > config.MAX_REPLENISH_AMOUNT:
                    print(f"MIN_REPLENISH_AMOUNT must be non-negative and not exceed MAX_REPLENISH_AMOUNT ({config.MAX_REPLENISH_AMOUNT}).")
                    return
            elif param == "MAX_REPLENISH_AMOUNT":
                if val < config.MIN_REPLENISH_AMOUNT:
                    print(f"MAX_REPLENISH_AMOUNT must be at least MIN_REPLENISH_AMOUNT ({config.MIN_REPLENISH_AMOUNT}).")
                    return
            else:
                print(f"Unknown parameter: {param}")
                return
            logger.info(f"Setting {param} to {val}")
            setattr(config, param, val)
            print(f"{param} set to {val}")
        except Exception as e:
            logger.error(f"Invalid replenish input: {e}")
            print("Invalid replenish parameter or value.")

    def do_rebalance(self, arg):
        """View or set rebalance parameters. Usage: rebalance [param] [value]"""
        args = arg.strip().split()
        if not args:
            print(f"REBALANCE_ETH_THRESHOLD: {getattr(config, 'REBALANCE_ETH_THRESHOLD', None)}")
            return
        if len(args) != 2:
            print("Usage: rebalance [param] [value]")
            return
        param, value = args
        param = param.upper()
        try:
            if param == "REBALANCE_ETH_THRESHOLD":
                val = float(value)
                if val < config.MIN_REBALANCE_THRESHOLD or val > config.MAX_REBALANCE_THRESHOLD:
                    print(f"REBALANCE_ETH_THRESHOLD must be between {config.MIN_REBALANCE_THRESHOLD} and {config.MAX_REBALANCE_THRESHOLD}.")
                    return
                logger.info(f"Setting {param} to {val}")
                setattr(config, param, val)
                print(f"{param} set to {val}")
            else:
                print(f"Unknown parameter: {param}")
        except Exception as e:
            logger.error(f"Invalid rebalance input: {e}")
            print("Invalid rebalance parameter or value.")

    def do_order_size(self, arg):
        """View or set order size multipliers. Usage: order_size [param] [value]"""
        args = arg.strip().split()
        if not args:
            print(f"SELL_SIZE_MULTIPLIER: {getattr(config, 'SELL_SIZE_MULTIPLIER', None)}")
            print(f"BUY_SIZE_MULTIPLIER: {getattr(config, 'BUY_SIZE_MULTIPLIER', None)}")
            return
        if len(args) != 2:
            print("Usage: order_size [param] [value]")
            return
        param, value = args
        param = param.upper()
        try:
            val = float(value)
            if param == "SELL_SIZE_MULTIPLIER":
                if val < config.MIN_SELL_SIZE_MULTIPLIER or val > config.MAX_SELL_SIZE_MULTIPLIER:
                    print(f"SELL_SIZE_MULTIPLIER must be between {config.MIN_SELL_SIZE_MULTIPLIER} and {config.MAX_SELL_SIZE_MULTIPLIER}.")
                    return
            elif param == "BUY_SIZE_MULTIPLIER":
                if val < config.MIN_BUY_SIZE_MULTIPLIER or val > config.MAX_BUY_SIZE_MULTIPLIER:
                    print(f"BUY_SIZE_MULTIPLIER must be between {config.MIN_BUY_SIZE_MULTIPLIER} and {config.MAX_BUY_SIZE_MULTIPLIER}.")
                    return
            else:
                print(f"Unknown parameter: {param}")
                return
            logger.info(f"Setting {param} to {val}")
            setattr(config, param, val)
            print(f"{param} set to {val}")
        except Exception as e:
            logger.error(f"Invalid order size input: {e}")
            print("Invalid order size parameter or value.")

    def do_system(self, arg):
        """View or set system parameters. Usage: system [param] [value]"""
        args = arg.strip().split()
        if not args:
            print(f"MAIN_LOOP_RECOVERY_DELAY: {getattr(config, 'MAIN_LOOP_RECOVERY_DELAY', None)}")
            print(f"HEARTBEAT_INTERVAL: {getattr(config, 'HEARTBEAT_INTERVAL', None)}")
            print(f"TIMESTAMP_VALIDATION: {getattr(config, 'TIMESTAMP_VALIDATION', None)}")
            return
        if len(args) != 2:
            print("Usage: system [param] [value]")
            return
        param, value = args
        param = param.upper()
        try:
            val = float(value)
            if param in ["MAIN_LOOP_RECOVERY_DELAY", "HEARTBEAT_INTERVAL", "TIMESTAMP_VALIDATION"]:
                if val < 0:
                    print(f"{param} must be non-negative.")
                    return
                logger.info(f"Setting {param} to {val}")
                setattr(config, param, val)
                print(f"{param} set to {val}")
            else:
                print(f"Unknown parameter: {param}")
        except Exception as e:
            logger.error(f"Invalid system input: {e}")
            print("Invalid system parameter or value.")

    def cmdloop(self, intro=None):
        """Custom cmdloop to handle interrupts and errors."""
        if intro is not None:
            self.intro = intro
        if self.intro:
            self.stdout.write(str(self.intro) + "\n")
        stop = None
        while not stop:
            try:
                super(BotCLI, self).cmdloop(intro="")
                break
            except KeyboardInterrupt:
                self.stdout.write("^C\n")
                self.do_exit("")
                stop = True
            except EOFError:
                self.stdout.write("\n")
                self.do_exit("")
                stop = True
            except Exception as e:
                logger.error(f"CLI error: {e}")
                self.stdout.write("An error occurred. Type 'exit' to shutdown.\n")

def start_cli():
    """Start the CLI interface."""
    try:
        logger.info("Starting CLI")
        cli = BotCLI()
        cli.cmdloop()
    except Exception as e:
        logger.error(f"CLI crashed: {e}", exc_info=True)
        shutdown_event.set()

class LSTMPricePredictor(nn.Module):
    def __init__(
        self,
        input_size=18,  # 18 features
        hidden_size=config.PYTORCH_HIDDEN_SIZE,
        num_layers=config.PYTORCH_NUM_LAYERS,
        dropout=config.PYTORCH_DROPOUT
    ):
        super(LSTMPricePredictor, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers

        # Bidirectional LSTM
        self.lstm = nn.LSTM(
            input_size,
            hidden_size,
            num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0,
            bidirectional=True
        )

        # Attention layer
        self.attention = nn.Linear(hidden_size * 2, 1)  # *2 for bidirectional
        self.attention_softmax = nn.Softmax(dim=1)

        # Fully connected layers
        self.fc1 = nn.Linear(hidden_size * 2, hidden_size)  # *2 for bidirectional
        self.bn1 = nn.BatchNorm1d(hidden_size)
        self.fc2 = nn.Linear(hidden_size, 1)

        # Residual connection from input
        self.residual_fc = nn.Linear(input_size, 1)

        self.relu = nn.ReLU()

    def forward(self, x):
        batch_size = x.size(0)

        # LSTM
        lstm_out, _ = self.lstm(x)  # [batch, seq_len, hidden_size * 2]

        # Attention
        attention_weights = self.attention_softmax(self.attention(lstm_out))  # [batch, seq_len, 1]
        context = torch.sum(lstm_out * attention_weights, dim=1)  # [batch, hidden_size * 2]

        # Fully connected
        out = self.fc1(context)  # [batch, hidden_size]
        out = self.bn1(out)
        out = self.relu(out)
        out = self.fc2(out)  # [batch, 1]

        # Residual connection
        residual = self.residual_fc(x[:, -1, :])  # Use last timestep
        out = out + residual

        return out

class WebSocketManager:
    def __init__(self, api_key, secret_key):
        self.api_key = api_key
        self.secret_key = secret_key
        self.ws_url = config.WEBSOCKET_COINBASE_URL
        self.local_url = config.WEBSOCKET_LOCAL_URL
        self.client_id = "Client"
        self.session_id = None
        self.local_ws = None
        self.coinbase_ws = None
        self.exchange = ccxt.coinbase({
            'apiKey': api_key,
            'secret': secret_key,
            'enableRateLimit': True,
        })  # Initialize Coinbase API client
        self.stop_event = threading.Event()
        self.lock = threading.Lock()
        self.command_queue = queue.Queue()
        self.websocket_thread = None
        self.trades_cache = []
        self.trade_lock = threading.Lock()
        self.is_processing_command = False
        self.is_identified = False
        self.logger = logging.getLogger(__name__)
        self.logger.debug("WebSocketManager initialized")
        self.init_ws_candle_buffer()

    @backoff.on_exception(backoff.expo, Exception, max_tries=5, max_time=60)
    def connect_coinbase(self):
        if self.stop_event.is_set():
            return
        self.coinbase_ws = websocket.WebSocketApp(
            self.ws_url,
            on_message=self.on_coinbase_message,
            on_error=self.on_coinbase_error,
            on_close=self.on_coinbase_close,
            on_open=self.on_coinbase_open,
        )
        self.logger.info("Connecting to Coinbase WebSocket...")
        self.coinbase_ws.run_forever(
            ping_interval=config.WEBSOCKET_PING_INTERVAL,
            ping_timeout=config.WEBSOCKET_PING_TIMEOUT,
            sslopt={"cert_reqs": ssl.CERT_NONE}
        )

    @backoff.on_exception(backoff.expo, Exception, max_tries=5, max_time=60)
    def connect_local(self):
        if self.stop_event.is_set():
            self.logger.info("Stop event set, aborting local WebSocket connection")
            return
        self.logger.info(f"Attempting to connect to local WebSocket at {self.local_url} as client_id={self.client_id}")
        try:
            self.local_ws = websocket.WebSocket()
            self.local_ws.connect(self.local_url)
            self.logger.info(f"Successfully connected to local WebSocket at {self.local_url} as client_id={self.client_id}")
            identify_msg = {
                "action": "identify",
                "client_id": self.client_id,
                "session_id": self.session_id,
            }
            self.local_ws.send(json.dumps(identify_msg))
            self.logger.debug(f"Sent identification: {identify_msg}")
        except Exception as e:
            self.logger.error(f"Failed to connect to local WebSocket: {e}", exc_info=True)
            raise

    def on_coinbase_message(self, ws, message):
        try:
            if self.stop_event.is_set():
                ws.close()
                return
            data = json.loads(message)
            if data.get("type") == "match":
                trade = {
                    "source": "coinbase",
                    "type": "trade",
                    "timestamp": int(pd.Timestamp(data["time"]).timestamp() * 1000),
                    "price": float(data["price"]),
                    "size": float(data["size"]),
                    "side": data["side"],
                }
                with self.trade_lock:
                    self.trades_cache.append(trade)
                    if len(self.trades_cache) > getattr(config, "MAX_TRADES_CACHE_SIZE", 10000):
                        self.trades_cache.pop(0)
                self.logger.debug(f"Processed match trade: price={trade['price']:.2f}, size={trade['size']:.4f}")
            elif data.get("type") == "ticker":
                if not all(key in data for key in ["price", "time"]):
                    self.logger.warning(f"Invalid ticker message: missing required fields {data}")
                    return
                trade = {
                    "source": "coinbase",
                    "type": "ticker",
                    "timestamp": int(pd.Timestamp(data["time"]).timestamp() * 1000),
                    "price": float(data["price"]),
                    "size": float(data.get("last_size", 0)),
                    "side": data.get("side", "unknown"),
                }
                with self.trade_lock:
                    self.trades_cache.append(trade)
                    if len(self.trades_cache) > getattr(config, "MAX_TRADES_CACHE_SIZE", 10000):
                        self.trades_cache.pop(0)
                self.aggregate_ws_candle(data)
                self.logger.debug(f"Processed ticker trade: price={trade['price']:.2f}, size={trade['size']:.4f}")
            time.sleep(0.01)
        except Exception as e:
            self.logger.error(f"Error in Coinbase WebSocket message: {str(e)}")

    def on_coinbase_error(self, ws, error):
        self.logger.error(f"Coinbase WebSocket error: {error}")
        if not self.stop_event.is_set():
            self.logger.info("Attempting to reconnect Coinbase WebSocket...")
            self.connect_coinbase()
            self.logger.debug("Coinbase WS reconnected")

    def on_coinbase_close(self, ws, close_status_code, close_msg):
        self.logger.info(f"Coinbase WebSocket closed: {close_status_code} - {close_msg}")
        if not self.stop_event.is_set():
            self.logger.info("Attempting to reconnect Coinbase WebSocket...")
            self.connect_coinbase()

    def on_coinbase_open(self, ws):
        subscription = json.dumps(
            {
                "type": "subscribe",
                "channels": [
                    {"name": "matches", "product_ids": [config.SYMBOL]},
                    {"name": "ticker", "product_ids": [config.SYMBOL]}
                ],
                "api_key": self.api_key,
                "secret": self.secret_key,
            }
        )
        ws.send(subscription)
        self.logger.info("Coinbase WebSocket subscribed to matches and ticker")

    def start(self):
        self.logger.info("Starting WebSocketManager for client_id=%s", self.client_id)
        self.connect_local()
        coinbase_thread = threading.Thread(target=self.connect_coinbase, daemon=True)
        coinbase_thread.start()
        local_thread = threading.Thread(target=self.handle_local_messages, daemon=True)
        local_thread.start()
        self.logger.info("Started WebSocket message handling thread for client_id=%s", self.client_id)
        return coinbase_thread

    def handle_local_messages(self):
        self.logger.info(
            "Starting handle_local_messages for client_id=%s, session_id=%s",
            self.client_id,
            self.session_id,
        )
        while not self.stop_event.is_set():
            try:
                message = self.local_ws.recv()
                self.logger.info("Received raw WebSocket message: %s", message)
                message_data = json.loads(message)
                self.logger.info("Processing message: %s", message_data)

                command = message_data.get("command", message_data.get("action"))
                if command == "identify":
                    self.session_id = message_data.get("session_id")
                    status = message_data.get("status")
                    if status == "success":
                        self.is_identified = True
                        self.logger.info(
                            "Identification successful: client_id=%s, session_id=%s",
                            self.client_id,
                            self.session_id,
                        )
                    else:
                        self.logger.warning(
                            "Identification response received without success status: client_id=%s, session_id=%s, message=%s",
                            self.client_id,
                            self.session_id,
                            message_data,
                        )
                elif command in ["optimize", "config_update"]:
                    status = message_data.get("status")
                    if status == "error":
                        self.logger.warning(
                            "Skipping %s command with error status: %s",
                            command,
                            message_data,
                        )
                        continue
                    request_id = message_data.get("request_id", "unknown")
                    parameters = message_data.get("parameters", {})
                    group = message_data.get("group", "unknown")
                    self.logger.info(
                        "Queuing %s command: ID=%s, group=%s, parameters=%s",
                        command,
                        request_id,
                        group,
                        parameters,
                    )
                    self.command_queue.put(
                        {
                            "command": command,
                            "request_id": request_id,
                            "group": group,
                            "parameters": parameters,
                        }
                    )
                else:
                    self.logger.warning("Unknown command received: %s", command)
            except websocket.WebSocketConnectionClosedException:
                self.logger.error("Local WebSocket connection closed, attempting to reconnect...")
                self.is_identified = False
                self.connect_local()
            except json.JSONDecodeError:
                self.logger.error("Failed to parse WebSocket message: %s", message)
            except Exception as e:
                self.logger.error("Error processing WebSocket message: %s", str(e))

    def process_command(self):
        if self.is_processing_command:
            self.logger.debug("Skipping command processing, already in progress")
            return False
        try:
            command_data = self.command_queue.get_nowait()
        except queue.Empty:
            return False
        self.is_processing_command = True
        try:
            command = command_data["command"]
            request_id = command_data["request_id"]
            group = command_data["group"]
            parameters = command_data["parameters"]
            self.logger.info(
                "Processing %s command from queue: ID=%s, group=%s, parameters=%s",
                command,
                request_id,
                group,
                parameters,
            )
            apply_parameters(parameters, request_id, command, self, group)
            response = {
                "command": command,
                "status": "applied",
                "request_id": request_id,
                "group": group,
                "timestamp": int(time.time() * 1000),
            }
            self.send(json.dumps(response))
            self.logger.info("Sent %s response: %s", command, response)
            return True
        except Exception as e:
            self.logger.error(
                "Error processing command %s (request_id=%s, group=%s): %s",
                command,
                request_id,
                group,
                e,
            )
            return False
        finally:
            self.is_processing_command = False
            self.command_queue.task_done()

    def ensure_local_connection(self):
        with self.lock:
            if not self.local_ws or not self.local_ws.connected:
                self.logger.warning("Local WebSocket disconnected, attempting reconnect...")
                retries = 3
                for attempt in range(retries):
                    try:
                        self.connect_local()
                        break
                    except Exception as e:
                        self.logger.error(f"Reconnect attempt {attempt + 1}/{retries} failed: {e}")
                        if attempt < retries - 1:
                            time.sleep(config.RECONNECT_DELAY)
                        else:
                            self.logger.error("Max reconnect attempts reached")

    def send(self, message):
        self.ensure_local_connection()
        with self.lock:
            try:
                data = json.loads(message) if isinstance(message, str) else message
                if isinstance(data, dict) and data.get("action") == "identify":
                    self.local_ws.send(message)
                    self.logger.info("WebSocket data sent successfully: identify")
                elif self.is_identified:
                    self.local_ws.send(message)
                    self.logger.info("WebSocket data sent successfully")
                else:
                    self.logger.warning("Skipping send: Client not yet identified")
            except Exception as e:
                self.logger.error(f"Failed to send WebSocket message: {e}")

    def close(self):
        self.stop_event.set()
        with self.lock:
            if self.local_ws and self.local_ws.connected:
                try:
                    self.local_ws.close()
                    self.logger.info("Local WebSocket closed")
                except Exception as e:
                    self.logger.error(f"Error closing local WebSocket: {e}")
            if self.coinbase_ws:
                try:
                    self.coinbase_ws.close()
                    self.logger.info("Coinbase WebSocket closed")
                except Exception as e:
                    self.logger.error(f"Error closing Coinbase WebSocket: {e}")

    def stop(self):
        """Stop the WebSocketManager and close all connections."""
        try:
            self.logger.info("Stopping WebSocketManager")
            self.close()
            self.stop_event.set()
        except Exception as e:
            self.logger.error(f"Error stopping WebSocketManager: {str(e)}")

    def get_new_trades(self):
        """Retrieves all trades from the cache and clears it."""
        with self.trade_lock:
            if not self.trades_cache:
                return []
            trades_to_return = list(self.trades_cache)
            self.trades_cache.clear()
            self.logger.debug(f"Retrieved {len(trades_to_return)} trades from cache for processing.")
        return trades_to_return

    def init_ws_candle_buffer(self):
        """Initialize buffer for WebSocket candlestick data with historical data."""
        self.ws_candle_buffer = pd.DataFrame(columns=["timestamp", "open", "high", "low", "close", "volume", "trades"])
        self.current_candle = None
        self.candle_interval = config.CANDLE_INTERVAL
        self.last_candle_time = None
        needed_candles_init = getattr(config, 'ML_LOOKBACK_PERIODS', 61) # Use ML_LOOKBACK_PERIODS or default
        fetch_limit_init = needed_candles_init + getattr(config, 'HISTORICAL_INIT_BUFFER', 10) # Add a buffer

        try:
            # Pre-populate with historical data
            timeframe = "1m"
            # Dynamic since calculation
            since_duration_seconds_init = (fetch_limit_init * 60) + (10 * 60) # 10 min safety buffer
            since_init_ms = int((time.time() - since_duration_seconds_init) * 1000)
            self.logger.info(
                f"Pre-populating ws_candle_buffer: fetching {fetch_limit_init} candles since "
                f"{pd.to_datetime(since_init_ms, unit='ms', utc=True)}"
            )
            ohlcv = self.exchange.fetch_ohlcv(config.SYMBOL, timeframe, since=since_init_ms, limit=fetch_limit_init)
            if ohlcv:
                historical_df = pd.DataFrame(
                    ohlcv,
                    columns=["timestamp", "open", "high", "low", "close", "volume"]
                )
                historical_df["timestamp"] = pd.to_datetime(historical_df["timestamp"], unit="ms", utc=True)
                historical_df["trades"] = 1  # Placeholder
                if not historical_df.empty and not historical_df.isna().all().all():
                    if self.ws_candle_buffer.empty:
                        self.ws_candle_buffer = historical_df  # Directly assign to avoid concat with empty DataFrame
                    else:
                        self.ws_candle_buffer = pd.concat([self.ws_candle_buffer, historical_df], ignore_index=True)
                    self.ws_candle_buffer = self.ws_candle_buffer.drop_duplicates(subset=["timestamp"], keep="last")
                    self.logger.info(f"Pre-populated ws_candle_buffer with {len(historical_df)} historical candles")
                    self.logger.debug(f"ws_candle_buffer rows={len(self.ws_candle_buffer)}, NaNs={self.ws_candle_buffer.isna().sum().to_dict()}")
                else:
                    self.logger.warning("Historical data is empty or all-NA, skipping pre-population")
            else:
                self.logger.warning("No historical data fetched for ws_candle_buffer")
        except Exception as e:
            self.logger.error(f"Error pre-populating ws_candle_buffer: {str(e)}")
        self.logger.info("Initialized WebSocket candlestick buffer")

    def aggregate_ws_candle(self, ticker_data):
        """Aggregate Coinbase ticker data into 1-minute OHLCV candles."""
        try:
            if not all(key in ticker_data for key in ["price", "time"]):
                self.logger.warning(f"Invalid ticker message: missing required fields {ticker_data}")
                return
            price = float(ticker_data["price"])
            volume = float(ticker_data.get("last_size", 0))
            timestamp = pd.to_datetime(ticker_data["time"], utc=True)
            trade_count = 1

            candle_time = timestamp.floor("min")

            if self.last_candle_time is None or candle_time > self.last_candle_time:
                if self.current_candle is not None:
                    temp_buffer = pd.DataFrame([self.current_candle])
                    if not temp_buffer.empty and not temp_buffer.isna().all().all():
                        self.ws_candle_buffer = pd.concat([self.ws_candle_buffer, temp_buffer], ignore_index=True)
                        self.logger.info(
                            f"Closed WS candle: timestamp={self.current_candle['timestamp']}, "
                            f"open={self.current_candle['open']:.2f}, close={self.current_candle['close']:.2f}, "
                            f"volume={self.current_candle['volume']:.4f}, trades={self.current_candle['trades']}"
                        )
                    else:
                        self.logger.warning("Skipping concatenation of empty or all-NA current candle")

                self.current_candle = {
                    "timestamp": candle_time,
                    "open": price,
                    "high": price,
                    "low": price,
                    "close": price,
                    "volume": volume,
                    "trades": trade_count
                }
                self.last_candle_time = candle_time
                self.logger.info(f"Started new WS candle: timestamp={candle_time}, price={price:.2f}")
            else: # Aggregating into the current_candle
                if self.current_candle is not None: # Ensure current_candle exists
                    self.current_candle["high"] = max(self.current_candle["high"], price)
                    self.current_candle["low"] = min(self.current_candle["low"], price)
                    self.current_candle["close"] = price
                    self.current_candle["volume"] += volume
                    self.current_candle["trades"] += trade_count
                else: # Should ideally not happen if last_candle_time is set
                    self.logger.warning("Attempted to aggregate into a non-existent current_candle. Re-initializing.")
                    # Re-initialize current_candle as if it's a new candle
                    self.current_candle = {
                        "timestamp": candle_time, "open": price, "high": price, "low": price,
                        "close": price, "volume": volume, "trades": trade_count
                    }
                    self.last_candle_time = candle_time # Reset last_candle_time as well

            # Update or append current candle in buffer
            if self.current_candle:
                temp_buffer = pd.DataFrame([self.current_candle])
                if not temp_buffer.empty and not temp_buffer.isna().all().all():
                    if not self.ws_candle_buffer.empty and self.ws_candle_buffer["timestamp"].iloc[-1] == candle_time:
                        self.ws_candle_buffer.iloc[-1] = temp_buffer.iloc[0]
                    else:
                        self.ws_candle_buffer = pd.concat([self.ws_candle_buffer, temp_buffer], ignore_index=True)
                    self.ws_candle_buffer = self.ws_candle_buffer.drop_duplicates(subset=["timestamp"], keep="last")
                    self.logger.debug(
                        f"Updated WS candle: timestamp={candle_time}, price={price:.2f}, volume={volume:.4f}, "
                        f"trades={self.current_candle['trades']}, buffer_rows={len(self.ws_candle_buffer)}"
                    )
                else:
                    self.logger.warning("Skipping concatenation of empty or all-NA temp_buffer for current_candle update")

            # Fallback to fetch historical data if buffer is too low
            needed_candles_agg_fallback = getattr(config, 'ML_LOOKBACK_PERIODS', 61)
            # Trigger fallback if buffer has less than, e.g., half of what's needed, or a fixed low number like 10.
            # Using a fixed low number might be simpler if ML_LOOKBACK_PERIODS is very large.
            # Let's use a threshold like 10 or needed_candles_agg_fallback / 2, whichever is more appropriate.
            # For now, sticking to the logic of needing at least half, or a minimum of 10.
            fallback_threshold = max(10, needed_candles_agg_fallback // 2) 
            if len(self.ws_candle_buffer) < fallback_threshold:
                fetch_limit_agg = needed_candles_agg_fallback + getattr(config, 'HISTORICAL_AGG_BUFFER', 10)
                self.logger.warning(
                    f"Low candle count in buffer: {len(self.ws_candle_buffer)}, "
                    f"threshold={fallback_threshold}. Attempting to fetch {fetch_limit_agg} historical candles."
                )
                try:
                    timeframe = "1m"
                    # Dynamic since calculation
                    since_duration_seconds_agg = (fetch_limit_agg * 60) + (10 * 60) # 10 min safety buffer
                    since_agg_ms = int((time.time() - since_duration_seconds_agg) * 1000)
                    ohlcv = self.exchange.fetch_ohlcv(config.SYMBOL, timeframe, since=since_agg_ms, limit=fetch_limit_agg)
                    if ohlcv:
                        historical_df = pd.DataFrame(
                            ohlcv,
                            columns=["timestamp", "open", "high", "low", "close", "volume"]
                        )
                        historical_df["timestamp"] = pd.to_datetime(historical_df["timestamp"], unit="ms", utc=True)
                        historical_df["trades"] = 1 # Placeholder
                        if not historical_df.empty and not historical_df.isna().all().all():
                            # Prepend historical data to prioritize existing buffer data on merge
                            self.ws_candle_buffer = pd.concat([historical_df, self.ws_candle_buffer], ignore_index=True).drop_duplicates(
                                subset=["timestamp"], keep="last"
                            )
                            self.ws_candle_buffer = self.ws_candle_buffer.sort_values(by="timestamp", ascending=True)
                            self.logger.info(f"Added {len(historical_df)} historical candles to ws_candle_buffer during aggregation fallback.")
                        else:
                            self.logger.warning("Historical data for fallback is empty or all-NA, skipping population")
                except Exception as e:
                    self.logger.error(f"Error fetching historical candles during aggregation fallback: {str(e)}")

            self.logger.debug(f"ws_candle_buffer rows={len(self.ws_candle_buffer)}, NaNs={self.ws_candle_buffer.isna().sum().to_dict()}")
        except Exception as e:
            self.logger.error(f"Error aggregating WS candle: {str(e)}")

    def get_ws_candles(self, lookback_minutes_param=120):
        needed_candles = config.ML_LOOKBACK_PERIODS
        min_buffer_lookback = math.ceil(needed_candles * 1.20)
        effective_lookback = max(lookback_minutes_param, min_buffer_lookback)
        self.logger.info(
            f"Fetching candles: needed={needed_candles}, lookback={effective_lookback} mins"
        )

        try:
            cutoff = pd.Timestamp.now(tz="UTC") - pd.Timedelta(minutes=effective_lookback)
            candles_from_buffer = self.ws_candle_buffer[self.ws_candle_buffer["timestamp"] >= cutoff].copy()

            current_candle_df = pd.DataFrame()
            if self.current_candle and pd.to_datetime(self.current_candle["timestamp"], utc=True) >= cutoff:
                temp_buffer = pd.DataFrame([self.current_candle])
                if not temp_buffer.empty and not temp_buffer.isna().all().all():
                    current_candle_df = temp_buffer

            if not current_candle_df.empty:
                recent_candles = pd.concat([candles_from_buffer, current_candle_df], ignore_index=True).drop_duplicates(
                    subset=["timestamp"], keep="last"
                ).sort_values(by="timestamp")
            else:
                recent_candles = candles_from_buffer

            self.logger.info(f"Retrieved {len(recent_candles)} candles from buffer")

            if len(recent_candles) < needed_candles:
                fetch_limit = needed_candles + config.HISTORICAL_AGG_BUFFER
                self.logger.warning(f"Insufficient candles: {len(recent_candles)}, fetching {fetch_limit}")
                try:
                    timeframe = "1m"
                    since_ms = int((time.time() - (fetch_limit * 60 + 600)) * 1000)
                    ohlcv = self.exchange.fetch_ohlcv(config.SYMBOL, timeframe, since=since_ms, limit=fetch_limit)
                    if ohlcv:
                        historical_df = pd.DataFrame(
                            ohlcv,
                            columns=["timestamp", "open", "high", "low", "close", "volume"]
                        )
                        historical_df["timestamp"] = pd.to_datetime(historical_df["timestamp"], unit="ms", utc=True)
                        historical_df["trades"] = 1
                        if not historical_df.empty and not historical_df.isna().all().all():
                            recent_candles = pd.concat([historical_df, recent_candles], ignore_index=True).drop_duplicates(
                                subset=["timestamp"], keep="last"
                            ).sort_values(by="timestamp")
                            self.logger.info(f"Added {len(historical_df)} historical candles")
                except Exception as e:
                    self.logger.error(f"Error fetching historical candles: {e}", exc_info=True)

            max_candles = needed_candles + 30
            if len(recent_candles) > max_candles:
                recent_candles = recent_candles.tail(max_candles)
                self.logger.info(f"Trimmed to {max_candles} candles")
            return recent_candles
        except Exception as e:
            self.logger.error(f"Error in get_ws_candles: {e}", exc_info=True)
            return pd.DataFrame(columns=["timestamp", "open", "high", "low", "close", "volume", "trades"])

# --- Global Feature Order Spacing State ---
def get_last_feature_order_price(bot_state):
    return bot_state.get('last_feature_order_price', None)

def set_last_feature_order_price(bot_state, price):
    bot_state['last_feature_order_price'] = price

# --- Helper: Check Global Feature Order Spacing (open orders + last executed) ---
def can_place_feature_order_with_global_spacing(bot_state, price, min_spacing=0.005):
    """
    Prevents placing a new feature order if any open feature order (buy or sell) or last executed feature order is within min_spacing (fractional, e.g. 0.005 = 0.5%) of price.
    Returns True if no conflict, False if too close.
    """
    all_orders = bot_state.get('buy_orders', []) + bot_state.get('sell_orders', [])
    for o in all_orders:
        if o.get('feature') not in (None, 'base') and o.get('status', 'open') == 'open':
            existing_price = float(o.get('price', 0))
            if abs(existing_price - price) < (min_spacing * price):
                return False
    last_exec = get_last_feature_order_price(bot_state)
    if last_exec is not None and abs(last_exec - price) < (min_spacing * price):
        return False
    return True

# NEW: Additional feature calculations
def compute_additional_features(df):
    df = df.copy()
    try:
        df["price_spread"] = df["high"] - df["low"]
        df["returns"] = df["close"].pct_change(fill_method=None).fillna(0)
        df["volume_change"] = df["volume"].pct_change(fill_method=None).fillna(0)
        df["trade_intensity"] = df["trades"] / df["volume"].replace(0, 1)
        df = df.fillna({
            "price_spread": 0.0,
            "returns": 0.0,
            "volume_change": 0.0,
            "trade_intensity": 0.0
        })
        logger.info(
            f"Additional features: spread_mean={df['price_spread'].mean():.2f}, "
            f"returns_mean={df['returns'].mean():.4f}, "
            f"volume_change_mean={df['volume_change'].mean():.4f}, "
            f"trade_intensity_mean={df['trade_intensity'].mean():.2f}"
        )
    except Exception as e:
        logger.error(f"Error computing additional features: {e}")
        df["price_spread"] = 0.0
        df["returns"] = 0.0
        df["volume_change"] = 0.0
        df["trade_intensity"] = 0.0
    return df

# Online learning functions
def online_update_sklearn(model, scaler, X, y, lookback=80):
    """
    Update SGDRegressor model online.
    Args:
        model: SGDRegressor model.
        scaler (StandardScaler): Scaler for features.
        X (pd.DataFrame): Input features (lookback periods).
        y (np.ndarray): Target close prices.
        lookback (int): Number of lookback periods (default: 80).
    Returns:
        SGDRegressor: Updated model.
    """
    try:
        features = [
            "close", "volume", "trades", "rsi", "ema", "volatility", "macd", "macd_signal",
            "bollinger_upper", "bollinger_lower", "momentum", "volume_trend", "atr", "vwap",
            "price_spread", "returns", "volume_change", "trade_intensity"
        ]
        # Validate inputs
        if not isinstance(X, pd.DataFrame):
            logger.error("X must be a pandas DataFrame")
            return model
        if len(X) < lookback:
            logger.error(f"Insufficient samples: X has {len(X)} rows, need {lookback}")
            return model
        if not all(f in X.columns for f in features):
            logger.error(f"Missing features in X: {[f for f in features if f not in X.columns]}")
            return model

        # Prepare sequence features for SGD
        X_seq = []
        for i in range(len(X) - lookback + 1):
            X_seq.append(X[features].iloc[i:i + lookback].values.flatten())
        X_seq = np.array(X_seq)
        y_seq = y[-len(X_seq):]

        if len(X_seq) == 0 or len(y_seq) == 0:
            logger.error("Empty sequence data for SGD update")
            return model
        if len(X_seq) != len(y_seq):
            logger.error(f"Sample mismatch: X has {len(X_seq)} samples, y has {len(y_seq)} samples")
            return model
        if X_seq.shape[1] != lookback * len(features):
            logger.error(f"Feature mismatch: expected {lookback * len(features)} features, got {X_seq.shape[1]}")
            return model

        # Scale features
        X_scaled = scaler.transform(X_seq)
        model.partial_fit(X_scaled, y_seq.ravel())
        logger.info("SGDRegressor updated online")
        return model
    except Exception as e:
        logger.error(f"Sklearn online update failed: {e}")
        return model

def online_update_pytorch(model, feature_scaler, target_scaler, X, y, device='cpu'):
    try:
        features = [
            "close", "volume", "trades", "rsi", "ema", "volatility", "macd", "macd_signal",
            "bollinger_upper", "bollinger_lower", "momentum", "volume_trend", "atr", "vwap",
            "price_spread", "returns", "volume_change", "trade_intensity"
        ]
        # Validate and convert X
        logger.debug(f"Received X type: {type(X)}, shape: {getattr(X, 'shape', 'N/A')}")
        if not isinstance(X, pd.DataFrame):
            if isinstance(X, (np.ndarray, list, tuple)):
                try:
                    if isinstance(X, np.ndarray) and X.ndim == 2 and X.shape[1] == len(features):
                        X = pd.DataFrame(X, columns=features)
                    elif isinstance(X, (list, tuple)) and len(X) > 0 and isinstance(X[0], (list, np.ndarray, tuple)) and len(X[0]) == len(features):
                        X = pd.DataFrame(X, columns=features)
                    else:
                        logger.error(f"Invalid X structure: expected 2D array/tuple with {len(features)} features, got {getattr(X, 'shape', len(X))}")
                        return model
                    logger.info("Converted non-DataFrame X to DataFrame")
                except Exception as e:
                    logger.error(f"Failed to convert X to DataFrame: {e}")
                    return model
            else:
                logger.error(f"X must be a pandas DataFrame or convertible array/tuple, got {type(X)}")
                return model

        # Filter to required features
        missing_features = [f for f in features if f not in X.columns]
        if missing_features:
            logger.error(f"Missing features in X: {missing_features}")
            return model
        X = X[features]  # Ensure exact feature set
        if X.shape[1] != len(features):
            logger.error(f"Feature mismatch: expected {len(features)} features, got {X.shape[1]}: {X.columns.tolist()}")
            return model

        # Check for NaN or infinite values
        if X.isna().any().any() or np.any(np.isinf(X.values)):
            logger.error("X contains NaN or infinite values")
            return model

        # Ensure sufficient sequence length
        expected_seq_len = config.PYTORCH_LOOKBACK  # Should be 64 for LSTMPricePredictor
        if len(X) < expected_seq_len:
            logger.error(f"Insufficient sequence length: got {len(X)}, need {expected_seq_len}")
            return model
        X = X.tail(expected_seq_len)  # Take the last 64 rows
        logger.debug(f"Trimmed X to {len(X)} rows for PyTorch update")

        # Validate y (expected to be close prices)
        logger.debug(f"Received y type: {type(y)}, shape: {getattr(y, 'shape', 'N/A')}")
        if isinstance(y, pd.Series):
            y = y.values
        y_array = np.array(y).reshape(-1, 1)
        if len(y_array) < expected_seq_len:
            logger.error(f"Insufficient y length: got {len(y_array)}, need {expected_seq_len}")
            return model
        if np.any(np.isnan(y_array)) or np.any(np.isinf(y_array)):
            logger.error("y contains NaN or infinite values")
            return model
        y_array = y_array[-expected_seq_len:]  # Align with X
        logger.debug(f"Using {len(y_array)} close prices as target for PyTorch update")

        model.train()
        # Disable batch norm stats update for single-sample training
        for module in model.modules():
            if isinstance(module, nn.BatchNorm1d):
                module.eval()  # Use running stats
                logger.debug("Set BatchNorm1d to eval mode for online update")

        optimizer = optim.Adam(model.parameters(), lr=config.ONLINE_LEARNING_RATE)
        criterion = nn.MSELoss()

        # Scale inputs
        X_scaled = feature_scaler.transform(X)
        X_tensor = torch.FloatTensor(X_scaled).unsqueeze(0).to(device)  # Shape: [1, 64, 18]
        logger.debug(f"X_tensor shape: {X_tensor.shape}")

        # Scale target
        y_scaled = target_scaler.transform(y_array).flatten()
        y_tensor = torch.FloatTensor(y_scaled).reshape(-1, 1).to(device)

        # Verify tensor shapes
        if X_tensor.shape[1] != expected_seq_len:
            logger.error(f"Invalid X_tensor sequence length: got {X_tensor.shape[1]}, expected {expected_seq_len}")
            return model
        if y_tensor.shape[0] != expected_seq_len:
            logger.error(f"Invalid y_tensor length: got {y_tensor.shape[0]}, expected {expected_seq_len}")
            return model

        optimizer.zero_grad()
        output = model(X_tensor)  # Shape: [1, 1]
        loss = criterion(output, y_tensor[-1:])  # Use last timestep
        loss.backward()
        optimizer.step()
        logger.info(f"PyTorch updated online with close prices, loss={loss.item():.4f}")
        return model
    except Exception as e:
        logger.error(f"PyTorch online update failed: {e}")
        return model

def online_update_xgboost(model, scaler, X, y):
    try:
        X_scaled = scaler.transform(X)
        dmatrix = xgb.DMatrix(X_scaled, label=y)
        model.update(dmatrix, iteration=1)
        logger.info("XGBoost updated online")
        return model
    except Exception as e:
        logger.error(f"Error updating XGBoost: {e}")
        return model

# NEW: Retrain ML models with enhanced logging
def retrain_ml_models(data, scaler_sklearn_rf, scaler_pytorch, scaler_xgb, scaler_meta):
    try:
        features = [
            "close", "volume", "trades", "rsi", "ema", "volatility", "macd", "macd_signal",
            "bollinger_upper", "bollinger_lower", "momentum", "volume_trend", "atr", "vwap",
            "price_spread", "returns", "volume_change", "trade_intensity"
        ]
        X = data[features].iloc[:-1]
        y = data["predicted_price"].iloc[:-1]

        if len(X) < config.MIN_DATA_POINTS:
            logger.warning(f"Insufficient data for retraining: {len(X)} rows")
            return None, None, None, None

        # RandomForestRegressor for batch training
        X_scaled_rf = scaler_sklearn_rf.fit_transform(X)
        sklearn_rf_model = RandomForestRegressor(
            n_estimators=config.SKLEARN_N_ESTIMATORS,
            max_depth=config.SKLEARN_MAX_DEPTH,
            random_state=42
        )
        sklearn_rf_model.fit(X_scaled_rf, y)
        logger.info("Retrained RandomForestRegressor")

        # SGDRegressor for online updates
        sklearn_sgd_model = SGDRegressor(random_state=84)
        sklearn_sgd_model.fit(X_scaled_rf, y)  # Initialize with same data
        logger.info("Initialized SGDRegressor")

        # PyTorch (feature scaler and target scaler)
        pytorch_feature_scaler = MinMaxScaler()
        X_scaled_pytorch = pytorch_feature_scaler.fit_transform(X)
        pytorch_target_scaler = MinMaxScaler()
        y_array = y.values.reshape(-1, 1)
        pytorch_target_scaler.fit(y_array)
        X_tensor = torch.FloatTensor(X_scaled_pytorch).unsqueeze(1)
        y_tensor = torch.FloatTensor(pytorch_target_scaler.transform(y_array)).reshape(-1, 1)
        pytorch_model = LSTMPricePredictor(input_size=len(features)).to(config.DEVICE)
        optimizer = optim.Adam(pytorch_model.parameters(), lr=config.PYTORCH_LEARNING_RATE)
        criterion = nn.MSELoss()
        for epoch in range(config.PYTORCH_NUM_EPOCHS):
            pytorch_model.train()
            optimizer.zero_grad()
            outputs = pytorch_model(X_tensor)
            loss = criterion(outputs, y_tensor)
            loss.backward()
            optimizer.step()
        logger.info(f"Retrained PyTorch: loss={loss.item():.4f}")

        # If PyTorch model/scalers are None, fallback to initial training
        if pytorch_model is None or pytorch_feature_scaler is None or pytorch_target_scaler is None:
            logger.warning("PyTorch retraining returned None, falling back to initial training function")
            from inspect import signature
            # Try to call train_pytorch_predictor with available args
            try:
                if 'lookback_minutes' in signature(train_pytorch_predictor).parameters:
                    pytorch_model, pytorch_feature_scaler, pytorch_target_scaler = train_pytorch_predictor(data, data, lookback_minutes=config.PYTORCH_LOOKBACK, num_epochs=config.PYTORCH_NUM_EPOCHS)
                else:
                    pytorch_model, pytorch_feature_scaler, pytorch_target_scaler = train_pytorch_predictor(data, data)
                if pytorch_model and pytorch_feature_scaler and pytorch_target_scaler:
                    logger.info("Fallback initial PyTorch training succeeded after retrain failure")
                else:
                    logger.error("Fallback initial PyTorch training also failed")
            except Exception as fallback_error:
                logger.error(f"Error in fallback initial PyTorch training: {fallback_error}")

        # XGBoost
        X_scaled_xgb = scaler_xgb.fit_transform(X)
        xgb_model = xgb.XGBRegressor(
            n_estimators=config.XGB_N_ESTIMATORS,
            max_depth=config.XGB_MAX_DEPTH,
            learning_rate=config.XGB_LEARNING_RATE,
            random_state=42
        )
        xgb_model.fit(X_scaled_xgb, y)
        logger.info("Retrained XGBoost")

        # Meta-Model
        sklearn_preds = sklearn_rf_model.predict(X_scaled_rf)
        pytorch_preds = pytorch_model(X_tensor).detach().numpy().flatten() if pytorch_model else np.zeros_like(y)
        xgb_preds = xgb_model.predict(X_scaled_xgb)
        meta_X = np.column_stack([sklearn_preds, pytorch_preds, xgb_preds, data["volatility"].iloc[:-1]])
        meta_X_scaled = scaler_meta.fit_transform(meta_X)
        meta_model = RandomForestRegressor(
            n_estimators=config.META_MODEL_N_ESTIMATORS,
            max_depth=config.META_MODEL_MAX_DEPTH,
            random_state=42
        )
        meta_model.fit(meta_X_scaled, y)
        logger.info("Retrained Meta-Model")

        return (sklearn_rf_model, sklearn_sgd_model), (pytorch_model, pytorch_feature_scaler, pytorch_target_scaler), xgb_model, meta_model
    except Exception as e:
        logger.error(f"Error retraining models: {e}", exc_info=True)
        return None, None, None, None

def _update_live_data_from_websocket(websocket_manager, main_ohlcv_df, temp_raw_trades_df):
    global ohlcv_df, raw_trades_for_ohlcv_df
    updated = False
    new_ohlcv_candles = pd.DataFrame()  # Initialize to prevent UnboundLocalError

    try:
        # Use ML_LOOKBACK_MINUTES from config
        lookback_minutes_param = max(config.ML_LOOKBACK_MINUTES, config.ML_LOOKBACK_PERIODS * 2)
        logger.info(f"Using lookback_minutes_param: {lookback_minutes_param} minutes")

        # Fetch new candles
        new_ohlcv_candles = websocket_manager.get_ws_candles(lookback_minutes_param=lookback_minutes_param)
        logger.info(f"Fetched {len(new_ohlcv_candles)} new candles from WebSocket")

        # Validate DataFrame
        if not isinstance(new_ohlcv_candles, pd.DataFrame):
            logger.warning("get_ws_candles did not return a DataFrame, initializing empty")
            new_ohlcv_candles = pd.DataFrame(columns=["timestamp", "open", "high", "low", "close", "volume", "trades"])

    except Exception as e:
        logger.error(f"Error fetching WebSocket candles: {e}", exc_info=True)
        # Fallback to exchange.fetch_ohlcv
        try:
            fetch_limit = config.BOLLINGER_WINDOW * 2
            new_ohlcv_candles = pd.DataFrame(
                exchange.fetch_ohlcv(
                    config.SYMBOL,
                    timeframe="1m",
                    limit=fetch_limit
                ),
                columns=["timestamp", "open", "high", "low", "close", "volume"]
            )
            new_ohlcv_candles["timestamp"] = pd.to_datetime(new_ohlcv_candles["timestamp"], unit="ms", utc=True)
            new_ohlcv_candles["trades"] = 0
            logger.info(f"Fetched {len(new_ohlcv_candles)} candles from exchange fallback")
        except Exception as e:
            logger.error(f"Fallback fetch_ohlcv failed: {e}")
            return ohlcv_df, raw_trades_for_ohlcv_df, updated

    try:
        if not new_ohlcv_candles.empty and len(new_ohlcv_candles) >= config.BOLLINGER_WINDOW:
            # Compute features
            new_ohlcv_candles["rsi"] = compute_rsi(new_ohlcv_candles["close"], periods=config.RSI_PERIOD)
            new_ohlcv_candles["ema"] = compute_ema(new_ohlcv_candles["close"], span=config.EMA_SPAN)
            new_ohlcv_candles["volatility"] = compute_volatility(new_ohlcv_candles, periods=config.VOLATILITY_WINDOW)
            new_ohlcv_candles["macd"], new_ohlcv_candles["macd_signal"] = compute_macd(
                new_ohlcv_candles["close"], fast=config.MACD_FAST, slow=config.MACD_SLOW, signal=config.MACD_SIGNAL
            )
            new_ohlcv_candles["bollinger_upper"], new_ohlcv_candles["bollinger_lower"] = compute_bollinger(
                new_ohlcv_candles["close"], window=config.BOLLINGER_WINDOW, num_std=config.BOLLINGER_NUM_STD
            )
            new_ohlcv_candles["momentum"] = compute_momentum(new_ohlcv_candles["close"], periods=config.MOMENTUM_PERIOD)
            new_ohlcv_candles["volume_trend"] = compute_volume_trend(new_ohlcv_candles["volume"], window=5)
            new_ohlcv_candles["atr"] = compute_atr(
                new_ohlcv_candles["high"], new_ohlcv_candles["low"], new_ohlcv_candles["close"], periods=config.ATR_PERIOD
            )
            new_ohlcv_candles["vwap"] = compute_vwap(new_ohlcv_candles, period=config.VWAP_PERIOD)
            new_ohlcv_candles = compute_additional_features(new_ohlcv_candles)
            new_ohlcv_candles["predicted_price"] = new_ohlcv_candles["close"].shift(-1).fillna(new_ohlcv_candles["close"].iloc[-1])
            new_ohlcv_candles["grid_level"] = 0

            # Fill NaNs for other features (volatility handled by compute_volatility)
            dynamic_defaults = {
                "rsi": new_ohlcv_candles["rsi"].mean() if not new_ohlcv_candles["rsi"].isna().all() else 50.0,
                "ema": new_ohlcv_candles["close"].mean(),
                "macd": 0.0,
                "macd_signal": 0.0,
                "bollinger_upper": new_ohlcv_candles["close"].mean(),
                "bollinger_lower": new_ohlcv_candles["close"].mean(),
                "momentum": 0.0,
                "volume_trend": 0.0,
                "atr": new_ohlcv_candles["close"].mean() * 0.01,
                "vwap": new_ohlcv_candles["close"].mean(),
                "price_spread": 0.0,
                "returns": 0.0,
                "volume_change": 0.0,
                "trade_intensity": 0.0,
                "predicted_price": new_ohlcv_candles["close"].iloc[-1],
                "grid_level": 0,
                "trades": 0
            }
            new_ohlcv_candles = new_ohlcv_candles.fillna(dynamic_defaults).ffill().bfill()

            # Update feature cache
            bot_state["feature_cache"] = pd.concat(
                [bot_state["feature_cache"], new_ohlcv_candles], ignore_index=True
            ).drop_duplicates(subset=["timestamp"], keep="last").tail(config.MAX_OHLCV_ROWS)
            logger.info(f"Updated feature cache: {len(bot_state['feature_cache'])} rows")

            ohlcv_df = pd.concat([main_ohlcv_df, new_ohlcv_candles], ignore_index=True).drop_duplicates(
                subset=["timestamp"], keep="last"
            )
            raw_trades_for_ohlcv_df = temp_raw_trades_df
            updated = True
        else:
            logger.warning("No new candles fetched or insufficient data, skipping feature update")
    except Exception as e:
        logger.error(f"Error processing candles: {e}")
        updated = False

    return ohlcv_df, raw_trades_for_ohlcv_df, updated

def fetch_trade_counts(exchange, symbol, historical_data=None, lookback_minutes=config.MIN_TRADE_MINUTES):
    try:
        logger.debug(f"Fetching trade data for {symbol} over {lookback_minutes} minutes")
        until = int(time.time() * 1000)
        since = until - (lookback_minutes * 60 * 1000)
        all_trades = []
        limit = 1000
        target_minutes = config.MIN_TRADE_MINUTES
        max_lookback = config.MAX_TRADE_LOOKBACK
        max_trades = config.MAX_TRADES
        max_retries = 7
        retry_count = 0
        minutes_covered = 0

        while len(all_trades) < max_trades and lookback_minutes <= max_lookback and retry_count < max_retries:
            trades = exchange.fetch_trades(symbol, since=since, limit=limit, params={"until": until})
            logger.debug(
                f"Fetched {len(trades)} trades from {pd.Timestamp(since, unit='ms', tz='UTC')} to {pd.Timestamp(until, unit='ms', tz='UTC')}"
            )
            if not trades:
                logger.warning("No trades returned, increasing lookback")
                lookback_minutes = min(lookback_minutes * 1.5, max_lookback)
                since = until - (lookback_minutes * 60 * 1000)
                retry_count += 1
                time.sleep(exchange.rateLimit / 1000)
                continue
            all_trades.extend(trades)
            earliest_timestamp = min(t["timestamp"] for t in trades)
            until = earliest_timestamp - 1000
            minutes_covered = (time.time() * 1000 - earliest_timestamp) / (60 * 1000)
            logger.debug(f"Minutes covered: {minutes_covered:.1f}")
            if minutes_covered >= target_minutes:
                logger.info(f"Reached target minutes: {minutes_covered:.1f}")
                break
            if until <= since:
                logger.info("Reached start of requested period")
                break
            time.sleep(exchange.rateLimit / 1000)

        if minutes_covered < target_minutes and lookback_minutes < max_lookback and retry_count < max_retries:
            logger.info(f"Minutes covered {minutes_covered:.1f} < {target_minutes}, retrying with extended lookback")
            lookback_minutes = min(lookback_minutes * 1.5, max_lookback)
            since = until - (lookback_minutes * 60 * 1000)
            additional_trades = exchange.fetch_trades(symbol, since=since, limit=limit, params={"until": until})
            if additional_trades:
                all_trades.extend(additional_trades)
                logger.debug(f"Added {len(additional_trades)} additional trades")
                earliest_timestamp = min(t["timestamp"] for t in all_trades)
                minutes_covered = (time.time() * 1000 - earliest_timestamp) / (60 * 1000)

        logger.info(f"Total trades fetched: {len(all_trades)} over {lookback_minutes} minutes")
        if len(all_trades) < config.MIN_TRADES:
            logger.warning(f"Fetched only {len(all_trades)} trades, may not cover enough time")
            with trade_lock:
                if trades_cache:
                    logger.info("Using WebSocket cache to supplement trades")
                    cache_trades = [
                        {
                            "timestamp": t["timestamp"],
                            "price": t["price"],
                            "amount": t["size"],
                        }
                        for t in trades_cache
                        if since <= t["timestamp"] <= until
                    ]
                    all_trades.extend(cache_trades)
                    logger.debug(f"Added {len(cache_trades)} cached trades")

        # Validate trade data structure and log sample
        required_keys = ["timestamp", "price", "amount"]
        sample_trade = all_trades[0] if all_trades else {}
        missing_keys = [key for key in required_keys if key not in sample_trade]
        if missing_keys:
            logger.error(f"Trade data missing required keys: {missing_keys}, sample trade: {sample_trade}")
            return None
        logger.debug(f"Sample trades: {all_trades[:5]}")

        # Log raw timestamp range
        raw_timestamps = [t["timestamp"] for t in all_trades]
        logger.debug(
            f"Raw timestamp range: min={min(raw_timestamps, default='N/A')}, max={max(raw_timestamps, default='N/A')}, sample={raw_timestamps[:5]}"
        )

        df = pd.DataFrame(all_trades, columns=["timestamp", "price", "amount"])
        if df.empty:
            logger.error("No trade data after processing")
            return None

        # Log DataFrame state before timestamp conversion
        logger.debug(
            f"Trade DataFrame before timestamp conversion: columns={df.columns.tolist()}, first_timestamp={df['timestamp'].iloc[0] if 'timestamp' in df and not df.empty else 'N/A'}"
        )

        if "timestamp" not in df:
            logger.error("Timestamp column missing in trade DataFrame")
            return None

        # Parse timestamps
        df["timestamp"] = pd.to_datetime(df["timestamp"], unit="ms", errors="coerce", utc=True)
        if df["timestamp"].isna().all():
            logger.warning("Millisecond timestamp parsing failed, trying seconds")
            # Fallback to inferring format if unit-based parsing fails
            df["timestamp"] = pd.to_datetime(df["timestamp"], errors="coerce", utc=True)
        if df["timestamp"].isna().any():
            logger.warning(
                f"Found {df['timestamp'].isna().sum()} invalid timestamps in trade data, filling with current time"
            )
            df["timestamp"] = df["timestamp"].fillna(pd.Timestamp.now(tz="UTC"))

        # Validate timestamp recency
        current_time = pd.Timestamp.now(tz="UTC")
        if df["timestamp"].max() < current_time - pd.Timedelta(minutes=lookback_minutes):
            logger.warning(f"Timestamps are older than expected, max timestamp: {df['timestamp'].max()}")

        # Log parsed timestamp range
        logger.debug(
            f"Parsed timestamp range: min={df['timestamp'].min()}, max={df['timestamp'].max()}, sample={df['timestamp'].head().tolist()}"
        )

        df["minute"] = df["timestamp"].dt.floor("1min")
        trade_counts = df.groupby("minute").size().reset_index(name="trades")
        trade_counts.rename(columns={"minute": "timestamp"}, inplace=True)
        price_agg = df.groupby("minute").agg({"price": ["last", "max", "min"], "amount": "sum"}).reset_index()
        price_agg.columns = ["timestamp", "close", "high", "low", "volume"]
        trade_counts = trade_counts.merge(price_agg, on="timestamp", how="left")
        trade_counts["close"] = trade_counts["close"].ffill()
        trade_counts["volume"] = trade_counts["volume"].ffill()
        trade_counts["high"] = trade_counts["high"].ffill()
        trade_counts["low"] = trade_counts["low"].ffill()
        logger.debug(
            f"Aggregated trade data: {len(trade_counts)} minutes, total volume={trade_counts['volume'].sum():.2f}, columns={trade_counts.columns.tolist()}"
        )

        if trade_counts["volume"].sum() == 0 and historical_data is not None and not historical_data.empty:
            last_volume = historical_data["volume"].tail(1).iloc[0]
            trade_counts["volume"] = last_volume
            logger.warning(f"No volume in fetched trades, using historical volume: {last_volume:.2f}")
        elif trade_counts["volume"].sum() == 0:
            logger.warning("No volume in fetched trades and no historical data available, setting volume to 0.0")

        # Compute features individually with error handling
        try:
            logger.debug("Computing rsi")
            trade_counts["rsi"] = compute_rsi(trade_counts["close"], periods=config.RSI_PERIOD)
        except Exception as e:
            logger.error(f"Error computing rsi: {e}")
            trade_counts["rsi"] = config.LOG_DEFAULT_RSI
        try:
            logger.debug("Computing ema")
            trade_counts["ema"] = compute_ema(trade_counts["close"], span=config.EMA_SPAN)
        except Exception as e:
            logger.error(f"Error computing ema: {e}")
            trade_counts["ema"] = trade_counts["close"].mean()
        try:
            logger.debug("Computing volatility")
            trade_counts["volatility"] = (
                trade_counts["close"].pct_change().rolling(config.VOLATILITY_WINDOW, min_periods=1).std() * 100
            )
            trade_counts["volatility"] = trade_counts["volatility"].fillna(config.LOG_DEFAULT_VOLATILITY)
        except Exception as e:
            logger.error(f"Error computing volatility: {e}")
            trade_counts["volatility"] = config.LOG_DEFAULT_VOLATILITY
        try:
            logger.debug("Computing macd")
            trade_counts["macd"], trade_counts["macd_signal"] = compute_macd(
                trade_counts["close"],
                fast=config.MACD_FAST,
                slow=config.MACD_SLOW,
                signal=config.MACD_SIGNAL,
            )
        except Exception as e:
            logger.error(f"Error computing macd: {e}")
            trade_counts["macd"] = config.LOG_DEFAULT_MACD
            trade_counts["macd_signal"] = config.LOG_DEFAULT_MACD_SIGNAL
        try:
            logger.debug("Computing bollinger bands")
            trade_counts["bollinger_upper"], trade_counts["bollinger_lower"] = compute_bollinger(
                trade_counts["close"],
                window=config.BOLLINGER_WINDOW,
                num_std=config.BOLLINGER_NUM_STD,
            )
            trade_counts["bollinger_upper"] = trade_counts["bollinger_upper"].fillna(trade_counts["close"].mean())
            trade_counts["bollinger_lower"] = trade_counts["bollinger_lower"].fillna(trade_counts["close"].mean())
        except Exception as e:
            logger.error(f"Error computing bollinger bands: {e}")
            trade_counts["bollinger_upper"] = trade_counts["close"].mean()
            trade_counts["bollinger_lower"] = trade_counts["close"].mean()
        try:
            logger.debug("Computing momentum")
            trade_counts["momentum"] = compute_momentum(trade_counts["close"], periods=config.MOMENTUM_PERIOD)
        except Exception as e:
            logger.error(f"Error computing momentum: {e}")
            trade_counts["momentum"] = config.LOG_DEFAULT_MOMENTUM
        try:
            logger.debug("Computing volume trend")
            trade_counts["volume_trend"] = compute_volume_trend(trade_counts["volume"])
        except Exception as e:
            logger.error(f"Error computing volume trend: {e}")
            trade_counts["volume_trend"] = config.LOG_DEFAULT_VOLUME_TREND
        try:
            logger.debug("Computing atr")
            trade_counts["atr"] = compute_atr(
                trade_counts["high"],
                trade_counts["low"],
                trade_counts["close"],
                periods=config.ATR_PERIOD,
            )
        except Exception as e:
            logger.error(f"Error computing atr: {e}")
            trade_counts["atr"] = trade_counts["close"].mean() * 0.01
        try:
            logger.debug("Computing vwap")
            trade_counts["vwap"] = compute_vwap(trade_counts, period=config.VWAP_PERIOD)
        except Exception as e:
            logger.error(f"Error computing vwap: {e}")
            trade_counts["vwap"] = trade_counts["close"].mean()
        try:
            logger.debug("Computing predicted price")
            trade_counts["predicted_price"] = trade_counts["close"].shift(-1).fillna(trade_counts["close"].mean())
        except Exception as e:
            logger.error(f"Error computing predicted price: {e}")
            trade_counts["predicted_price"] = trade_counts["close"].mean()
        trade_counts["grid_level"] = 0

        # Log trade_counts state before final assignment
        logger.debug(f"trade_counts after feature computation: {trade_counts.head().to_dict()}")

        result = trade_counts[
            [
                "timestamp",
                "close",
                "volume",
                "trades",
                "rsi",
                "ema",
                "volatility",
                "macd",
                "macd_signal",
                "bollinger_upper",
                "bollinger_lower",
                "momentum",
                "volume_trend",
                "atr",
                "vwap",
                "predicted_price",
                "grid_level",
                "high",
                "low",
            ]
        ].copy()
        result = (
            result.bfill()
            .ffill()
            .fillna(
                {
                    "rsi": config.LOG_DEFAULT_RSI,
                    "ema": result["close"].mean(),
                    "volatility": config.LOG_DEFAULT_VOLATILITY,
                    "macd": config.LOG_DEFAULT_MACD,
                    "macd_signal": config.LOG_DEFAULT_MACD_SIGNAL,
                    "bollinger_upper": result["close"].mean(),
                    "bollinger_lower": result["close"].mean(),
                    "momentum": config.LOG_DEFAULT_MOMENTUM,
                    "volume_trend": config.LOG_DEFAULT_VOLUME_TREND,
                    "atr": result["close"].mean() * 0.01,
                    "vwap": result["close"].mean(),
                    "predicted_price": result["close"].mean(),
                    "grid_level": 0,
                    "trades": config.DEFAULT_TRADE_COUNT,
                    "high": result["close"].mean(),
                    "low": result["close"].mean(),
                }
            )
        )

        minutes_covered = (result["timestamp"].max() - result["timestamp"].min()).total_seconds() / 60
        logger.info(f"Trades cover {minutes_covered:.1f} minutes")
        if minutes_covered < target_minutes:
            logger.warning(f"Fetched only {minutes_covered:.1f} minutes of trade data, needed {target_minutes}")

        logger.info(f"Fetched {len(result)} minutes of trade data")
        return result
    except Exception as e:
        logger.error(
            f"Error fetching trade counts: {e}, trade_counts state: {trade_counts.head().to_dict() if 'trade_counts' in locals() and not trade_counts.empty else 'N/A'}, DataFrame state: {df.head().to_dict() if 'df' in locals() and not df.empty else 'N/A'}"
        )
        return None

def compute_macd(data, fast=12, slow=26, signal=9):
    """Compute MACD and signal line, returning filled series."""
    try:
        exp1 = pd.Series(data).ewm(span=fast, adjust=False).mean()
        exp2 = pd.Series(data).ewm(span=slow, adjust=False).mean()
        macd = exp1 - exp2
        macd_signal = macd.ewm(span=signal, adjust=False).mean()
        macd = macd.fillna(0)
        macd_signal = macd_signal.fillna(0)
        logger.info(
            f"MACD computed: mean={macd.mean():.4f}, std={macd.std():.4f}, "
            f"signal_mean={macd_signal.mean():.4f}, signal_std={macd_signal.std():.4f}"
        )
        return macd, macd_signal
    except Exception as e:
        logger.error(f"Error computing MACD: {e}")
        return pd.Series(0, index=pd.Series(data).index), pd.Series(0, index=pd.Series(data).index)

def compute_bollinger(data, window=config.BOLLINGER_WINDOW, num_std=config.BOLLINGER_NUM_STD):
    """Compute Bollinger Bands, returning upper and lower bands with filled series."""
    try:
        series = pd.Series(data)
        if series.empty:
            logger.warning("Input data for Bollinger Bands is empty. Returning empty series.")
            return pd.Series(dtype=float), pd.Series(dtype=float)

        # Step 1: Handle NaNs in the input series
        if series.isna().any():
            mean_fill_value = series.mean()
            if pd.isna(mean_fill_value):  # Handles case where series might be all NaNs
                logger.warning("Input data for Bollinger Bands is all NaNs or empty after pd.Series. Filling with 0.0.")
                mean_fill_value = 0.0
            logger.warning(f"NaN values in input data for Bollinger, filling with mean/0 ({mean_fill_value:.4f}): {series.isna().sum()}")
            series = series.fillna(mean_fill_value)
        
        # Step 2: Calculate rolling mean and std with min_periods=1
        # Ensure window is not larger than the series length to avoid issues with rolling
        effective_window = min(window, len(series))
        if effective_window < 1: # Should only happen if series was empty and somehow passed initial check
             effective_window = 1 # Default to 1 to prevent error with rolling window 0

        rolling_mean = series.rolling(window=effective_window, min_periods=1).mean()
        rolling_std = series.rolling(window=effective_window, min_periods=1).std()

        # Step 3: Fill NaNs in rolling_std (especially the first value which is NaN if window > 1, or if std is 0 for a window)
        # Filling with 0.0 implies no deviation initially or for constant price windows.
        rolling_std = rolling_std.fillna(0.0)

        # Step 4: Calculate bands
        upper = rolling_mean + (rolling_std * num_std)
        lower = rolling_mean - (rolling_std * num_std)

        # Step 5: Final check for NaNs (should be rare if steps above are correct)
        # This handles any residual NaNs, perhaps if rolling_mean itself became NaN (e.g., if series was empty and fillna(0.0) resulted in 0/0 for mean)
        if upper.isna().any() or lower.isna().any():
            logger.warning("Unexpected NaN values in Bollinger Bands after calculation. Applying series mean fallback.")
            # series.mean() uses the already NaN-filled series from Step 1
            fallback_mean = series.mean() 
            if pd.isna(fallback_mean): 
                logger.error("Series mean is NaN during Bollinger fallback. Input data might be problematic. Defaulting bands to 0.")
                fallback_mean = 0.0 # Absolute fallback
            
            upper = upper.fillna(fallback_mean)
            lower = lower.fillna(fallback_mean)
        
        # Ensure no NaNs are returned, critical for consumers of this function
        if upper.isna().any() or lower.isna().any():
            logger.error("CRITICAL: Bollinger Bands still contain NaNs after all fallbacks. Returning series of zeros.")
            # Create series with the same index as the input to maintain alignment
            zero_series = pd.Series(0.0, index=series.index)
            return zero_series, zero_series

        logger.info(
            f"Bollinger Bands computed: upper_mean={upper.mean():.4f}, upper_std={upper.std():.4f}, "
            f"lower_mean={lower.mean():.4f}, lower_std={lower.std():.4f}"
        )
        return upper, lower
    except Exception as e:
        logger.error(f"Error computing Bollinger Bands: {e}")
        # Fallback in case of any other exception during computation
        try:
            s_data = pd.Series(data) # Ensure data is a Series for .mean() and .index
            mean_value = s_data.mean()
            if pd.isna(mean_value): # if data was all NaNs or empty
                mean_value = 0.0
            # Return Series with the same index as the input data
            return pd.Series(mean_value, index=s_data.index), pd.Series(mean_value, index=s_data.index)
        except: # Broad except for the fallback's own potential failure
            # If even creating a series from data fails, return empty series of appropriate dtype
            return pd.Series(dtype=float), pd.Series(dtype=float)

def compute_momentum(data, periods=config.MOMENTUM_PERIOD):
    """Compute Momentum, returning filled series."""
    try:
        series = pd.Series(data)
        if series.isna().any():
            logger.warning(f"NaN values in input data, filling with mean: {series.isna().sum()}")
            series = series.fillna(series.mean())
        momentum = series.diff(periods).fillna(0)
        if momentum.isna().any():
            logger.warning("NaN values in momentum after computation, filling with 0")
            momentum = momentum.fillna(0)
        logger.info(f"Momentum computed: mean={momentum.mean():.4f}, std={momentum.std():.4f}")
        return momentum
    except Exception as e:
        logger.error(f"Error computing Momentum: {e}")
        return pd.Series(0, index=pd.Series(data).index)

def compute_volume_trend(data, window=config.VOLATILITY_WINDOW):
    """Compute Volume Trend, returning filled series."""
    try:
        series = pd.Series(data)
        if series.isna().any():
            logger.warning(f"NaN values in input data, filling with mean: {series.isna().sum()}")
            series = series.fillna(series.mean())
        volume_trend = series.pct_change().rolling(window=window).mean().fillna(0)
        if volume_trend.isna().any():
            logger.warning("NaN values in volume trend after computation, filling with 0")
            volume_trend = volume_trend.fillna(0)
        logger.info(f"Volume Trend computed: mean={volume_trend.mean():.4f}, std={volume_trend.std():.4f}")
        return volume_trend
    except Exception as e:
        logger.error(f"Error computing Volume Trend: {e}")
        return pd.Series(0, index=pd.Series(data).index)

def compute_rsi(data, periods=config.RSI_PERIOD):
    """Compute Relative Strength Index (RSI), returning filled series."""
    try:
        series = pd.Series(data)
        if series.isna().any():
            logger.warning(f"NaN values in input data, filling with mean: {series.isna().sum()}")
            series = series.fillna(series.mean())
        delta = series.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=periods).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=periods).mean()
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))
        rsi = rsi.fillna(50.0)
        if rsi.isna().any():
            logger.warning("NaN values in RSI after computation, filling with 50.0")
            rsi = rsi.fillna(50.0)
        logger.info(f"RSI computed: mean={rsi.mean():.4f}, std={rsi.std():.4f}")
        return rsi
    except Exception as e:
        logger.error(f"Error computing RSI: {e}")
        return pd.Series(50.0, index=pd.Series(data).index)

def compute_ema(data, span=config.EMA_SPAN):
    """Compute Exponential Moving Average (EMA), returning filled series."""
    try:
        series = pd.Series(data)
        if series.isna().any():
            logger.warning(f"NaN values in input data, filling with mean: {series.isna().sum()}")
            series = series.fillna(series.mean())
        ema = series.ewm(span=span, adjust=False).mean()
        ema = ema.fillna(series.mean())
        if ema.isna().any():
            logger.warning("NaN values in EMA after computation, filling with series mean")
            ema = ema.fillna(series.mean())
        logger.info(f"EMA computed: mean={ema.mean():.4f}, std={ema.std():.4f}")
        return ema
    except Exception as e:
        logger.error(f"Error computing EMA: {e}")
        mean_value = pd.Series(data).mean()
        return pd.Series(mean_value, index=pd.Series(data).index)

def compute_atr(high, low, close, periods=config.ATR_PERIOD):
    """Compute Average True Range (ATR), returning filled series."""
    try:
        high = pd.Series(high)
        low = pd.Series(low)
        close = pd.Series(close)
        if high.isna().any() or low.isna().any() or close.isna().any():
            logger.warning(f"NaN values in inputs: high={high.isna().sum()}, low={low.isna().sum()}, close={close.isna().sum()}")
            high = high.fillna(high.mean())
            low = low.fillna(low.mean())
            close = close.fillna(close.mean())
        tr1 = high - low
        tr2 = (high - close.shift(1)).abs()
        tr3 = (low - close.shift(1)).abs()
        tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
        atr = tr.rolling(window=periods).mean()
        atr = atr.fillna(atr.mean())
        if atr.isna().any():
            logger.warning("NaN values in ATR after computation, filling with close mean * 0.01")
            atr = atr.fillna(close.mean() * 0.01)
        logger.info(f"ATR computed: mean={atr.mean():.4f}, std={atr.std():.4f}")
        return atr
    except Exception as e:
        logger.error(f"Error computing ATR: {e}")
        return pd.Series(0, index=pd.Series(close).index)

def compute_vwap(data, period=config.VWAP_PERIOD):
    """Compute Volume Weighted Average Price (VWAP), returning filled series."""
    try:
        if not isinstance(data, pd.DataFrame):
            raise ValueError("Input must be a pandas DataFrame")
        required_columns = ["close", "volume", "timestamp"]
        missing_columns = [col for col in required_columns if col not in data.columns]
        if missing_columns:
            raise ValueError(f"Missing required columns: {missing_columns}")

        df = data[["close", "volume", "timestamp"]].copy()
        if df[["close", "volume"]].isna().any().any():
            logger.warning(
                f"NaN values in inputs: close={df['close'].isna().sum()}, volume={df['volume'].isna().sum()}"
            )
            df["close"] = df["close"].fillna(df["close"].mean())
            df["volume"] = df["volume"].fillna(df["volume"].mean())

        if df["timestamp"].isna().any():
            logger.warning(f"NaN values in timestamp: {df['timestamp'].isna().sum()}, filling with current time")
            df["timestamp"] = df["timestamp"].fillna(pd.Timestamp.now(tz="UTC"))

        df["date"] = df["timestamp"].dt.date
        df["price_volume"] = df["close"] * df["volume"]
        df["cum_price_volume"] = df.groupby("date")["price_volume"].cumsum()
        df["cum_volume"] = df.groupby("date")["volume"].cumsum()

        # Avoid division by zero
        df["vwap"] = df["cum_price_volume"] / df["cum_volume"].replace(0, np.nan)
        vwap = df["vwap"].rolling(window=period, min_periods=1).mean()
        vwap = vwap.fillna(df["close"].mean())
        
        if vwap.isna().any():
            logger.warning("NaN values in VWAP after computation, filling with close mean")
            vwap = vwap.fillna(df["close"].mean())

        logger.info(f"VWAP computed: mean={vwap.mean():.4f}, std={vwap.std():.4f}")
        return vwap
    except Exception as e:
        logger.error(f"Error computing VWAP: {e}")
        mean_value = pd.Series(data["close"]).mean() if isinstance(data, pd.DataFrame) and "close" in data else 0.0
        return pd.Series(mean_value, index=data.index if isinstance(data, pd.DataFrame) else range(len(data)))
    
def compute_volatility(ohlcv_df, periods=14, timeframe_minutes=1):
    """
    Compute advanced volatility using an ensemble of Parkinson, Garman-Klass, Yang-Zhang, and realized volatility,
    with robust NaN handling, dynamic weighting, and volume/trade adjustments for accuracy.

    Args:
        ohlcv_df (pd.DataFrame): DataFrame with 'open', 'high', 'low', 'close', 'volume', 'trades' columns.
        periods (int): Lookback period for volatility calculation (default: 14).
        timeframe_minutes (int): Timeframe of each candle in minutes (default: 1 for 1-minute candles).

    Returns:
        pd.Series: Volatility values (annualized, no NaNs), aligned with ohlcv_df index, scaled to ~0.03-0.04.
    """
    # Validate and preprocess inputs
    required_columns = ['open', 'high', 'low', 'close', 'volume', 'trades']
    if not all(col in ohlcv_df.columns for col in required_columns):
        logger.error(f"Missing required columns in ohlcv_df: {required_columns}")
        return pd.Series(0.03, index=ohlcv_df.index)

    # Copy to avoid modifying input
    df = ohlcv_df[required_columns].copy()

    # Fill missing values with rolling means or defaults
    for col in ['open', 'high', 'low', 'close']:
        df[col] = df[col].fillna(df[col].rolling(window=periods, min_periods=1).mean()).fillna(df['close'].mean())
    df['volume'] = df['volume'].fillna(df['volume'].rolling(window=periods, min_periods=1).mean()).fillna(0.0)
    df['trades'] = df['trades'].fillna(df['trades'].rolling(window=periods, min_periods=1).mean()).fillna(0.0)

    # Ensure positive values to prevent log errors
    for col in ['high', 'low', 'close', 'open']:
        df[col] = df[col].clip(lower=1e-6)

    # Extract series
    close = df['close']
    high = df['high']
    low = df['low']
    open_ = df['open']
    volume = df['volume']
    trades = df['trades']

    # 1. Parkinson Volatility
    hl_ratio = np.log(high / low)
    parkinson_vol = hl_ratio.rolling(window=periods, min_periods=1).std() * np.sqrt(365 * 1440 / timeframe_minutes / 0.361)

    # 2. Garman-Klass Volatility
    hl_term = 0.5 * np.log(high / low) ** 2
    oc_term = (2 * np.log(2) - 1) * np.log(close / open_) ** 2
    gk_vol = np.sqrt(hl_term - oc_term).rolling(window=periods, min_periods=1).mean() * np.sqrt(365 * 1440 / timeframe_minutes)

    # 3. Yang-Zhang Volatility
    oc_vol = np.log(open_ / close.shift(1))
    cc_vol = np.log(close / open_)
    yz_vol = np.sqrt(
        oc_vol.rolling(window=periods, min_periods=1).var() +
        0.164 * cc_vol.rolling(window=periods, min_periods=1).var() +
        0.836 * hl_ratio.rolling(window=periods, min_periods=1).var()
    ) * np.sqrt(365 * 1440 / timeframe_minutes)

    # 4. Log-Return Volatility
    returns = np.log(close / close.shift(1))
    log_vol = returns.rolling(window=periods, min_periods=1).std() * np.sqrt(365 * 1440 / timeframe_minutes)

    # 5. Realized Volatility (trade-based)
    trade_intensity = trades / trades.rolling(window=periods, min_periods=1).mean()
    realized_vol = (returns ** 2).rolling(window=periods, min_periods=1).mean() * trade_intensity * np.sqrt(365 * 1440 / timeframe_minutes)

    # 6. ATR Adjustment
    atr = compute_atr(high, low, close, periods=periods)
    atr_vol = atr / close.rolling(window=periods, min_periods=1).mean() * np.sqrt(365 * 1440 / timeframe_minutes)

    # Dynamic weighting based on volume
    volume_norm = volume / volume.rolling(window=periods, min_periods=1).mean()
    high_activity = (volume_norm > 1.5).astype(float)
    weights = {
        'parkinson': 0.3 + 0.1 * high_activity,
        'gk': 0.3 + 0.05 * high_activity,
        'yz': 0.2,
        'log': 0.1 - 0.05 * high_activity,
        'realized': 0.1 - 0.1 * high_activity
    }
    weights_sum = sum(weights.values())
    for key in weights:
        weights[key] = weights[key] / weights_sum

    # Volume-weighted scaling
    activity_weight = (0.6 * volume_norm + 0.4 * trade_intensity).clip(lower=0.5, upper=1.5)

    # Ensemble
    ensemble_vol = (
        weights['parkinson'] * parkinson_vol +
        weights['gk'] * gk_vol +
        weights['yz'] * yz_vol +
        weights['log'] * log_vol +
        weights['realized'] * realized_vol
    ) * activity_weight

    # ATR blend (10%)
    ensemble_vol = 0.9 * ensemble_vol + 0.1 * atr_vol

    # Robust NaN handling
    if ensemble_vol.isna().any():
        mean_vol = ensemble_vol.mean() if not ensemble_vol.isna().all() else 0.035
        ensemble_vol = ensemble_vol.fillna(mean_vol)

    # Clip negative values
    ensemble_vol = ensemble_vol.clip(lower=0.0)

    # Scale to target range (~0.03-0.04)
    target_mean = 0.035
    current_mean = ensemble_vol.mean()
    if current_mean > 0:
        ensemble_vol = ensemble_vol * (target_mean / current_mean)

    # Log statistics
    logger.info(
        f"Volatility computed: mean={ensemble_vol.mean():.4f}, std={ensemble_vol.std():.4f}, "
        f"parkinson_mean={parkinson_vol.mean():.4f}, gk_mean={gk_vol.mean():.4f}, "
        f"yz_mean={yz_vol.mean():.4f}, log_mean={log_vol.mean():.4f}, realized_mean={realized_vol.mean():.4f}"
    )

    return ensemble_vol

def train_meta_model(prediction_history, min_samples=1):
    """
    Train meta-model using base model predictions.
    Args:
        prediction_history (pd.DataFrame): History of predictions and volatility.
        min_samples (int): Minimum samples required for training.
    Returns:
        tuple: (meta_model, meta_scaler)
    """
    try:
        logger.debug(f"Prediction history: rows={len(prediction_history) if prediction_history is not None else 0}, columns={prediction_history.columns.tolist() if prediction_history is not None else []}, sample={prediction_history.tail(1).to_dict() if prediction_history is not None and not prediction_history.empty else {}}")
        if prediction_history is None or len(prediction_history) < min_samples:
            logger.warning(f"Insufficient data for meta-model training: {len(prediction_history) if prediction_history is not None else 0} samples, need {min_samples}")
            return None, None

        features = ["sklearn_pred", "pytorch_pred", "xgb_pred", "volatility"]
        target = "actual_price"
        if not all(col in prediction_history.columns for col in features + [target]):
            logger.error(f"Missing required columns in prediction_history: {prediction_history.columns}")
            return None, None

        X = prediction_history[features].copy()
        y = prediction_history[target].copy()

        # Handle NaN values
        X = X.fillna(X.mean())
        y = y.fillna(y.mean())
        if X.isna().any().any() or y.isna().any():
            logger.error("NaN values persist in meta-model data after filling")
            return None, None

        # Initialize and fit scaler
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X.values)  # Use .values to avoid feature names
        logger.debug(f"Scaler fitted: scale_={scaler.scale_.tolist()}, mean_={scaler.mean_.tolist()}")

        model = GradientBoostingRegressor(n_estimators=50, max_depth=3, random_state=42)
        model.fit(X_scaled, y)

        y_pred = model.predict(X_scaled)
        mse = mean_squared_error(y, y_pred)
        logger.info(f"Meta-model trained: Train MSE={mse:.6f}")

        # Save scaler
        joblib.dump(scaler, "client_meta_scaler.pkl")
        logger.info(f"Saved meta-model scaler to client_meta_scaler.pkl with scale_={scaler.scale_.tolist()}")

        return model, scaler
    except Exception as e:
        logger.error(f"Error training meta-model: {e}")
        return None, None

def predict_meta_model(meta_model, meta_scaler, sklearn_pred, pytorch_pred, xgb_pred, volatility, current_close):
    """
    Predict using the trained meta-model, accumulating prediction history, retrying training if needed, tracking accuracy, and adjusting config parameters.
    Args:
        meta_model: Trained GradientBoostingRegressor model.
        meta_scaler: StandardScaler for input features.
        sklearn_pred (float): Sklearn model prediction.
        pytorch_pred (float): PyTorch model prediction.
        xgb_pred (float): XGBoost model prediction.
        volatility (float): Current volatility.
        current_close (float): Current close price for fallback and accuracy tracking.
    Returns:
        float: Meta-model prediction or mean of base predictions if prediction fails.
    """
    try:
        logger.info(f"Starting meta-model prediction: sklearn_pred={sklearn_pred:.2f}, pytorch_pred={pytorch_pred:.2f}, xgb_pred={xgb_pred:.2f}, volatility={volatility:.2f}, current_close={current_close:.2f}")
        valid_preds = [p for p in [sklearn_pred, pytorch_pred, xgb_pred] if np.isfinite(p)]
        fallback = np.mean(valid_preds) if valid_preds else current_close

        # Accumulate prediction history
        new_entry = pd.DataFrame(
            {
                "sklearn_pred": [sklearn_pred if np.isfinite(sklearn_pred) else current_close],
                "pytorch_pred": [pytorch_pred if np.isfinite(pytorch_pred) else current_close],
                "xgb_pred": [xgb_pred if np.isfinite(xgb_pred) else current_close],
                "volatility": [volatility if np.isfinite(volatility) else 0.1],
                "current_price": [current_close],
                "actual_price": [current_close]  # Use current_close as proxy for actual_price
            }
        )
        bot_state["prediction_history"] = pd.concat(
            [bot_state.get("prediction_history", pd.DataFrame(columns=new_entry.columns)), new_entry],
            ignore_index=True
        ).tail(1000)  # Keep last 1000 entries
        logger.info(f"Accumulated prediction_history: rows={len(bot_state['prediction_history'])}, latest={new_entry.to_dict('records')[0]}")

        # Track prediction accuracy
        logger.info("Calculating base model prediction errors")
        errors = {
            "sklearn": abs(sklearn_pred - current_close) if np.isfinite(sklearn_pred) else float("inf"),
            "pytorch": abs(pytorch_pred - current_close) if np.isfinite(pytorch_pred) else float("inf"),
            "xgb": abs(xgb_pred - current_close) if np.isfinite(xgb_pred) else float("inf")
        }
        logger.info(f"Base model errors: sklearn={errors['sklearn']:.2f}, pytorch={errors['pytorch']:.2f}, xgb={errors['xgb']:.2f}")
        bot_state.setdefault("base_model_errors", []).append(errors)
        bot_state["base_model_errors"] = bot_state["base_model_errors"][-100:]  # Keep last 100 errors

        # Adjust config parameters based on accuracy
        if len(bot_state["base_model_errors"]) >= 10:  # Require sufficient data
            mean_errors = {
                model: np.mean([e[model] for e in bot_state["base_model_errors"]])
                for model in ["sklearn", "pytorch", "xgb"]
            }
            total_error = sum(mean_errors.values())
            if total_error > 0:
                weights = {
                    model: (1 / mean_errors[model]) / sum(1 / e for e in mean_errors.values())
                    for model in mean_errors
                }
                config.SKLEARN_WEIGHT = weights["sklearn"]
                config.PYTORCH_WEIGHT = weights["pytorch"]
                config.XGB_WEIGHT = weights["xgb"]
                logger.info(f"Adjusted config weights: sklearn={config.SKLEARN_WEIGHT:.3f}, pytorch={config.PYTORCH_WEIGHT:.3f}, xgb={config.XGB_WEIGHT:.3f}")

        if any(pred is None or not np.isfinite(pred) for pred in [sklearn_pred, pytorch_pred, xgb_pred, volatility]):
            logger.error(f"Invalid input: sklearn_pred={sklearn_pred}, pytorch_pred={pytorch_pred}, xgb_pred={xgb_pred}, volatility={volatility}")
            return fallback

        meta_input = np.array([[sklearn_pred, pytorch_pred, xgb_pred, volatility]])
        if np.any(np.isnan(meta_input)):
            logger.error(f"NaN values in meta-model input: {meta_input}")
            return fallback

        # Load saved model/scaler or retry training if not initialized or not fitted
        if meta_model is None or meta_scaler is None or not hasattr(meta_scaler, "scale_"):
            logger.info(f"Meta-model/scaler issue detected, attempting retry or load: meta_model={'meta_model' in bot_state}, meta_scaler={'meta_scaler' in bot_state}, scaler_fitted={hasattr(meta_scaler, 'scale_') if meta_scaler is not None else False}")
            history_rows = len(bot_state.get("prediction_history", pd.DataFrame()))
            if history_rows >= 1:  # Retry training if sufficient history
                try:
                    meta_model, meta_scaler = train_meta_model(bot_state["prediction_history"], min_samples=1)
                    if meta_model is not None and meta_scaler is not None and hasattr(meta_scaler, "scale_"):
                        bot_state["meta_model"] = meta_model
                        bot_state["meta_scaler"] = meta_scaler
                        logger.info(f"Retrained meta-model with {history_rows} prediction_history rows")
                    else:
                        logger.error(f"Meta-model training failed; attempting to load saved files")
                except Exception as train_error:
                    logger.error(f"Meta-model training retry failed: {train_error}")
            else:
                logger.warning(f"Insufficient prediction_history for training: {history_rows} rows, need 1")

            # Load saved files as fallback
            try:
                if os.path.exists("client_meta_model.pkl") and os.path.exists("client_meta_scaler.pkl"):
                    meta_model = joblib.load("client_meta_model.pkl")
                    meta_scaler = joblib.load("client_meta_scaler.pkl")
                    logger.info(f"Loaded saved meta-model and scaler from client_meta_model.pkl and client_meta_scaler.pkl")
                    if not hasattr(meta_scaler, "scale_"):
                        logger.error(f"Loaded meta-model scaler is not fitted; base predictions: sklearn={sklearn_pred:.2f}, pytorch={pytorch_pred:.2f}, xgb={xgb_pred:.2f}, volatility={volatility:.2f}, fallback={fallback:.2f}, prediction_history_rows={history_rows}, meta_model_initialized={'meta_model' in bot_state}")
                        return fallback
                else:
                    history_sample = bot_state["prediction_history"].tail(1).to_dict() if history_rows > 0 else {}
                    logger.error(f"Meta-model or scaler not initialized/fitted, saved files not found; base predictions: sklearn={sklearn_pred:.2f}, pytorch={pytorch_pred:.2f}, xgb={xgb_pred:.2f}, volatility={volatility:.2f}, fallback={fallback:.2f}, prediction_history_rows={history_rows}, prediction_history_sample={history_sample}, meta_model_initialized={'meta_model' in bot_state}")
                    return fallback
            except Exception as load_error:
                logger.error(f"Failed to load saved meta-model/scaler: {load_error}; base predictions: sklearn={sklearn_pred:.2f}, pytorch={pytorch_pred:.2f}, xgb={xgb_pred:.2f}, volatility={volatility:.2f}, fallback={fallback:.2f}, prediction_history_rows={history_rows}, meta_model_initialized={'meta_model' in bot_state}")
                return fallback

        meta_input_scaled = meta_scaler.transform(meta_input)
        prediction = meta_model.predict(meta_input_scaled)[0]
        if not np.isfinite(prediction) or not 1000 < prediction < 10000:
            logger.error(f"Invalid meta-model prediction: {prediction}, using fallback={fallback:.2f}")
            return fallback

        logger.info(f"Meta-model prediction: {prediction:.2f}")
        return prediction
    except Exception as e:
        logger.error(f"Meta-model prediction failed: {e}, using fallback={fallback:.2f}")
        return fallback

def optimize_config_parameters(predictions, actuals, current_params, bounds):
    """
    Optimize config parameters (ML_TREND_WEIGHT, ML_CONFIDENCE_THRESHOLD) using gradient-based optimization.

    Args:
        predictions (np.array): Ensemble predictions
        actuals (np.array): Actual prices
        current_params (dict): Current parameter values
        bounds (dict): Parameter bounds

    Returns:
        dict: Updated parameter values
    """
    try:
        learning_rate = 0.01
        new_params = current_params.copy()
        mse = np.mean((predictions - actuals) ** 2)

        for param, value in current_params.items():
            if param in bounds:
                delta = 0.01
                new_params[param] = value + delta
                perturbed_pred = predictions * (1.0 + delta if param == "ML_TREND_WEIGHT" else 1.0)
                perturbed_mse = np.mean((perturbed_pred - actuals) ** 2)
                gradient = (perturbed_mse - mse) / delta
                new_params[param] = value - learning_rate * gradient
                new_params[param] = max(bounds[param][0], min(bounds[param][1], new_params[param]))

        return new_params
    except Exception as error:
        logger.error(f"Parameter optimization failed: {error}")
        return current_params

def train_sklearn_predictor(data, trade_data=None, lookback=80):
    """
    Train separate RandomForestRegressor and SGDRegressor models with individual scalers.
    Args:
        data (pd.DataFrame): Historical OHLCV data.
        trade_data (pd.DataFrame): Trade count data.
        lookback (int): Number of lookback periods (default: 80).
    Returns:
        tuple: (rf_model, rf_scaler, sgd_model, sgd_scaler, lookback)
    """
    try:
        if data is None or data.empty:
            logger.warning("Input data is None or empty")
            return None, None, None, None, lookback
        if len(data) < lookback + 2:
            logger.warning(f"Insufficient data for sklearn training: {len(data)} samples, need {lookback + 2}")
            return None, None, None, None, lookback

        # Prepare data
        data = data.copy()
        required_columns = ["close", "volume", "rsi", "ema", "volatility", "high", "low"]
        missing_columns = [col for col in required_columns if col not in data.columns]
        if missing_columns:
            logger.error(f"Missing required columns in data: {missing_columns}")
            return None, None, None, None, lookback

        # Ensure timestamp is valid
        if (
            "timestamp" not in data.columns
            or data["timestamp"].isna().all()
            or not pd.api.types.is_datetime64_any_dtype(data["timestamp"])
        ):
            logger.warning("Invalid or missing timestamps in data, setting to current time")
            data["timestamp"] = pd.Timestamp.now(tz="UTC")
        data["timestamp"] = pd.to_datetime(data["timestamp"], errors="coerce").dt.tz_convert("UTC")
        data["timestamp"] = data["timestamp"].fillna(pd.Timestamp.now(tz="UTC"))

        # Merge with trade_data if provided
        if trade_data is not None and not trade_data.empty and "timestamp" in trade_data.columns:
            trade_data = trade_data.copy()
            trade_data["timestamp"] = pd.to_datetime(trade_data["timestamp"], errors="coerce").dt.tz_convert("UTC")
            trade_data["timestamp"] = trade_data["timestamp"].fillna(pd.Timestamp.now(tz="UTC"))
            trade_data_for_merge = trade_data[["timestamp", "trades"]].rename(columns={"trades": "new_trades"})
            data = data.merge(trade_data_for_merge, on="timestamp", how="left")
            data["trades"] = data["new_trades"].combine_first(data.get("trades", 0))
            data.drop(columns=["new_trades"], inplace=True)
            data["trades"] = data["trades"].fillna(0).astype(int)
        else:
            logger.info("No valid trade_data provided, setting trades to 0")
            data["trades"] = data.get("trades", 0).fillna(0).astype(int)

        # Prepare 18 features
        data["macd"], data["macd_signal"] = compute_macd(data["close"])
        data["bollinger_upper"], data["bollinger_lower"] = compute_bollinger(data["close"])
        data["momentum"] = compute_momentum(data["close"])
        data["volume_trend"] = compute_volume_trend(data["volume"])
        data["atr"] = compute_atr(data["high"], data["low"], data["close"])
        data["vwap"] = compute_vwap(data)
        data = compute_additional_features(data)
        features = [
            "close", "volume", "trades", "rsi", "ema", "volatility", "macd", "macd_signal",
            "bollinger_upper", "bollinger_lower", "momentum", "volume_trend", "atr", "vwap",
            "price_spread", "returns", "volume_change", "trade_intensity"
        ]
        data["target"] = data["close"].shift(-1).fillna(data["close"].iloc[-1])

        # Validate and fill NaN values
        data = data[features + ["target"]].fillna(
            {
                "rsi": 50.0,
                "ema": data["close"].mean(),
                "volatility": 0.1,
                "macd": 0.0,
                "macd_signal": 0.0,
                "bollinger_upper": data["close"].mean(),
                "bollinger_lower": data["close"].mean(),
                "momentum": 0.0,
                "volume_trend": 0.0,
                "atr": data["close"].mean() * 0.01,
                "vwap": data["close"].mean(),
                "price_spread": 0.0,
                "returns": 0.0,
                "volume_change": 0.0,
                "trade_intensity": 0.0,
                "trades": 0,
                "target": data["close"].mean(),
            }
        )

        # Validate numeric features
        for feature in features + ["target"]:
            if not pd.api.types.is_numeric_dtype(data[feature]):
                logger.error(f"Feature {feature} contains non-numeric values")
                return None, None, None, None, lookback

        # RF Model: Single time step features
        rf_scaler = StandardScaler()
        rf_X = data[features]
        rf_y = data["target"]
        rf_X_scaled = rf_scaler.fit_transform(rf_X)
        rf_model = RandomForestRegressor(
            n_estimators=config.SKLEARN_N_ESTIMATORS,
            max_depth=config.SKLEARN_MAX_DEPTH,
            random_state=42,
        )
        rf_model.fit(rf_X_scaled, rf_y)
        rf_train_mse = mean_squared_error(rf_y, rf_model.predict(rf_X_scaled))

        # Save RF scaler
        rf_scaler_file = "client_sklearn_rf_scaler.pkl"
        if os.path.exists(rf_scaler_file):
            os.remove(rf_scaler_file)
            logger.info(f"Removed existing RF scaler file: {rf_scaler_file}")
        joblib.dump(rf_scaler, rf_scaler_file)
        logger.info(f"Saved RF scaler to {rf_scaler_file}")

        # SGD Model: Sequence features
        sgd_scaler = StandardScaler()
        target_scaler = StandardScaler()
        X, y = [], []
        for i in range(lookback, len(data)):
            X.append(data[features].iloc[i - lookback:i].values.flatten())
            y.append(data["target"].iloc[i])
        if not X or len(X) < 20:
            logger.warning(f"Too few training samples for SGD: {len(X)}")
            return None, None, None, None, lookback
        X = np.array(X)
        y = np.array(y).reshape(-1, 1)
        sgd_X_scaled = sgd_scaler.fit_transform(X)
        y_scaled = target_scaler.fit_transform(y)
        sgd_model = SGDRegressor(random_state=42, max_iter=1000, tol=1e-3)
        sgd_model.fit(sgd_X_scaled, y_scaled.ravel())
        y_pred_scaled = sgd_model.predict(sgd_X_scaled)
        y_pred = target_scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).ravel()
        sgd_train_mse = mean_squared_error(y, y_pred)

        # Save SGD scaler
        sgd_scaler_file = "client_sklearn_sgd_scaler.pkl"
        if os.path.exists(sgd_scaler_file):
            os.remove(sgd_scaler_file)
            logger.info(f"Removed existing SGD scaler file: {sgd_scaler_file}")
        joblib.dump(sgd_scaler, sgd_scaler_file)
        logger.info(f"Saved SGD scaler to {sgd_scaler_file}")

        logger.info(
            f"Sklearn RF trained: Samples={len(rf_X)}, MSE={rf_train_mse:.6f}, Features={features}\n"
            f"Sklearn SGD trained: Samples={len(X)}, MSE={sgd_train_mse:.6f}, Features={lookback * len(features)}"
        )
        return rf_model, rf_scaler, sgd_model, sgd_scaler, lookback
    except Exception as e:
        logger.error(f"Error training Sklearn model: {e}")
        return None, None, None, None, lookback

def train_pytorch_predictor(trade_data, ohlcv_data, lookback_minutes=64, num_epochs=config.PYTORCH_NUM_EPOCHS):  # Align with PYTORCH_LOOKBACK
    try:
        if ohlcv_data is None or ohlcv_data.empty:
            logger.error("OHLCV data is None or empty")
            return None, None, None
        if trade_data is None or trade_data.empty:
            logger.warning("Trade data is None or empty, using OHLCV only")
            trade_data = ohlcv_data[["timestamp"]].assign(trades=0)
        else:
            trade_data = trade_data.copy()

        required_columns = ["timestamp", "close", "volume", "rsi", "ema", "high", "low"]
        missing_columns = [col for col in required_columns if col not in ohlcv_data.columns]
        if missing_columns:
            logger.error(f"Missing required columns in ohlcv_data: {missing_columns}")
            return None, None, None

        # Ensure timestamps
        ohlcv_data = ohlcv_data.copy()
        trade_data = trade_data.copy()
        ohlcv_data["timestamp"] = pd.to_datetime(ohlcv_data["timestamp"], errors="coerce").dt.tz_convert("UTC")
        ohlcv_data["timestamp"] = ohlcv_data["timestamp"].fillna(pd.Timestamp.now(tz="UTC"))
        trade_data["timestamp"] = pd.to_datetime(trade_data["timestamp"], errors="coerce").dt.tz_convert("UTC")
        trade_data["timestamp"] = trade_data["timestamp"].fillna(pd.Timestamp.now(tz="UTC"))

        merged_data = (
            ohlcv_data[["timestamp", "close", "volume", "rsi", "ema", "high", "low"]]
            .merge(trade_data[["timestamp", "trades"]], on="timestamp", how="left")
            .fillna({"trades": 0})
        )

        if len(merged_data) < lookback_minutes + 2:
            logger.error(f"Insufficient data: {len(merged_data)} samples, need at least {lookback_minutes + 2}")
            return None, None, None

        # Compute 18 features
        merged_data["macd"], merged_data["macd_signal"] = compute_macd(merged_data["close"])
        merged_data["bollinger_upper"], merged_data["bollinger_lower"] = compute_bollinger(merged_data["close"])
        merged_data["momentum"] = compute_momentum(merged_data["close"])
        merged_data["volume_trend"] = compute_volume_trend(merged_data["volume"])
        merged_data["atr"] = compute_atr(merged_data["high"], merged_data["low"], merged_data["close"])
        merged_data["vwap"] = compute_vwap(merged_data)
        merged_data = compute_additional_features(merged_data)
        merged_data["volatility"] = (
            merged_data["close"].pct_change().rolling(config.VOLATILITY_WINDOW, min_periods=1).std() * 100
        )
        features = [
            "close", "volume", "trades", "rsi", "ema", "volatility", "macd", "macd_signal",
            "bollinger_upper", "bollinger_lower", "momentum", "volume_trend", "atr", "vwap",
            "price_spread", "returns", "volume_change", "trade_intensity"
        ]
        merged_data["target"] = merged_data["close"].shift(-1).fillna(merged_data["close"].iloc[-1])  # Next close price

        # Validate and fill NaN values
        merged_data = merged_data[features + ["target"]].fillna(
            {
                "rsi": 50.0,
                "ema": merged_data["close"].mean(),
                "volatility": 0.1,
                "macd": 0.0,
                "macd_signal": 0.0,
                "bollinger_upper": merged_data["close"].mean(),
                "bollinger_lower": merged_data["close"].mean(),
                "momentum": 0.0,
                "volume_trend": 0.0,
                "atr": merged_data["close"].mean() * 0.01,
                "vwap": merged_data["close"].mean(),
                "price_spread": 0.0,
                "returns": 0.0,
                "volume_change": 0.0,
                "trade_intensity": 0.0,
                "trades": 0,
                "target": merged_data["close"].mean(),
            }
        )

        # Validate numeric features
        for feature in features + ["target"]:
            if not pd.api.types.is_numeric_dtype(merged_data[feature]):
                logger.error(f"Feature {feature} contains non-numeric values")
                return None, None, None

        # Scale features
        feature_scaler = MinMaxScaler()
        logger.debug(f"Fitting feature scaler with features: {features}")
        feature_scaler.fit(merged_data[features])
        scaled_features = feature_scaler.transform(merged_data[features])
        if hasattr(feature_scaler, "feature_names_in_"):
            logger.info(f"PyTorch feature scaler names: {feature_scaler.feature_names_in_.tolist()}")

        # Scale target
        target_scaler = MinMaxScaler()
        y = merged_data["target"].values.reshape(-1, 1)
        target_scaler.fit(y)
        y_scaled = target_scaler.transform(y).flatten()
        logger.debug(f"Target scaler: min={target_scaler.data_min_[0]:.2f}, max={target_scaler.data_max_[0]:.2f}")

        # Prepare sequences
        X, y_train = [], []
        for i in range(lookback_minutes, len(scaled_features)):
            X.append(scaled_features[i - lookback_minutes : i])
            y_train.append(y_scaled[i])
        X = np.array(X)
        y_train = np.array(y_train)

        train_size = int(len(X) * 0.8)
        if train_size < 10 or len(X) - train_size < 5:
            logger.error(f"Insufficient split: Train {train_size}, Test {len(X) - train_size}")
            return None, None, None

        X_train, X_test = X[:train_size], X[train_size:]
        y_train, y_test = y_train[:train_size], y_train[train_size:]

        X_train = torch.FloatTensor(X_train).to(config.DEVICE)
        y_train = torch.FloatTensor(y_train).view(-1, 1).to(config.DEVICE)
        X_test = torch.FloatTensor(X_test).to(config.DEVICE)
        y_test = torch.FloatTensor(y_test).view(-1, 1).to(config.DEVICE)

        # Initialize model
        model = LSTMPricePredictor(
            input_size=len(features),
            hidden_size=config.PYTORCH_HIDDEN_SIZE,
            num_layers=config.PYTORCH_NUM_LAYERS,
            dropout=config.PYTORCH_DROPOUT
        ).to(config.DEVICE)
        criterion = nn.MSELoss()
        optimizer = optim.Adam(model.parameters(), lr=config.PYTORCH_LEARNING_RATE)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)

        # Training loop
        best_test_loss = float('inf')
        patience = 20
        counter = 0
        for epoch in range(num_epochs):
            model.train()
            optimizer.zero_grad()
            outputs = model(X_train)
            loss = criterion(outputs, y_train)
            loss.backward()
            optimizer.step()

            model.eval()
            with torch.no_grad():
                test_outputs = model(X_test)
                test_loss = criterion(test_outputs, y_test)

            scheduler.step(test_loss)

            if test_loss < best_test_loss:
                best_test_loss = test_loss
                counter = 0
            else:
                counter += 1
                if counter >= patience:
                    logger.info(f"Early stopping at epoch {epoch + 1}")
                    break

            if (epoch + 1) % 20 == 0:
                logger.info(
                    f"PyTorch Epoch {epoch + 1}/{num_epochs}, Train Loss: {loss.item():.6f}, Test Loss: {test_loss.item():.6f}"
                )

        # Save scalers
        feature_scaler_file = "client_pytorch_scaler.pkl"
        target_scaler_file = "client_pytorch_target_scaler.pkl"
        if os.path.exists(feature_scaler_file):
            os.remove(feature_scaler_file)
            logger.info(f"Removed existing feature scaler file: {feature_scaler_file}")
        if os.path.exists(target_scaler_file):
            os.remove(target_scaler_file)
            logger.info(f"Removed existing target scaler file: {target_scaler_file}")
        joblib.dump(feature_scaler, feature_scaler_file)
        joblib.dump(target_scaler, target_scaler_file)
        logger.info(f"Saved feature scaler to {feature_scaler_file}")
        logger.info(f"Saved target scaler to {target_scaler_file}")

        logger.info(
            f"PyTorch Training Data: samples={len(merged_data)}, "
            f"last_close={merged_data['close'].iloc[-1]:.2f}, last_volume={merged_data['volume'].iloc[-1]:.2f}, "
            f"last_trades={merged_data['trades'].iloc[-1]:.0f}, rsi={merged_data['rsi'].iloc[-1]:.2f}, "
            f"ema={merged_data['ema'].iloc[-1]:.2f}, atr={merged_data['atr'].iloc[-1]:.2f}, "
            f"vwap={merged_data['vwap'].iloc[-1]:.2f}, Features: {features}"
        )
        return model, feature_scaler, target_scaler
    except Exception as e:
        logger.error(f"Error training PyTorch model: {e}")
        return None, None, None

def train_xgboost_predictor(data, trade_data=None, lookback=60):
    """
    Train XGBoost model with sequence features.
    Args:
        data (pd.DataFrame): Historical OHLCV data.
        trade_data (pd.DataFrame): Trade count data.
        lookback (int): Number of lookback periods (default: 60).
    Returns:
        tuple: (model, scaler, lookback)
    """
    try:
        logger.debug(f"XGBoost version: {xgb.__version__}")

        if data is None or data.empty:
            logger.warning("Input data is None or empty")
            return None, None, lookback
        if len(data) < lookback + 1:
            logger.warning(f"Insufficient data for XGBoost training: {len(data)} samples, need {lookback + 1}")
            return None, None, lookback

        # Prepare data
        data = data.copy()
        required_columns = ["close", "volume", "rsi", "ema", "volatility", "high", "low"]
        missing_columns = [col for col in required_columns if col not in data.columns]
        if missing_columns:
            logger.error(f"Missing required columns in data: {missing_columns}")
            return None, None, lookback

        # Ensure timestamp is valid
        if (
            "timestamp" not in data.columns
            or data["timestamp"].isna().all()
            or not pd.api.types.is_datetime64_any_dtype(data["timestamp"])
        ):
            logger.warning("Invalid or missing timestamps in data, setting to current time")
            data["timestamp"] = pd.Timestamp.now(tz="UTC")
        data["timestamp"] = pd.to_datetime(data["timestamp"], errors="coerce").dt.tz_convert("UTC")
        data["timestamp"] = data["timestamp"].fillna(pd.Timestamp.now(tz="UTC"))

        # Merge with trade_data if provided
        if trade_data is not None and not trade_data.empty and "timestamp" in trade_data.columns:
            trade_data = trade_data.copy()
            trade_data["timestamp"] = pd.to_datetime(trade_data["timestamp"], errors="coerce").dt.tz_convert("UTC")
            trade_data["timestamp"] = trade_data["timestamp"].fillna(pd.Timestamp.now(tz="UTC"))
            trade_data_for_merge = trade_data[["timestamp", "trades"]].rename(columns={"trades": "new_trades"})
            data = data.merge(trade_data_for_merge, on="timestamp", how="left")
            data["trades"] = data["new_trades"].combine_first(data.get("trades", 0))
            data.drop(columns=["new_trades"], inplace=True)
            data["trades"] = data["trades"].fillna(0).astype(int)
        else:
            logger.info("No valid trade_data provided, setting trades to 0")
            data["trades"] = data.get("trades", 0).fillna(0).astype(int)

        # Prepare 18 features
        data["macd"], data["macd_signal"] = compute_macd(data["close"])
        data["bollinger_upper"], data["bollinger_lower"] = compute_bollinger(data["close"])
        data["momentum"] = compute_momentum(data["close"])
        data["volume_trend"] = compute_volume_trend(data["volume"])
        data["atr"] = compute_atr(data["high"], data["low"], data["close"])
        data["vwap"] = compute_vwap(data)
        data = compute_additional_features(data)
        features = [
            "close", "volume", "trades", "rsi", "ema", "volatility", "macd", "macd_signal",
            "bollinger_upper", "bollinger_lower", "momentum", "volume_trend", "atr", "vwap",
            "price_spread", "returns", "volume_change", "trade_intensity"
        ]
        data["target"] = data["close"].shift(-1).fillna(data["close"].iloc[-1])

        # Validate and fill NaN values
        data = data[features + ["target"]].fillna(
            {
                "rsi": 50.0,
                "ema": data["close"].mean(),
                "volatility": 0.1,
                "macd": 0.0,
                "macd_signal": 0.0,
                "bollinger_upper": data["close"].mean(),
                "bollinger_lower": data["close"].mean(),
                "momentum": 0.0,
                "volume_trend": 0.0,
                "atr": data["close"].mean() * 0.01,
                "vwap": data["close"].mean(),
                "price_spread": 0.0,
                "returns": 0.0,
                "volume_change": 0.0,
                "trade_intensity": 0.0,
                "trades": 0,
                "target": data["close"].mean(),
            }
        )

        # Validate numeric features
        for feature in features + ["target"]:
            if not pd.api.types.is_numeric_dtype(data[feature]):
                logger.error(f"Feature {feature} contains non-numeric values")
                return None, None, lookback

        # Scale features
        scaler = StandardScaler()
        logger.debug(f"Fitting XGBoost scaler with features: {features}")
        scaler.fit(data[features])
        scaled_features = scaler.transform(data[features])
        if hasattr(scaler, "feature_names_in_"):
            logger.info(f"XGBoost scaler feature names after fit: {scaler.feature_names_in_.tolist()}")

        # Prepare sequences
        X, y = [], []
        for i in range(lookback, len(scaled_features)):
            X.append(scaled_features[i - lookback : i].flatten())
            y.append(data["target"].iloc[i])

        if not X or len(X) < 10:
            logger.warning(f"Too few training samples after lookback: {len(X)}")
            return None, None, lookback

        X = np.array(X)
        y = np.array(y)
        logger.debug(f"XGBoost training data: X shape={X.shape}, y shape={y.shape}")

        # Train-test split
        train_size = int(len(X) * 0.8)
        if train_size < 5 or len(X) - train_size < 2:
            logger.warning(f"Insufficient split: Train {train_size}, Test {len(X) - train_size}")
            return None, None, lookback

        X_train, X_test = X[:train_size], X[train_size:]
        y_train, y_test = y[:train_size], y[train_size:]

        # Train model
        model = xgb.XGBRegressor(
            n_estimators=50,
            max_depth=4,
            learning_rate=0.1,
            random_state=42,
            alpha=1.0,
            reg_lambda=1.0,
            min_child_weight=3,
        )
        model.fit(X_train, y_train)

        # Evaluate
        y_train_pred = model.predict(X_train)
        y_test_pred = model.predict(X_test)
        train_mse = mean_squared_error(y_train, y_train_pred)
        test_mse = mean_squared_error(y_test, y_test_pred)

        # Save scaler
        scaler_file = "client_xgb_scaler.pkl"
        if os.path.exists(scaler_file):
            os.remove(scaler_file)
            logger.info(f"Removed existing XGBoost scaler file: {scaler_file}")
        joblib.dump(scaler, scaler_file)
        logger.info(f"Saved XGBoost scaler to {scaler_file}")

        logger.info(
            f"XGBoost model trained. Train samples: {len(X_train)}, Test samples: {len(X_test)}, "
            f"Train MSE: {train_mse:.6f}, Test MSE: {test_mse:.6f}, Features: {len(features) * lookback}"
        )
        return model, scaler, lookback
    except Exception as e:
        logger.error(f"Error training XGBoost model: {e}")
        return None, None, lookback

def predict_sklearn_price(model, scaler, lookback, recent_data, current_price, last_valid_prediction=None):
    """
    Predict price using sklearn RandomForestRegressor model.
    Args:
        model: RandomForestRegressor model.
        scaler (StandardScaler): Scaler for features.
        lookback (int): Number of lookback periods (e.g., 80).
        recent_data (pd.DataFrame): Input data with required features.
        current_price (float): Current market price.
        last_valid_prediction (float): Last predicted price for fallback.
    Returns:
        float: Predicted price.
    """
    try:
        features = [
            "close", "volume", "trades", "rsi", "ema", "volatility", "macd", "macd_signal",
            "bollinger_upper", "bollinger_lower", "momentum", "volume_trend", "atr", "vwap",
            "price_spread", "returns", "volume_change", "trade_intensity"
        ]
        data = recent_data.copy()
        if data.empty:
            logger.warning("Sklearn input data is empty, returning last valid prediction")
            return last_valid_prediction or current_price

        # Load scaler if not provided
        scaler_file = "client_sklearn_rf_scaler.pkl"
        if scaler is None and os.path.exists(scaler_file):
            scaler = joblib.load(scaler_file)
            logger.info(f"Loaded RF scaler from {scaler_file}")
        elif scaler is None:
            logger.error(f"RF scaler file {scaler_file} not found")
            return last_valid_prediction or current_price

        # Validate feature names
        if hasattr(scaler, "feature_names_in_") and list(scaler.feature_names_in_) != features:
            logger.error(f"Feature mismatch: scaler expects {scaler.feature_names_in_}, got {features}")
            return last_valid_prediction or current_price

        # Filter and compute features
        missing_features = [f for f in features if f not in data.columns]
        if missing_features:
            logger.warning(f"Missing features: {missing_features}, computing additional features")
            data = compute_additional_features(data)
        data = data.reindex(columns=features, fill_value=None)

        # Fill NaNs
        default_values = {
            "close": current_price, "volume": 0.0, "trades": 0, "rsi": 50.0, "ema": current_price,
            "volatility": 0.1, "macd": 0.0, "macd_signal": 0.0, "bollinger_upper": current_price,
            "bollinger_lower": current_price, "momentum": 0.0, "volume_trend": 0.0,
            "atr": current_price * 0.01, "vwap": current_price, "price_spread": 0.0,
            "returns": 0.0, "volume_change": 0.0, "trade_intensity": 0.0
        }
        data = data.fillna(default_values)
        logger.debug(f"Data after prep: {len(data)} rows, columns: {data.columns.tolist()}")

        # Validate data size
        if len(data) < lookback:
            logger.warning(f"Insufficient data: {len(data)} rows, needed {lookback}")
            return last_valid_prediction or current_price

        # Predict using RF model
        prediction_history = getattr(predict_sklearn_price, "history", collections.deque(maxlen=5))
        X_scaled = scaler.transform(data[features].tail(lookback))
        if model is None:
            logger.error("RF model is None")
            return last_valid_prediction or current_price
        predicted_price = model.predict(X_scaled[-1].reshape(1, -1))[0]
        prediction_history.append(predicted_price)
        avg_predicted = np.mean(list(prediction_history))
        logger.debug(f"Sklearn RF prediction: raw={predicted_price:.2f}, avg={avg_predicted:.2f}")

        # Validate prediction
        max_deviation = current_price * 0.1
        if not np.isfinite(avg_predicted) or abs(avg_predicted - current_price) > max_deviation or avg_predicted == 0.0:
            logger.warning(f"Invalid prediction {avg_predicted:.2f}, using last valid")
            return last_valid_prediction or current_price

        predict_sklearn_price.history = prediction_history
        return avg_predicted
    except Exception as e:
        logger.error(f"Sklearn prediction error: {e}")
        logger.debug(f"Debug prompt: Sklearn prediction exception - {type(e).__name__}: {str(e)}")
        logger.debug("Debug prompt: Check model state, feature scaler, and input data format")
        logger.debug("Optimization prompt: Verify feature consistency between training and prediction")
        return last_valid_prediction or current_price

def predict_pytorch_price(model, feature_scaler, target_scaler, lookback, recent_data, current_price, last_valid_prediction=None):
    try:
        features = [
            "close", "volume", "trades", "rsi", "ema", "volatility", "macd", "macd_signal",
            "bollinger_upper", "bollinger_lower", "momentum", "volume_trend", "atr", "vwap",
            "price_spread", "returns", "volume_change", "trade_intensity"
        ]
        data = recent_data.copy()
        if data.empty:
            logger.warning("PyTorch input data empty, returning last valid prediction")
            return last_valid_prediction or current_price

        # Load scalers if not provided
        feature_scaler_file = "client_pytorch_scaler.pkl"
        target_scaler_file = "client_pytorch_target_scaler.pkl"
        if feature_scaler is None and os.path.exists(feature_scaler_file):
            feature_scaler = joblib.load(feature_scaler_file)
            logger.info(f"Loaded feature scaler from {feature_scaler_file}")
        if target_scaler is None and os.path.exists(target_scaler_file):
            target_scaler = joblib.load(target_scaler_file)
            logger.info(f"Loaded target scaler from {target_scaler_file}")
        if feature_scaler is None or target_scaler is None:
            logger.error(f"Scaler files missing: feature={feature_scaler_file}, target={target_scaler_file}")
            return last_valid_prediction or current_price

        # Validate feature names
        if hasattr(feature_scaler, "feature_names_in_") and list(feature_scaler.feature_names_in_) != features:
            logger.error(f"Feature mismatch: scaler expects {feature_scaler.feature_names_in_}, got {features}")
            return last_valid_prediction or current_price

        # Filter to required features
        missing_features = [f for f in features if f not in data.columns]
        if missing_features:
            logger.warning(f"Missing features: {missing_features}, filling with defaults")
        data = data.reindex(columns=features, fill_value=None)

        # Fill NaNs
        default_values = {
            "close": current_price, "volume": 0.0, "trades": 0, "rsi": 50.0, "ema": current_price,
            "volatility": 0.1, "macd": 0.0, "macd_signal": 0.0, "bollinger_upper": current_price,
            "bollinger_lower": current_price, "momentum": 0.0, "volume_trend": 0.0,
            "atr": current_price * 0.01, "vwap": current_price, "price_spread": 0.0,
            "returns": 0.0, "volume_change": 0.0, "trade_intensity": 0.0
        }
        data = data.fillna(default_values)
        logger.debug(f"Data after filtering: {len(data)} rows, columns: {data.columns.tolist()}")

        # Validate data size
        if len(data) < lookback:
            logger.warning(f"Insufficient data: {len(data)} rows, needed {lookback}")
            return last_valid_prediction or current_price

        # Predict
        model.eval()
        X_scaled = feature_scaler.transform(data[features].tail(lookback))
        X_tensor = torch.FloatTensor(X_scaled).unsqueeze(0).to(config.DEVICE)  # [1, lookback, 18]
        with torch.no_grad():
            predicted_scaled = model(X_tensor).cpu().numpy().flatten()[0]
        logger.debug(f"Raw model output: scaled={predicted_scaled:.4f}")

        # Inverse transform
        predicted_price = target_scaler.inverse_transform([[predicted_scaled]])[0][0]
        # Clip prediction
        actual_price = data["close"].iloc[-1]
        predicted_price = np.clip(predicted_price, actual_price - 50, actual_price + 50)
        logger.debug(f"PyTorch prediction: scaled={predicted_scaled:.4f}, price={predicted_price:.2f}, "
                     f"target_scaler min={target_scaler.data_min_[0]:.2f}, max={target_scaler.data_max_[0]:.2f}")

        # Use prediction history
        prediction_history = getattr(predict_pytorch_price, "history", collections.deque(maxlen=5))
        prediction_history.append(predicted_price)
        avg_predicted = np.mean(list(prediction_history))

        # Validate prediction
        max_deviation = current_price * 0.1
        if not np.isfinite(avg_predicted) or abs(avg_predicted - current_price) > max_deviation:
            logger.warning(f"Invalid prediction {avg_predicted:.2f}, using last valid")
            return last_valid_prediction or current_price

        predict_pytorch_price.history = prediction_history
        return avg_predicted
    except Exception as e:
        logger.error(f"PyTorch prediction error: {e}")
        logger.debug(f"Debug prompt: PyTorch prediction exception - {type(e).__name__}: {str(e)}")
        logger.debug("Debug prompt: Check model device, tensor shapes, and gradient computation")
        logger.debug("Optimization prompt: Verify model.eval() mode and tensor data types")
        return last_valid_prediction or current_price

def predict_xgboost_price(model, scaler, lookback, recent_data, current_price, last_valid_prediction=None):
    """
    Predict price using XGBoost model.
    Args:
        model: XGBoost model.
        scaler (StandardScaler): Scaler for features.
        lookback (int): Number of lookback periods (e.g., 60).
        recent_data (pd.DataFrame): Input data with required features.
        current_price (float): Current market price.
        last_valid_prediction (float): Last predicted price for fallback.
    Returns:
        float: Predicted price.
    """
    try:
        features = [
            "close", "volume", "trades", "rsi", "ema", "volatility", "macd", "macd_signal",
            "bollinger_upper", "bollinger_lower", "momentum", "volume_trend", "atr", "vwap",
            "price_spread", "returns", "volume_change", "trade_intensity"
        ]
        data = recent_data.copy()
        if data.empty:
            logger.warning("XGBoost input data empty, returning last valid prediction")
            return last_valid_prediction or current_price

        # Load scaler if not provided
        scaler_file = "client_xgb_scaler.pkl"
        if scaler is None and os.path.exists(scaler_file):
            scaler = joblib.load(scaler_file)
            logger.info(f"Loaded XGBoost scaler from {scaler_file}")
        elif scaler is None:
            logger.error(f"XGBoost scaler file {scaler_file} not found")
            return last_valid_prediction or current_price

        # Validate feature names
        if hasattr(scaler, "feature_names_in_") and list(scaler.feature_names_in_) != features:
            logger.error(f"Feature mismatch: scaler expects {scaler.feature_names_in_}, got {features}")
            return last_valid_prediction or current_price

        # Filter and compute features
        missing_features = [f for f in features if f not in data.columns]
        if missing_features:
            logger.warning(f"Missing features: {missing_features}, computing additional features")
            data = compute_additional_features(data)
        data = data.reindex(columns=features, fill_value=None)

        # Fill NaNs
        default_values = {
            "close": current_price, "volume": 0.0, "trades": 0, "rsi": 50.0, "ema": current_price,
            "volatility": 0.1, "macd": 0.0, "macd_signal": 0.0, "bollinger_upper": current_price,
            "bollinger_lower": current_price, "momentum": 0.0, "volume_trend": 0.0,
            "atr": current_price * 0.01, "vwap": current_price, "price_spread": 0.0,
            "returns": 0.0, "volume_change": 0.0, "trade_intensity": 0.0
        }
        data = data.fillna(default_values)
        logger.debug(f"Data after prep: {len(data)} rows, columns: {data.columns.tolist()}")

        # Validate data size
        if len(data) < lookback:
            logger.warning(f"Insufficient data: {len(data)} rows, needed {lookback}")
            return last_valid_prediction or current_price

        # Prepare features: flatten lookback periods
        prediction_history = getattr(predict_xgboost_price, "history", collections.deque(maxlen=5))
        X = data[features].tail(lookback)
        X_scaled = scaler.transform(X)
        X_flat = X_scaled.flatten().reshape(1, -1)  # Shape: (1, lookback * num_features)
        expected_features = lookback * len(features)
        if X_flat.shape[1] != expected_features:
            logger.error(f"Feature shape mismatch: expected {expected_features}, got {X_flat.shape[1]}")
            return last_valid_prediction or current_price

        predicted_price = model.predict(X_flat)[0]
        prediction_history.append(predicted_price)
        avg_predicted = np.mean(list(prediction_history))
        logger.debug(f"XGBoost prediction: raw={predicted_price:.2f}, avg={avg_predicted:.2f}")

        # Validate prediction
        max_deviation = current_price * 0.1
        if not np.isfinite(avg_predicted) or abs(avg_predicted - current_price) > max_deviation or avg_predicted == 0.0:
            logger.warning(f"Invalid prediction {avg_predicted:.2f}, using last valid")
            return last_valid_prediction or current_price

        predict_xgboost_price.history = prediction_history
        return avg_predicted
    except Exception as e:
        logger.error(f"XGBoost prediction error: {e}")
        return last_valid_prediction or current_price

def calculate_locked_funds(buy_orders, sell_orders, current_price):
    locked_usd = 0
    locked_eth = 0
    for order in buy_orders:
        try:
            price = order.get("price", current_price)
            size = order.get("size", order.get("amount", 0))
            feature = order.get("feature", "base")
            locked_usd += size * price
        except Exception as e:
            logger.error(
                f"Error calculating locked USD for order {order.get('id', 'unknown')}, feature={feature}: {e}"
            )
    for order in sell_orders:
        try:
            size = order.get("size", order.get("amount", 0))
            feature = order.get("feature", "base")
            locked_eth += size
        except Exception as e:
            logger.error(
                f"Error calculating locked ETH for order {order.get('id', 'unknown')}, feature={feature}: {e}"
            )
    return locked_usd, locked_eth

@backoff.on_exception(backoff.expo, ccxt.RateLimitExceeded, max_tries=5)
def sync_balances(exchange):
    try:
        for attempt in range(3):
            balance = exchange.fetch_balance()
            eth = float(balance["ETH"]["free"]) if "ETH" in balance else bot_state["eth_balance"]
            usd = float(balance["USD"]["free"]) if "USD" in balance else bot_state["usd_balance"]

            # --- MODIFIED: Reserve funds for Supertrend AND Breakout ---
            reserved_eth = 0.0
            if is_supertrend_in_position():
                reserved_eth += config.STRATEGY_POSITION_SIZE
                logger.info(f"Supertrend is in position. Reserving {config.STRATEGY_POSITION_SIZE} ETH.")

            if bot_state.get("breakout_position", {}).get("active") and bot_state["breakout_position"]["side"] == "buy":
                breakout_size = bot_state["breakout_position"].get("size", 0.0)
                reserved_eth += breakout_size
                logger.info(f"Breakout position is active. Reserving {breakout_size} ETH.")

            if reserved_eth > 0:
                if eth >= reserved_eth:
                    eth -= reserved_eth
                    logger.info(f"Total reserved: {reserved_eth} ETH. Grid bot available ETH: {eth:.6f}")
                else:
                    logger.warning(f"Not enough free ETH to reserve for strategies. Free: {eth}, Needed: {reserved_eth}")
            # --- END MODIFICATION ---

            if usd < config.MIN_USD_BALANCE:
                logger.warning(f"USD balance {usd:.2f} below minimum {config.MIN_USD_BALANCE:.2f}, pausing trading")
                bot_state["paused"] = True
            if eth != bot_state["eth_balance"] or usd != bot_state["usd_balance"]:
                logger.info(
                    f"Balance updated (attempt {attempt + 1}/3): ETH {bot_state['eth_balance']:.6f} -> {eth:.6f}, USD {bot_state['usd_balance']:.2f} -> {usd:.2f}"
                )
            bot_state["eth_balance"] = eth
            bot_state["usd_balance"] = usd
            time.sleep(exchange.rateLimit / 1000)
            return eth, usd
    except ccxt.AuthenticationError as e:
        logger.error(f"Authentication error syncing balances: {e}")
        bot_state["paused"] = True
        return bot_state["eth_balance"], bot_state["usd_balance"]
    except Exception as e:
        logger.error(f"Balance sync failed: {e}")
        if attempt < 2:
            logger.info(f"Retrying balance sync, attempt {attempt + 2}/3")
            time.sleep(1)
        else:
            logger.error("Failed to sync balances after 3 attempts")
            bot_state["paused"] = True
            return bot_state["eth_balance"], bot_state["usd_balance"]

def log_trade_to_ml_file(timestamp, trade_id, price, volume, profit):
    """
    Log trade data to gridbot_ml.log (CSV: timestamp, trade_id, price, volume, profit).
    This is the file that the LLM agent monitors for analysis.
    """
    headers = ["timestamp", "trade_id", "price", "volume", "profit"]
    row = {
        "timestamp": timestamp,
        "trade_id": trade_id,
        "price": price,
        "volume": volume,
        "profit": profit
    }
    
    with ml_log_lock:
        file_exists = os.path.isfile(gridbot_ml_log)
        with open(gridbot_ml_log, "a", newline="") as f:
            writer = csv.DictWriter(f, fieldnames=headers)
            if not file_exists or os.stat(gridbot_ml_log).st_size == 0:
                writer.writeheader()
            writer.writerow(row)

def initialize_ml_log():
    """
    Initialize the gridbot_ml.log file with headers if it doesn't exist.
    This ensures the file is created even before any trades occur.
    """
    try:
        if not os.path.exists(gridbot_ml_log):
            headers = ["timestamp", "trade_id", "price", "volume", "profit"]
            with open(gridbot_ml_log, "w", newline="") as f:
                writer = csv.DictWriter(f, fieldnames=headers)
                writer.writeheader()
            logger.info(f"Initialized gridbot_ml.log at {gridbot_ml_log}")
    except Exception as e:
        logger.error(f"Failed to initialize gridbot_ml.log: {e}")

def log_trade_profit(buy_price, sell_price, size, order_id):
    feature = bot_state["sell_orders"][-1].get("feature", "base") if bot_state["sell_orders"] else "base"
    profit_eth = (sell_price - buy_price) * size
    profit_usd = profit_eth * sell_price
    bot_state["total_pl"] += profit_eth
    config.TOTAL_PL += profit_usd
    
    # Log to standard logger
    logger.info(
        f"Trade {order_id}: Buy {buy_price:.2f}, Sell {sell_price:.2f}, Size {size:.6f}, "
        f"Profit: {profit_eth:.6f} ETH (${profit_usd:.4f}), Total P/L: {bot_state['total_pl']:.6f} ETH "
        f"(${config.TOTAL_PL:.4f}), feature={feature}"
    )
    
    # Log to ML analysis file (gridbot_ml.log)
    try:
        timestamp = datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S")
        log_trade_to_ml_file(timestamp, order_id, sell_price, size, profit_usd)
        logger.info(f"Trade logged to ML file: {order_id}")
    except Exception as e:
        logger.error(f"Failed to log trade to ML file: {e}")

def place_sell_after_buy(exchange, order_id, price, size, feature=None, feature_order_id=None):
    try:
        ticker = exchange.fetch_ticker(config.SYMBOL)
        current_ask = float(ticker["ask"])
        sell_price = max(price + config.GRID_SIZE, current_ask + 0.01)
        sell_price = float(exchange.price_to_precision(config.SYMBOL, sell_price))
        eth, _ = sync_balances(exchange)
        adjusted_size = min(
            eth,
            float(exchange.amount_to_precision(config.SYMBOL, size * config.SELL_SIZE_MULTIPLIER)),
        )
        if adjusted_size < config.MIN_POSITION_SIZE:
            logger.warning(
                f"Adjusted sell size {adjusted_size:.6f} below minimum {config.MIN_POSITION_SIZE:.6f}, skipping sell order"
            )
            return None
        # Find the buy order's feature if not provided
        if feature is None:
            feature = "base"
            for order in bot_state["buy_orders"]:
                if order["id"] == order_id:
                    feature = order.get("feature", "base")
                    break
        # Generate feature_order_id if not provided and feature is not base
        if feature != "base" and feature_order_id is None:
            feature_order_id = str(uuid4())
        logger.info(
            f"Placing sell after buy {order_id} at {sell_price:.2f} with size {adjusted_size:.6f}, feature={feature}, feature_order_id={feature_order_id}"
        )

        for attempt in range(3):
            if eth >= adjusted_size:
                try:
                    # Check if order placement is enabled
                    if not getattr(config, "ENABLE_ORDER_PLACEMENT", False):
                        # Simulation mode - create a fake order
                        import uuid
                        fake_order_id = f"SIM_{uuid.uuid4().hex[:8]}"
                        logger.info(f"[SIMULATION] Sell order {fake_order_id} SIMULATED at {sell_price:.2f}, size={adjusted_size:.6f}, feature={feature}")
                        # Create a fake order object for consistency
                        fake_order = {
                            "id": fake_order_id,
                            "status": "open",
                            "price": sell_price,
                            "amount": adjusted_size,
                            "feature": feature
                        }
                        if feature_order_id is not None:
                            fake_order["feature_order_id"] = feature_order_id
                        return fake_order
                    
                    order = exchange.create_limit_sell_order(
                        config.SYMBOL,
                        adjusted_size,
                        sell_price,
                        params={"post_only": True},
                    )
                    order = exchange.fetch_order(order["id"])
                    logger.info(
                        f"Sell order placed: {order['id']} at {sell_price:.2f}, Status: {order['status']}, feature={feature}, feature_order_id={feature_order_id}"
                    )
                    buy_prices[order["id"]] = price
                    order["feature"] = feature
                    if feature_order_id is not None:
                        order["feature_order_id"] = feature_order_id
                    return order
                except ccxt.InvalidOrder as e:
                    logger.error(f"Sell failed: {e}")
                    sell_price += 0.01
                    sell_price = float(exchange.price_to_precision(config.SYMBOL, sell_price))
                    logger.info(f"Retrying with adjusted price: {sell_price:.2f}, attempt {attempt + 1}/3")
                    time.sleep(1)
                except ccxt.AuthenticationError as e:
                    logger.error(f"Authentication error placing sell order: {e}")
                    bot_state["paused"] = True
                    return None
                except Exception as e:
                    logger.error(f"Sell failed: {e}")
                    time.sleep(1)
            else:
                logger.warning(f"Insufficient ETH: {eth:.6f} < {adjusted_size:.6f}, attempt {attempt + 1}/3")
                time.sleep(1)
        logger.error(f"Failed to place sell after 3 attempts for {order_id}, feature={feature}")
        return None
    except Exception as e:
        logger.error(f"Error placing sell order after buy {order_id}: {e}")
        return None

def place_buy_after_sell(exchange, order_id, price, size, feature=None, feature_order_id=None):
    try:
        buy_price = float(exchange.price_to_precision(config.SYMBOL, price - config.GRID_SIZE))
        adjusted_size = float(exchange.amount_to_precision(config.SYMBOL, config.POSITION_SIZE))
        cost = buy_price * adjusted_size
        # Find the sell order's feature if not provided
        if feature is None:
            feature = "base"
            for order in bot_state["sell_orders"]:
                if order["id"] == order_id:
                    feature = order.get("feature", "base")
                    break
        # Generate feature_order_id if not provided and feature is not base
        if feature != "base" and feature_order_id is None:
            feature_order_id = str(uuid4())
        logger.info(
            f"Placing buy after sell {order_id} at {buy_price:.2f} with size {adjusted_size:.6f}, feature={feature}, feature_order_id={feature_order_id}"
        )

        for attempt in range(3):
            _, usd = sync_balances(exchange)
            if usd >= cost:
                try:
                    # Check if order placement is enabled
                    if not getattr(config, "ENABLE_ORDER_PLACEMENT", False):
                        # Simulation mode - create a fake order
                        import uuid
                        fake_order_id = f"SIM_{uuid.uuid4().hex[:8]}"
                        logger.info(f"[SIMULATION] Buy order {fake_order_id} SIMULATED at {buy_price:.2f}, size={adjusted_size:.6f}, feature={feature}")
                        # Create a fake order object for consistency
                        fake_order = {
                            "id": fake_order_id,
                            "status": "open",
                            "price": buy_price,
                            "amount": adjusted_size,
                            "feature": feature
                        }
                        if feature_order_id is not None:
                            fake_order["feature_order_id"] = feature_order_id
                        return fake_order
                    
                    order = exchange.create_limit_buy_order(
                        config.SYMBOL,
                        adjusted_size,
                        buy_price,
                        params={"post_only": True},
                    )
                    order = exchange.fetch_order(order["id"])
                    logger.info(
                        f"Buy order placed: {order['id']} at {buy_price:.2f}, Status: {order['status']}, feature={feature}, feature_order_id={feature_order_id}"
                    )
                    prior_buy_price = buy_prices.pop(order_id, None)
                    if prior_buy_price:
                        log_trade_profit(prior_buy_price, price, size, order_id)
                    order["feature"] = feature
                    if feature_order_id is not None:
                        order["feature_order_id"] = feature_order_id
                    return order
                except ccxt.AuthenticationError as e:
                    logger.error(f"Authentication error placing buy order: {e}")
                    bot_state["paused"] = True
                    return None
                except Exception as e:
                    logger.error(f"Buy failed: {e}")
                    time.sleep(1)
            else:
                logger.warning(f"Insufficient USD: {usd:.2f} < {cost:.2f}, attempt {attempt + 1}/3")
                time.sleep(1)
        logger.error(f"Failed to place buy after 3 attempts for {order_id}, feature={feature}")
        return None
    except Exception as e:
        logger.error(f"Error placing buy order after sell {order_id}: {e}")
        return None

def replenish_eth(exchange, target_eth_amount):
    try:
        eth, usd = sync_balances(exchange)
        # Calculate locked funds from open orders
        locked_usd, locked_eth = calculate_locked_funds(
            bot_state["buy_orders"], bot_state["sell_orders"], get_current_price()
        )
        total_usd = usd + locked_usd
        total_eth = eth + locked_eth
        logger.info(
            f"Checking ETH replenishment: Current ETH={eth:.6f}, Locked ETH={locked_eth:.6f}, Total ETH={total_eth:.6f}, "
            f"Target ETH={target_eth_amount:.6f}, Free USD={usd:.2f}, Locked USD={locked_usd:.2f}, Total USD={total_usd:.2f}"
        )
        if total_eth >= target_eth_amount * config.REPLENISH_ETH_THRESHOLD:
            logger.info(
                f"Total ETH balance {total_eth:.6f} sufficient ({config.REPLENISH_ETH_THRESHOLD*100}% of target {target_eth_amount:.6f}), no replenishment needed"
            )
            return True
        eth_needed = target_eth_amount - total_eth
        if eth_needed < config.MIN_REPLENISH_AMOUNT:
            logger.info(
                f"ETH needed {eth_needed:.6f} below minimum {config.MIN_REPLENISH_AMOUNT}, skipping replenishment"
            )
            return True
        usd_cost = eth_needed * float(exchange.fetch_ticker(config.SYMBOL)["last"])
        if total_usd < usd_cost:
            logger.warning(f"Insufficient total USD {total_usd:.2f} to buy {eth_needed:.6f} ETH (cost {usd_cost:.2f})")
            return False
        if usd < config.MIN_USD_BALANCE + usd_cost:
            logger.warning(
                f"Free USD balance {usd:.2f} too low to maintain MIN_USD_BALANCE {config.MIN_USD_BALANCE:.2f} after replenishment"
            )
            return False
        if usd_cost <= 0:
            logger.error(f"Invalid usd_cost: {usd_cost}, skipping replenishment")
            return False
        logger.debug(f"Replenishing ETH: usd_cost={usd_cost}, type={type(usd_cost)}, eth_needed={eth_needed}")
        order = exchange.create_market_buy_order(
            config.SYMBOL, usd_cost, params={"createMarketBuyOrderRequiresPrice": False}
        )
        logger.info(f"Replenishing ETH: Attempted to buy ~{eth_needed:.6f} ETH for ${usd_cost:.2f}")
        order = exchange.fetch_order(order["id"])
        filled_amount = float(order["filled"]) if order["filled"] else 0.0
        logger.info(f"ETH replenished: Bought {filled_amount:.6f} ETH, ID: {order['id']}")
        return True
    except ccxt.AuthenticationError as e:
        logger.error(f"Authentication error replenishing ETH: {e}")
        bot_state["paused"] = True
        return False
    except Exception as e:
        logger.error(f"Failed to replenish ETH: {e}")
        return False

def apply_parameters(params, request_id, command, ws_manager, group="unknown"):
    logger.debug(
        f"Starting apply_parameters: command={command}, request_id={request_id}, group={group}, parameters={params}"
    )
    try:
        # Define all configurable parameters from config.py
        float_params = [
            "grid_size",
            "position_size",
            "max_order_range",
            "check_order_frequency",
            "initial_eth_percentage",
            "target_eth_buffer",
            "replenish_eth_threshold",
            "min_replenish_amount",
            "rebalance_eth_threshold",
            "ml_trend_weight",
            "ml_confidence_threshold",
            "ml_grid_adjust_factor",
            "ml_position_adjust_factor",
            "ml_max_adjustment_threshold",
            "volatility_grid_factor",
            "volatility_position_factor",
            "volume_factor_min",
            "volume_factor_max",
            "price_drift_threshold",
            "sell_size_multiplier",
            "ml_learning_rate",
            "online_learning_rate",
        ]
        int_params = [
            "num_buy_grid_lines",
            "num_sell_grid_lines",
            "max_total_orders",
            "stagnation_timeout",
            "volatility_window",
            "trade_lookback_minutes",
            "min_trade_minutes",
            "max_trade_lookback",
            "max_trades",
            "min_trades",
            "lookback",
            "sklearn_n_estimators",
            "sklearn_max_depth",
            "pytorch_hidden_size",
            "pytorch_num_layers",
            "pytorch_num_epochs",
            "pytorch_batch_size",
            "ml_batch_size",
            "retrain_interval_seconds",
        ]
        dict_params = ["ml_ensemble_weights", "ml_feature_weights"]

        # Load current config from client_config.json if it exists
        current_config = {}
        try:
            with open("client_config.json", "r") as f:
                current_config = json.load(f)
        except Exception as e:
            logger.warning(f"Failed to load client_config.json: {e}, using config.py defaults")

        # Initialize new_params with current values
        new_params = {
            key: current_config.get(key.upper(), getattr(config, key.upper()))
            for key in float_params + int_params + dict_params
        }

        # Check reset_required flag and reset cooldown
        reset_required = params.get("reset_required", False)
        last_reset_time = bot_state.get("last_reset_time", 0)
        reset_cooldown = 300  # 60 minutes in seconds
        reset_detected = False
        change_tolerance = 0.3  # 30% tolerance for parameter changes

        # Update parameters based on incoming message
        changes = {}
        for key in params:
            if key in [
                "reset_required",
                "timestamp",
                "predicted_price",
                "current_price",
                "confidence",
                "volatility",
                "volume",
                "trades",
                "reason",
            ]:
                continue  # Skip metadata fields
            current_value = new_params.get(key)
            new_value = params.get(key)
            if key in float_params:
                min_val = getattr(config, f"MIN_{key.upper()}", 0.0)
                max_val = getattr(config, f"MAX_{key.upper()}", float("inf"))
                new_params[key] = max(min_val, min(max_val, float(new_value)))
                if (
                    current_value is not None
                    and abs(new_params[key] - current_value) / current_value > change_tolerance
                ):
                    changes[key] = (current_value, new_params[key])
                    reset_detected = True
            elif key in int_params:
                min_val = getattr(config, f"MIN_{key.upper()}", 0)
                max_val = getattr(config, f"MAX_{key.upper()}", float("inf"))
                new_params[key] = max(min_val, min(max_val, int(new_value)))
                if (
                    current_value is not None
                    and abs(new_params[key] - current_value) / max(current_value, 1) > change_tolerance
                ):
                    changes[key] = (current_value, new_params[key])
                    reset_detected = True
            elif key in dict_params:
                new_params[key] = new_value
                if current_value != new_value:
                    changes[key] = (current_value, new_value)
                    reset_detected = True
            else:
                logger.warning(f"Ignoring unknown parameter: {key}")

        # Apply parameters and decide on reset
        if changes:
            logger.info(
                f"Applying parameter changes from {command} (request_id={request_id}, group={group}): {changes}"
            )
            for key, value in new_params.items():
                setattr(config, key.upper(), value)
                logger.debug(f"Set config.{key.upper()} = {value}")
            config_dict = {k.upper(): v for k, v in new_params.items()}
            try:
                with open("client_config.json", "w") as f:
                    json.dump(config_dict, f, indent=4)
                logger.info(f"Saved configuration to client_config.json: {config_dict}")
            except Exception as e:
                logger.error(f"Failed to save configuration: {e}")

            # Check reset conditions
            if reset_required or (reset_detected and time.time() - last_reset_time > reset_cooldown):
                bot_state["needs_reset"] = True
                bot_state["last_reset_time"] = time.time()
                logger.info(
                    f"Grid reset triggered for {command}, request_id={request_id}, group={group}, changes={changes}, reset_required={reset_required}"
                )
            else:
                bot_state["needs_reset"] = False
                logger.info(
                    f"Parameter changes applied without grid reset for {command}, request_id={request_id}, group={group}, changes={changes}"
                )
        else:
            logger.info(f"No parameter changes needed from {command} (request_id={request_id}, group={group})")

        # Send response to server
        response = {
            "command": command,
            "status": "applied",
            "request_id": request_id,
            "group": group,
            "timestamp": int(time.time() * 1000),
        }
        ws_manager.send(json.dumps(response))
        logger.info(f"Sent {command} response: {response}")
    except (ValueError, TypeError) as e:
        logger.error(
            f"Invalid parameters in {command} (request_id={request_id}, group={group}): {e}",
            exc_info=True,
        )
    except Exception as e:
        logger.error(
            f"Unexpected error applying parameters for {command} (request_id={request_id}, group={group}): {e}",
            exc_info=True,
        )

def adjust_grid_with_funds_check(ws_manager, exchange):
    try:
        logger.debug("Starting grid adjustment")
        volatility = bot_state.get("volatility", 0.06)
        config.GRID_SIZE = max(2.0, config.VOLATILITY_WINDOW * volatility * 0.3)
        position_size = min(0.0012, config.POSITION_SIZE * 1.2)  # Align with Section 10
        base_buy_lines = 20
        base_sell_lines = 20
        feature_buy_lines = 16  # Align with 26 total (10 base + 16 feature)
        feature_sell_lines = 16
        base_price = get_current_price()
        num_features = 8  # MACD, Bollinger Bands, Momentum, Volume Trend, RSI, EMA, ATR, VWAP
        lines_per_feature = max(8, feature_buy_lines // num_features)

        logger.info(
            f"Grid adjustment parameters: grid_size={config.GRID_SIZE:.2f}, position_size={position_size:.6f}, "
            f"base_buy_lines={base_buy_lines}, base_sell_lines={base_sell_lines}, "
            f"feature_buy_lines={feature_buy_lines}, feature_sell_lines={feature_sell_lines}, "
            f"lines_per_feature={lines_per_feature}, base_price={base_price:.2f}"
        )

        # Compute feature signals
        close_prices = ohlcv_df["close"].tail(100)
        high_prices = ohlcv_df["high"].tail(100)
        low_prices = ohlcv_df["low"].tail(100)
        volume = ohlcv_df["volume"].tail(100)
        feature_data = ohlcv_df.tail(100)

        macd, macd_signal = compute_macd(close_prices)
        upper_bb, lower_bb = compute_bollinger(close_prices)
        momentum = compute_momentum(close_prices)
        volume_trend = compute_volume_trend(volume)
        rsi = compute_rsi(close_prices)
        ema = compute_ema(close_prices)
        atr = compute_atr(high_prices, low_prices, close_prices)
        vwap = compute_vwap(feature_data)

        feature_names = ["MACD", "Bollinger Bands", "Momentum", "Volume Trend", "RSI", "EMA", "ATR", "VWAP"]
        features = [
            (macd.iloc[-1] - macd_signal.iloc[-1]) / (macd.std() + 1e-6),
            (close_prices.iloc[-1] - lower_bb.iloc[-1]) / (upper_bb.iloc[-1] - lower_bb.iloc[-1] + 1e-6),
            momentum.iloc[-1] / (momentum.std() + 1e-6),
            volume_trend.iloc[-1] / (volume_trend.std() + 1e-6),
            (rsi.iloc[-1] - 50) / (rsi.std() + 1e-6),
            (close_prices.iloc[-1] - ema.iloc[-1]) / (ema.std() + 1e-6),
            atr.iloc[-1] / (atr.std() + 1e-6),
            (close_prices.iloc[-1] - vwap.iloc[-1]) / (vwap.std() + 1e-6),
        ]
        feature_weights = np.array(features) / (np.abs(features).sum() + 1e-6)
        logger.info(
            f"Feature weights: {', '.join([f'{name}={weight:.4f}' for name, weight in zip(feature_names, feature_weights)])}"
        )

        # Cancel existing orders
        cancelled = cancel_orders(exchange, bot_state["buy_orders"] + bot_state["sell_orders"], config.SYMBOL)
        bot_state["buy_orders"] = []
        bot_state["sell_orders"] = []

        # Place base buy orders
        base_buy_count = 0
        for i in range(base_buy_lines):
            price = base_price - (i + 1) * config.GRID_SIZE
            price = float(exchange.price_to_precision(config.SYMBOL, price))
            if check_funds("buy", price, position_size, exchange):
                order_id = place_order("buy", price, position_size, ws_manager, exchange)
                if order_id:
                    bot_state["buy_orders"].append(
                        {
                            "id": order_id,
                            "side": "buy",
                            "price": price,
                            "size": position_size,
                            "status": "open",
                            "timestamp": int(time.time() * 1000),
                            "feature": "base",
                        }
                    )
                    base_buy_count += 1
                    logger.info(f"Base buy order placed: id={order_id}, price={price:.2f}, feature=base")
                else:
                    logger.warning(f"Failed to place base buy order at {price:.2f}, feature=base")
            else:
                logger.warning(f"Skipped base buy order at {price:.2f} due to insufficient funds, feature=base")

        # Place feature-based buy orders with dynamic cap logic
        feature_buy_count = 0
        FEATURE_ORDER_CAP = getattr(config, "FEATURE_ORDER_CAP", 84)
        SOFT_PER_FEATURE_CAP = FEATURE_ORDER_CAP // 2
        features_list = [fn.lower() for fn in feature_names]
        def count_open_feature_orders_for_side(bot_state, feature, side):
            orders = bot_state["buy_orders"] if side == "buy" else bot_state["sell_orders"]
            return sum(1 for o in orders if o.get("feature", "base").lower() == feature.lower() and o.get("status", "open") == "open")
        def count_total_open_feature_orders_for_side(bot_state, side):
            orders = bot_state["buy_orders"] if side == "buy" else bot_state["sell_orders"]
            return sum(1 for o in orders if o.get("feature", "base").lower() in features_list and o.get("status", "open") == "open")

        for feature_idx, (weight, feature_name) in enumerate(zip(feature_weights, feature_names)):
            for line_idx in range(lines_per_feature):
                open_feature_orders = count_total_open_feature_orders_for_side(bot_state, "buy")
                open_for_this_feature = count_open_feature_orders_for_side(bot_state, feature_name, "buy")
                # Allow if under global cap, and (soft cap not exceeded or others are underutilized)
                allow = False
                if open_feature_orders < FEATURE_ORDER_CAP:
                    if open_for_this_feature < SOFT_PER_FEATURE_CAP:
                        allow = True
                    else:
                        # Allow exceeding soft cap if slots are available
                        allow = True
                        logger.info(f"[DYNAMIC CAP] Feature '{feature_name}' exceeding soft cap ({SOFT_PER_FEATURE_CAP}), but slots available (total open: {open_feature_orders}/{FEATURE_ORDER_CAP})")
                if not allow:
                    logger.info(f"[DYNAMIC CAP] Feature '{feature_name}' cannot place more buy orders: open={open_for_this_feature}, total open={open_feature_orders}, cap={FEATURE_ORDER_CAP}")
                    continue
                offset = (base_buy_lines + feature_idx * lines_per_feature + line_idx + 1) * config.GRID_SIZE
                price_adjustment = weight * config.GRID_SIZE * 0.5
                price = base_price - offset + price_adjustment
                price = float(exchange.price_to_precision(config.SYMBOL, price))
                if check_funds("buy", price, position_size, exchange):
                    order_id = place_order("buy", price, position_size, ws_manager, exchange)
                    if order_id:
                        bot_state["buy_orders"].append(
                            {
                                "id": order_id,
                                "side": "buy",
                                "price": price,
                                "size": position_size,
                                "status": "open",
                                "timestamp": int(time.time() * 1000),
                                "feature": feature_name,
                            }
                        )
                        feature_buy_count += 1
                        logger.info(
                            f"Feature buy order placed: id={order_id}, price={price:.2f}, feature={feature_name}, weight={weight:.4f}"
                        )
                    else:
                        logger.warning(f"Failed to place feature buy order at {price:.2f}, feature={feature_name}")
                else:
                    logger.warning(f"Skipped feature buy order at {price:.2f} due to insufficient funds, feature={feature_name}")

        # Log open feature order counts for buy side
        open_feature_orders = count_total_open_feature_orders_for_side(bot_state, "buy")
        per_feature_counts = {fn: count_open_feature_orders_for_side(bot_state, fn, "buy") for fn in features_list}
        logger.info(f"[DYNAMIC CAP] Total open feature buy orders: {open_feature_orders}/{FEATURE_ORDER_CAP}, per-feature: {per_feature_counts}")

        logger.info(f"[SUMMARY] Base buy orders placed: {base_buy_count}, Feature buy orders placed: {feature_buy_count}")
        # --- ENFORCE ORDER SPACING: After buy checks ---
        logger.info("[ORDER SPACING] [AFTER BUY] Enforcing order spacing after buy order checks.")
        enforce_order_spacing(bot_state, min_spacing=getattr(config, "MIN_FEATURE_ORDER_SPACING", 2.0))

        # Place base sell orders
        base_sell_count = 0
        for i in range(base_sell_lines):
            price = base_price + (i + 1) * config.GRID_SIZE
            price = float(exchange.price_to_precision(config.SYMBOL, price))
            if check_funds("sell", price, position_size, exchange):
                order_id = place_order("sell", price, position_size, ws_manager, exchange)
                if order_id:
                    bot_state["sell_orders"].append(
                        {
                            "id": order_id,
                            "side": "sell",
                            "price": price,
                            "size": position_size,
                            "status": "open",
                            "timestamp": int(time.time() * 1000),
                            "feature": "base",
                        }
                    )
                    base_sell_count += 1
                    logger.info(f"Base sell order placed: id={order_id}, price={price:.2f}, feature=base")
                else:
                    logger.warning(f"Failed to place base sell order at {price:.2f}, feature=base")
            else:
                logger.warning(f"Skipped base sell order at {price:.2f} due to insufficient funds, feature=base")

        # Place feature-based sell orders with dynamic cap logic
        feature_sell_count = 0
        for feature_idx, (weight, feature_name) in enumerate(zip(feature_weights, feature_names)):
            for line_idx in range(lines_per_feature):
                open_feature_orders = count_total_open_feature_orders_for_side(bot_state, "sell")
                open_for_this_feature = count_open_feature_orders_for_side(bot_state, feature_name, "sell")
                allow = False
                if open_feature_orders < FEATURE_ORDER_CAP:
                    if open_for_this_feature < SOFT_PER_FEATURE_CAP:
                        allow = True
                    else:
                        allow = True
                        logger.info(f"[DYNAMIC CAP] Feature '{feature_name}' exceeding soft cap ({SOFT_PER_FEATURE_CAP}), but slots available (total open: {open_feature_orders}/{FEATURE_ORDER_CAP})")
                if not allow:
                    logger.info(f"[DYNAMIC CAP] Feature '{feature_name}' cannot place more sell orders: open={open_for_this_feature}, total open={open_feature_orders}, cap={FEATURE_ORDER_CAP}")
                    continue
                offset = (base_sell_lines + feature_idx * lines_per_feature + line_idx + 1) * config.GRID_SIZE
                price_adjustment = weight * config.GRID_SIZE * 0.5
                price = base_price + offset + price_adjustment
                price = float(exchange.price_to_precision(config.SYMBOL, price))
                if check_funds("sell", price, position_size, exchange):
                    order_id = place_order("sell", price, position_size, ws_manager, exchange)
                    if order_id:
                        bot_state["sell_orders"].append(
                            {
                                "id": order_id,
                                "side": "sell",
                                "price": price,
                                "size": position_size,
                                "status": "open",
                                "timestamp": int(time.time() * 1000),
                                "feature": feature_name,
                            }
                        )
                        feature_sell_count += 1
                        logger.info(
                            f"Feature sell order placed: id={order_id}, price={price:.2f}, feature={feature_name}, weight={weight:.4f}"
                        )
                    else:
                        logger.warning(f"Failed to place feature sell order at {price:.2f}, feature={feature_name}")
                else:
                    logger.warning(f"Skipped feature sell order at {price:.2f} due to insufficient funds, feature={feature_name}")

        # Log open feature order counts for sell side
        open_feature_orders = count_total_open_feature_orders_for_side(bot_state, "sell")
        per_feature_counts = {fn: count_open_feature_orders_for_side(bot_state, fn, "sell") for fn in features_list}
        logger.info(f"[DYNAMIC CAP] Total open feature sell orders: {open_feature_orders}/{FEATURE_ORDER_CAP}, per-feature: {per_feature_counts}")

        logger.info(f"[SUMMARY] Base sell orders placed: {base_sell_count}, Feature sell orders placed: {feature_sell_count}")
        # --- ENFORCE ORDER SPACING: After sell checks ---
        logger.info("[ORDER SPACING] [AFTER SELL] Enforcing order spacing after sell order checks.")
        enforce_order_spacing(bot_state, min_spacing=getattr(config, "MIN_FEATURE_ORDER_SPACING", 2.0))

        logger.info(
            f"Grid adjusted: {len(bot_state['buy_orders'])} buy orders, {len(bot_state['sell_orders'])} sell orders"
        )
        bot_state["paused"] = False
    except Exception as e:
        logger.error(f"Failed to adjust grid: {e}")
    #    bot_state["paused"] = True

def check_funds(order_type, price, size, exchange):
    try:
        balance = exchange.fetch_balance()
        usd_balance = float(balance.get("USD", {}).get("free", 0.0))
        eth_balance = float(balance.get("ETH", {}).get("free", 0.0))
        logger.debug(
            f"Checking funds: order_type={order_type}, price={price:.2f}, size={size:.6f}, usd_balance={usd_balance:.2f}, eth_balance={eth_balance:.6f}"
        )
        if order_type == "buy":
            cost = price * size
            if usd_balance < cost:
                logger.warning(
                    f"Insufficient USD {usd_balance:.2f} to buy {size:.6f} ETH at {price:.2f} (cost {cost:.2f})"
                )
                return False
        else:
            if eth_balance < size:
                logger.warning(f"Insufficient ETH {eth_balance:.6f} to sell {size:.6f} at {price:.2f}")
                return False
        return True
    except Exception as e:
        logger.error(f"Failed to check funds: {e}")
        return False

def check_sgd_mse(sgd_mse):
    """
    Validates and analyzes SGD model MSE/loss for optimization and debugging.
    
    Args:
        sgd_mse: The SGD model's MSE/loss value (can be None, float, or array-like)
    
    Returns:
        bool: True if MSE is acceptable, False if optimization needed
    """
    try:
        # Handle None case
        if sgd_mse is None:
            logger.warning("SGD MSE is None - model may not have converged or trained properly")
            logger.debug("Debug prompt: Check SGD model training parameters - max_iter, tol, learning_rate_init")
            logger.debug("Optimization prompt: Consider increasing max_iter or adjusting learning rate")
            return False
        
        # Handle array-like MSE (multiple values)
        if hasattr(sgd_mse, '__iter__') and not isinstance(sgd_mse, str):
            try:
                mse_value = float(sgd_mse[-1])  # Take last value if array
                logger.debug(f"SGD MSE array detected, using final value: {mse_value:.6f}")
            except (IndexError, ValueError, TypeError):
                logger.error("Failed to extract MSE from array-like object")
                logger.debug("Debug prompt: Check SGD model loss_ or mse_ attribute format")
                return False
        else:
            try:
                mse_value = float(sgd_mse)
            except (ValueError, TypeError):
                logger.error(f"Invalid SGD MSE format: {type(sgd_mse)} - {sgd_mse}")
                logger.debug("Debug prompt: Verify SGD model has valid loss_ or mse_ attribute")
                return False
        
        # Validate MSE value ranges and provide optimization guidance
        if mse_value < 0:
            logger.error(f"Invalid SGD MSE (negative): {mse_value:.6f}")
            logger.debug("Debug prompt: Check model implementation - MSE should be non-negative")
            return False
        elif mse_value == 0:
            logger.warning("SGD MSE is exactly 0 - possible overfitting or perfect fit")
            logger.debug("Optimization prompt: Check for data leakage or overfitting")
            return True
        elif mse_value < 0.001:
            logger.info(f"Excellent SGD MSE: {mse_value:.6f} - model performing very well")
            return True
        elif mse_value < 0.01:
            logger.info(f"Good SGD MSE: {mse_value:.6f} - acceptable model performance")
            return True
        elif mse_value < 0.1:
            logger.warning(f"Moderate SGD MSE: {mse_value:.6f} - model may need optimization")
            logger.debug("Optimization prompt: Consider feature engineering, hyperparameter tuning, or more training data")
            return True
        elif mse_value < 1.0:
            logger.warning(f"High SGD MSE: {mse_value:.6f} - significant optimization needed")
            logger.debug("Optimization prompt: Review feature selection, increase training iterations, adjust learning rate")
            return False
        else:
            logger.error(f"Very high SGD MSE: {mse_value:.6f} - model training likely failed")
            logger.debug("Debug prompt: Check data preprocessing, feature scaling, and model convergence")
            logger.debug("Optimization prompt: Verify data quality, increase max_iter, or try different solver")
            return False
            
    except Exception as e:
        logger.error(f"Critical error in SGD MSE validation: {e}")
        logger.debug(f"Debug prompt: Exception in check_sgd_mse - investigate: {type(e).__name__}: {str(e)}")
        return False

def place_order(side, price, size, ws_manager, exchange):
    try:
        # Check if order placement is enabled
        if not getattr(config, "ENABLE_ORDER_PLACEMENT", False):
            # Simulation mode - create a fake order ID and log the action
            import uuid
            fake_order_id = f"SIM_{uuid.uuid4().hex[:8]}"
            logger.info(f"[SIMULATION] {side.capitalize()} order {fake_order_id} SIMULATED at {price:.2f}, size={size:.6f}")
            return fake_order_id
            
        # Enforce global feature order spacing for feature orders
        min_feature_spacing = getattr(config, "MIN_FEATURE_ORDER_SPACING", 0.005)
        # Determine if this is a feature order (not base)
        feature = None
        if hasattr(exchange, 'last_feature'):
            feature = getattr(exchange, 'last_feature')
        if feature is None:
            feature = 'base'
        is_feature = feature not in (None, 'base')
        if is_feature:
            if not can_place_feature_order_with_global_spacing(bot_state, price, min_feature_spacing):
                logger.info(f"[SPACING] Blocked feature order at {price:.2f} due to global spacing constraint.")
                return None
        order = exchange.create_limit_order(
            symbol=config.SYMBOL,
            side=side,
            amount=size,
            price=price,
            params={"post_only": True},
        )
        order_id = order["id"]
        feature = order.get("feature", "base")  # Get feature from order if set
        logger.info(f"{side.capitalize()} order {order_id} placed at {price:.2f}, size={size:.6f}, feature={feature}")
        order_msg = {
            "type": "order",
            "id": order_id,
            "side": side,
            "price": price,
            "size": size,
            "status": "open",
            "timestamp": int(time.time() * 1000),
            "feature": feature,
        }
        ws_manager.send(json.dumps([order_msg]))
        logger.info(f"Sent order to server: {order_msg}")
        # If feature order, update last_feature_order_price
        if feature not in (None, "base"):
            set_last_feature_order_price(bot_state, price)
        return order_id
    except ccxt.AuthenticationError as e:
        logger.error(f"Authentication error placing {side} order: {e}")
        bot_state["paused"] = True
        return None
    except Exception as e:
        logger.error(f"Failed to place {side} order: {e}")
        return None

# --- Helper: Check Global Feature Order Spacing ---
def can_place_feature_order_with_spacing(bot_state, price, min_spacing=0.005):
    """
    Prevents placing a new feature order if any open feature order (buy or sell) is within min_spacing (fractional, e.g. 0.005 = 0.5%) of price.
    Returns True if no conflict, False if too close to any open feature order.
    """
    all_orders = bot_state.get('buy_orders', []) + bot_state.get('sell_orders', [])
    for o in all_orders:
        if o.get('feature') not in (None, 'base') and o.get('status', 'open') == 'open':
            existing_price = float(o.get('price', 0))
            if abs(existing_price - price) < (min_spacing * price):
                return False
    return True

def get_current_price():
    try:
        ticker = exchange.fetch_ticker(config.SYMBOL)
        price = float(ticker["last"])
        logger.debug(f"Fetched current price: {price:.2f}")
        return price
    except Exception as e:
        logger.error(f"Failed to fetch current price: {e}")
        return bot_state.get("current_price", bot_state.get("initial_buy_price", 0.0))

def cancel_orders(exchange, orders, symbol, delay=0.1):
    cancelled_orders = []
    for order in orders:
        try:
            order_info = exchange.fetch_order(order["id"], symbol)
            if order_info["status"] in ["closed", "canceled"]:
                logger.info(f"Order {order['id']} already {order_info['status']}, skipping cancellation")
                continue
            exchange.cancel_order(order["id"], symbol)
            logger.info(f"Cancelled order {order['id']}")
            cancelled_orders.append(order)
            time.sleep(delay)
        except ccxt.AuthenticationError as e:
            logger.error(f"Authentication error cancelling order {order['id']}: {e}")
            bot_state["paused"] = True
        except Exception as e:
            logger.error(f"Failed to cancel order {order['id']}: {str(e)}, order_info={order}")
        try:
            exchange.cancelOrders([order["id"] for order in orders], symbol=config.SYMBOL)
            logger.info(f"Canceled orders: {[order['id'] for order in orders]}")
        except Exception as e:
            logger.error(f"Cancel order error: {e}, orders={[order['id'] for order in orders]}")
        time.sleep(delay)
    return cancelled_orders

def order_to_dict(order, status=None):
    order_dict = {
        "type": "order",
        "id": order.get("id", ""),
        "side": order.get("side", ""),
        "price": float(order.get("price", 0.0)),
        "size": float(order.get("size", order.get("amount", 0.0))),
        "status": status if status is not None else order.get("status", "unknown"),
        "timestamp": int(order.get("timestamp", time.time() * 1000)),
        "feature": order.get("feature", "base"),
    }
    return order_dict

def fetch_historical_data(exchange, symbol, timeframe="1m", limit=config.LOOKBACK):
    try:
        ohlcv = exchange.fetch_ohlcv(symbol, timeframe, limit=limit)
        df = pd.DataFrame(ohlcv, columns=["timestamp", "open", "high", "low", "close", "volume"])
        df["timestamp"] = pd.to_datetime(df["timestamp"], unit="ms", errors="coerce").dt.tz_localize("UTC")
        if df["timestamp"].isna().any():
            logger.warning("Invalid timestamps in historical data, setting to current time")
            df["timestamp"] = pd.Timestamp.now(tz="UTC")
        df["trades"] = 0
        df["rsi"] = compute_rsi(df["close"], periods=config.RSI_PERIOD)
        df["ema"] = compute_ema(df["close"], span=config.EMA_SPAN)
        df["volatility"] = df["close"].pct_change().rolling(config.VOLATILITY_WINDOW, min_periods=1).std() * 100
        df["macd"], df["macd_signal"] = compute_macd(
            df["close"],
            fast=config.MACD_FAST,
            slow=config.MACD_SLOW,
            signal=config.MACD_SIGNAL,
        )
        df["bollinger_upper"], df["bollinger_lower"] = compute_bollinger(
            df["close"],
            window=config.BOLLINGER_WINDOW,
            num_std=config.BOLLINGER_NUM_STD,
        )
        df["momentum"] = compute_momentum(df["close"], periods=config.MOMENTUM_PERIOD)
        df["volume_trend"] = compute_volume_trend(df["volume"])
        df["atr"] = compute_atr(df["high"], df["low"], df["close"], periods=config.ATR_PERIOD)
        df["vwap"] = compute_vwap(df, period=config.VWAP_PERIOD)
        df["predicted_price"] = df["close"].shift(-1).fillna(df["close"].mean())
        df["grid_level"] = 0
        df = (
            df.bfill()
            .ffill()
            .fillna(
                {
                    "rsi": config.LOG_DEFAULT_RSI,
                    "ema": df["close"].mean(),
                    "volatility": config.LOG_DEFAULT_VOLATILITY,
                    "macd": config.LOG_DEFAULT_MACD,
                    "macd_signal": config.LOG_DEFAULT_MACD_SIGNAL,
                    "bollinger_upper": df["close"].mean(),
                    "bollinger_lower": df["close"].mean(),
                    "momentum": config.LOG_DEFAULT_MOMENTUM,
                    "volume_trend": config.LOG_DEFAULT_VOLUME_TREND,
                    "atr": df["close"].mean() * 0.01,
                    "vwap": df["close"].mean(),
                    "predicted_price": df["close"].mean(),
                    "grid_level": 0,
                    "trades": config.DEFAULT_TRADE_COUNT,
                }
            )
        )
        logger.debug(f"Fetched historical data: {len(df)} rows, last timestamp={df['timestamp'].iloc[-1]}")
        return df
    except Exception as e:
        logger.error(f"Error fetching historical data: {e}")
        return None

# --- Utility: Count Open Feature Orders ---
def count_open_feature_orders(bot_state, feature, side):
    """
    Count open orders for a given feature and side ('buy' or 'sell').
    """
    orders = bot_state["buy_orders"] if side == "buy" else bot_state["sell_orders"]
    return sum(1 for o in orders if o.get("feature") == feature and o.get("status", "open") == "open")

def count_total_open_feature_orders(bot_state):
    """
    Count all open feature-based orders (across all features and both sides).
    """
    return sum(1 for o in bot_state["buy_orders"] + bot_state["sell_orders"] if o.get("feature") not in (None, "base") and o.get("status", "open") == "open")

def count_total_open_base_orders(bot_state):
    """
    Count all open base grid orders (across both sides).
    """
    return sum(1 for o in bot_state["buy_orders"] + bot_state["sell_orders"] if o.get("feature", "base") == "base" and o.get("status", "open") == "open")

def log_feature_trade(
    timestamp, feature, order_type, order_id, price, size, status, profit=None
):
    """
    Append a feature-based trade event to feature_trades.csv.
    """
    headers = [
        "timestamp", "feature", "order_type", "order_id", "price", "size", "status", "profit"
    ]
    row = {
        "timestamp": timestamp,
        "feature": feature,
        "order_type": order_type,
        "order_id": order_id,
        "price": price,
        "size": size,
        "status": status,
        "profit": profit if profit is not None else ""
    }
    with csv_lock:
        file_exists = os.path.isfile(feature_trades_csv)
        with open(feature_trades_csv, "a", newline="") as f:
            writer = csv.DictWriter(f, fieldnames=headers)
            if not file_exists or os.stat(feature_trades_csv).st_size == 0:
                writer.writeheader()
            writer.writerow(row)
            
def recommend_dynamic_params(usd_balance, eth_balance, volatility_val, meta_confidence, recent_accuracy=None, price_val=None):
    """
    Suggests config parameter adjustments based on balances, volatility, and meta-model confidence.
    Only logs recommendations; user should update config.py for persistent changes.
    """
    recommendations = {}
    # 1. Adjust EMA/ATR/VWAP/Bollinger periods: lower in high vol, higher in low vol
    if volatility_val is not None:
        if volatility_val > 0.25:
            recommendations['EMA_SPAN'] = max(config.MIN_EMA_SPAN, config.EMA_SPAN - 2)
            recommendations['ATR_PERIOD'] = max(config.MIN_ATR_PERIOD, config.ATR_PERIOD - 2)
            recommendations['VWAP_PERIOD'] = max(config.MIN_VWAP_PERIOD, config.VWAP_PERIOD - 2)
            recommendations['BOLLINGER_WINDOW'] = max(config.MIN_BOLLINGER_WINDOW, config.BOLLINGER_WINDOW - 2)
        elif volatility_val < 0.13:
            recommendations['EMA_SPAN'] = min(config.MAX_EMA_SPAN, config.EMA_SPAN + 2)
            recommendations['ATR_PERIOD'] = min(config.MAX_ATR_PERIOD, config.ATR_PERIOD + 2)
            recommendations['VWAP_PERIOD'] = min(config.MAX_VWAP_PERIOD, config.VWAP_PERIOD + 2)
            recommendations['BOLLINGER_WINDOW'] = min(config.MAX_BOLLINGER_WINDOW, config.BOLLINGER_WINDOW + 2)
    # 2. Adjust DCA min_spacing: relax if balances are healthy, tighten if one side is low
    min_spacing = 0.005
    total_value = usd_balance + eth_balance * (price_val if price_val else 1)
    eth_pct = (eth_balance * (price_val if price_val else 1)) / total_value if total_value > 0 else 0.5
    if usd_balance < config.MIN_USD_BALANCE * 2:
        min_spacing = 0.008  # tighten DCA if USD low
    elif eth_pct < 0.2:
        min_spacing = 0.008  # tighten DCA if ETH low
    elif usd_balance > config.MIN_USD_BALANCE * 10 and eth_pct > 0.5:
        min_spacing = 0.003  # relax if healthy
    recommendations['DCA_min_spacing'] = min_spacing
    # 3. Adjust volatility threshold: lower if meta confidence is high, raise if low
    if meta_confidence is not None:
        if meta_confidence > 0.8:
            recommendations['VOLATILITY_THRESHOLD'] = max(0.10, config.VOLATILITY_THRESHOLD - 0.02)
        elif meta_confidence < 0.6:
            recommendations['VOLATILITY_THRESHOLD'] = min(0.3, config.VOLATILITY_THRESHOLD + 0.02)
    # 4. If recent_accuracy is provided and low, suggest more aggressive changes
    if recent_accuracy is not None and recent_accuracy < 0.5:
        recommendations['EMA_SPAN'] = max(config.MIN_EMA_SPAN, config.EMA_SPAN - 4)
        recommendations['ATR_PERIOD'] = max(config.MIN_ATR_PERIOD, config.ATR_PERIOD - 4)
        recommendations['DCA_min_spacing'] = 0.01
        recommendations['VOLATILITY_THRESHOLD'] = max(0.08, config.VOLATILITY_THRESHOLD - 0.04)
    logger.info(f"[PARAM RECOMMEND] Based on balances (USD={usd_balance:.2f}, ETH={eth_balance:.4f}), volatility={volatility_val}, meta_conf={meta_confidence}, accuracy={recent_accuracy}: {recommendations}")
    logger.info("[PARAM RECOMMEND] To persist these changes, update config.py accordingly.")
    return recommendations

###############################################################
# === NEW: Dynamic Auto Parameter Optimization (Non-Disruptive) ===

def auto_optimize_params_bi(performance_history, config_module, min_trades=10):
    """
    Dynamically optimize config parameters based on recent performance, in both directions.
    Args:
        performance_history (list): List of dicts with keys like 'profit', 'trade_count', etc.
        config_module: The config module to update.
        min_trades (int): Minimum trades before optimizing.
    Returns:
        dict: Proposed parameter changes.
    """
    if len(performance_history) < min_trades:
        return {}
    avg_profit = np.mean([x['profit'] for x in performance_history])
    avg_trades = np.mean([x['trade_count'] for x in performance_history])
    changes = {}
    # If profit is low, try increasing grid size (wider grid)
    if avg_profit < 0:
        new_grid = min(config_module.GRID_SIZE * 1.1, getattr(config_module, 'MAX_GRID_SIZE', 100))
        if new_grid != config_module.GRID_SIZE:
            changes['GRID_SIZE'] = new_grid
    # If profit is high, try decreasing grid size (tighter grid for more trades)
    elif avg_profit > 0.5:
        new_grid = max(config_module.GRID_SIZE * 0.9, getattr(config_module, 'MIN_GRID_SIZE', 1))
        if new_grid != config_module.GRID_SIZE:
            changes['GRID_SIZE'] = new_grid
    # If trade count is too low, decrease grid size (tighter grid)
    if avg_trades < getattr(config_module, 'MIN_TRADES_PER_PERIOD', 1):
        new_grid = max(config_module.GRID_SIZE * 0.9, getattr(config_module, 'MIN_GRID_SIZE', 1))
        if new_grid != config_module.GRID_SIZE:
            changes['GRID_SIZE'] = new_grid
    # If trade count is too high, increase grid size (wider grid)
    elif avg_trades > getattr(config_module, 'MAX_TRADES_PER_PERIOD', 100):
        new_grid = min(config_module.GRID_SIZE * 1.1, getattr(config_module, 'MAX_GRID_SIZE', 100))
        if new_grid != config_module.GRID_SIZE:
            changes['GRID_SIZE'] = new_grid
    # Example: Adjust ML_TREND_WEIGHT up/down based on profit trend
    if avg_profit > 0.5:
        new_weight = min(config_module.ML_TREND_WEIGHT + 0.05, getattr(config_module, 'MAX_ML_TREND_WEIGHT', 0.9))
        if new_weight != config_module.ML_TREND_WEIGHT:
            changes['ML_TREND_WEIGHT'] = new_weight
    elif avg_profit < 0:
        new_weight = max(config_module.ML_TREND_WEIGHT - 0.05, getattr(config_module, 'MIN_ML_TREND_WEIGHT', 0.1))
        if new_weight != config_module.ML_TREND_WEIGHT:
            changes['ML_TREND_WEIGHT'] = new_weight
    # Example: Adjust ML_CONFIDENCE_THRESHOLD up/down based on trade count
    if avg_trades > getattr(config_module, 'MAX_TRADES_PER_PERIOD', 100):
        new_thresh = min(config_module.ML_CONFIDENCE_THRESHOLD + 0.05, getattr(config_module, 'MAX_CONFIDENCE_THRESHOLD', 0.9))
        if new_thresh != config_module.ML_CONFIDENCE_THRESHOLD:
            changes['ML_CONFIDENCE_THRESHOLD'] = new_thresh
    elif avg_trades < getattr(config_module, 'MIN_TRADES_PER_PERIOD', 1):
        new_thresh = max(config_module.ML_CONFIDENCE_THRESHOLD - 0.05, getattr(config_module, 'MIN_CONFIDENCE_THRESHOLD', 0.3))
        if new_thresh != config_module.ML_CONFIDENCE_THRESHOLD:
            changes['ML_CONFIDENCE_THRESHOLD'] = new_thresh
    return changes

def apply_param_changes_bi(config_module, changes):
    """
    Applies parameter changes to the config module.
    Args:
        config_module: The config module to update.
        changes (dict): Dict of param: value.
    """
    for k, v in changes.items():
        setattr(config_module, k, v)
    logger.info(f"[AUTO-OPTIMIZE] Applied parameter changes: {changes}")
# (No-op: superseded by apply_param_changes_bi)
    pass

# --- Order Spacing Enforcement ---

def enforce_order_spacing(bot_state, min_spacing=2.0, feature_name=None, recent_trade_data=None):
    """
    Enforce spacing only for feature orders (not base), grouped by feature, for each side. Base orders are left untouched.
    Each feature's orders are spaced independently.
    """
    try:
        logger.info("[ORDER SPACING] --- Begin Spacing Enforcement ---")
        for side in ["buy_orders", "sell_orders"]:
            orders = bot_state.get(side, [])
            logger.info(f"[ORDER SPACING] Checking {side}: {len(orders)} open orders.")
            # Group feature orders by feature (exclude base orders)
            feature_orders = {}
            for o in orders:
                feature = o.get("feature")
                if feature and feature != "base":
                    feature_orders.setdefault(feature, []).append(o)
            logger.info(f"[ORDER SPACING] {side}: {sum(len(v) for v in feature_orders.values())} feature orders across {len(feature_orders)} features.")
            for feature, orders_list in feature_orders.items():
                logger.info(f"[ORDER SPACING] {side} feature '{feature}': {len(orders_list)} orders before spacing check.")
                if len(orders_list) < 2:
                    logger.info(f"[ORDER SPACING] {side} feature '{feature}': Only one order, skipping spacing check.")
                    continue
                # --- Corrected sorting and spacing logic ---
                if side == "buy_orders":
                    # For buys: sort descending (highest first), space downward
                    sorted_orders = sorted(orders_list, key=lambda o: o.get("price", 0), reverse=True)
                else:
                    # For sells: sort ascending (lowest first), space upward
                    sorted_orders = sorted(orders_list, key=lambda o: o.get("price", 0))
                adjusted = False
                prev_price = None
                # Dynamic spacing: use feature value if provided, else min_spacing
                dynamic_spacing = min_spacing
                if feature_name and recent_trade_data is not None and feature_name in recent_trade_data.columns:
                    try:
                        dynamic_spacing = float(recent_trade_data[feature_name].iloc[-1])
                        if not (0.01 <= dynamic_spacing <= 100):
                            dynamic_spacing = min_spacing
                    except Exception:
                        dynamic_spacing = min_spacing
                logger.info(f"[ORDER SPACING] {side} feature '{feature}': Using spacing {dynamic_spacing:.2f}.")
                for idx, order in enumerate(sorted_orders):
                    price = order.get("price", 0)
                    if prev_price is not None:
                        if side == "buy_orders":
                            # Each next buy should be at least dynamic_spacing LOWER than previous
                            gap = prev_price - price
                            logger.info(f"[ORDER SPACING] Buy order id={order.get('id')}: price={price:.2f}, prev_price={prev_price:.2f}, gap={gap:.2f}")
                            if gap < dynamic_spacing:
                                new_price = prev_price - dynamic_spacing
                                logger.info(f"[ORDER SPACING] Feature '{feature}' buy order id={order.get('id')} too close to previous (${price:.2f} vs ${prev_price:.2f}), adjusting to ${new_price:.2f} (spacing={dynamic_spacing:.2f})")
                                cancel_and_replace_order(order, new_price, side, bot_state)
                                adjusted = True
                                prev_price = new_price
                            else:
                                prev_price = price
                        else:
                            # Each next sell should be at least dynamic_spacing HIGHER than previous
                            gap = price - prev_price
                            logger.info(f"[ORDER SPACING] Sell order id={order.get('id')}: price={price:.2f}, prev_price={prev_price:.2f}, gap={gap:.2f}")
                            if gap < dynamic_spacing:
                                new_price = prev_price + dynamic_spacing
                                logger.info(f"[ORDER SPACING] Feature '{feature}' sell order id={order.get('id')} too close to previous (${price:.2f} vs ${prev_price:.2f}), adjusting to ${new_price:.2f} (spacing={dynamic_spacing:.2f})")
                                cancel_and_replace_order(order, new_price, side, bot_state)
                                adjusted = True
                                prev_price = new_price
                            else:
                                prev_price = price
                    else:
                        prev_price = price
                if adjusted:
                    logger.info(f"[ORDER SPACING] {side} feature '{feature}' orders adjusted for spacing {dynamic_spacing:.2f}.")
                else:
                    logger.info(f"[ORDER SPACING] {side} feature '{feature}' orders already properly spaced.")
        logger.info("[ORDER SPACING] --- End Spacing Enforcement ---")
    except Exception as e:
        logger.error(f"[ORDER SPACING] Error enforcing order spacing: {e}")

def cancel_and_replace_order(order, new_price, side, bot_state):
    """
    Cancel the order on the exchange, remove from bot_state, and place a new order at new_price, updating bot_state.
    """
    try:
        order_id = order.get('id')
        # 1. Cancel the order on the exchange
        if 'exchange' in globals():
            try:
                exchange.cancel_order(order_id)
                logger.info(f"[ORDER SPACING] Cancelled order id={order_id} on exchange.")
            except Exception as e:
                logger.error(f"[ORDER SPACING] Failed to cancel order id={order_id}: {e}")
        # 2. Remove from bot_state
        order_list = bot_state[side]
        bot_state[side] = [o for o in order_list if o.get('id') != order_id]
        # 3. Place new order at new_price
        symbol = getattr(config, 'SYMBOL', None)
        # Use the same position size as the cancelled order (try 'size' first, then 'amount', then fallback)
        amount = order.get('size')
        if amount is None:
            amount = order.get('amount', getattr(config, 'POSITION_SIZE', 0.0018))
        # Ensure amount is a float
        try:
            amount = float(amount)
        except Exception:
            amount = getattr(config, 'POSITION_SIZE', 0.0018)

        # Re-apply amount precision for the symbol
        try:
            amount = float(exchange.amount_to_precision(symbol, amount))
        except Exception:
            pass

        # Check notional minimum if available
        min_notional = None
        try:
            market = exchange.market(symbol) if 'exchange' in globals() and symbol else None
            if market and 'limits' in market and 'cost' in market['limits'] and market['limits']['cost']:
                min_notional = market['limits']['cost'].get('min', None)
        except Exception as e:
            logger.warning(f"[ORDER SPACING] Could not fetch min notional for {symbol}: {e}")

        notional = amount * float(new_price)
        tried_position_size = False
        # If notional is too low, retry with config.POSITION_SIZE
        if min_notional is not None and notional < min_notional:
            logger.warning(f"[ORDER SPACING] Notional ({notional}) is below min_notional ({min_notional}) for {symbol}. Retrying with config.POSITION_SIZE.")
            amount = getattr(config, 'POSITION_SIZE', 0.0018)
            try:
                amount = float(exchange.amount_to_precision(symbol, amount))
            except Exception:
                pass
            notional = amount * float(new_price)
            tried_position_size = True
        # If still too low, log and skip
        if min_notional is not None and notional < min_notional:
            logger.error(f"[ORDER SPACING] Notional ({notional}) is still below min_notional ({min_notional}) for {symbol} even with POSITION_SIZE. Skipping order placement.")
            return

        order_type = order.get('type', 'limit')
        params = order.get('params', {})
        new_order = None
        if 'exchange' in globals() and symbol:
            try:
                if side == 'buy_orders':
                    new_order = exchange.create_limit_buy_order(symbol, amount, new_price, params)
                else:
                    new_order = exchange.create_limit_sell_order(symbol, amount, new_price, params)
                logger.info(f"[ORDER SPACING] Placed new {side[:-7]} order at ${new_price:.2f}, amount={amount}")
                # Add to bot_state, preserving feature and always storing a valid float price
                if new_order and isinstance(new_order, dict):
                    new_order['feature'] = order.get('feature', 'base')
                    order_price = float(new_order.get('price')) if new_order.get('price') is not None else float(new_price)
                    new_order['price'] = order_price
                    bot_state[side].append(new_order)
            except Exception as e:
                if tried_position_size:
                    logger.error(f"[ORDER SPACING] Failed to place new order at ${new_price:.2f} with POSITION_SIZE: {e}")
                else:
                    logger.warning(f"[ORDER SPACING] Failed to place new order at ${new_price:.2f} with original size, retrying with POSITION_SIZE.")
                    # Retry once with POSITION_SIZE if not already tried
                    amount = getattr(config, 'POSITION_SIZE', 0.0018)
                    try:
                        amount = float(exchange.amount_to_precision(symbol, amount))
                    except Exception:
                        pass
                    try:
                        if side == 'buy_orders':
                            new_order = exchange.create_limit_buy_order(symbol, amount, new_price, params)
                        else:
                            new_order = exchange.create_limit_sell_order(symbol, amount, new_price, params)
                        logger.info(f"[ORDER SPACING] Placed new {side[:-7]} order at ${new_price:.2f}, amount={amount} (retry)")
                        if new_order and isinstance(new_order, dict):
                            new_order['feature'] = order.get('feature', 'base')
                            order_price = float(new_order.get('price')) if new_order.get('price') is not None else float(new_price)
                            new_order['price'] = order_price
                            bot_state[side].append(new_order)
                    except Exception as e2:
                        logger.error(f"[ORDER SPACING] Failed to place new order at ${new_price:.2f} with POSITION_SIZE (retry): {e2}")
        else:
            logger.warning(f"[ORDER SPACING] Could not place new order: exchange or symbol not available.")
    except Exception as e:
        logger.error(f"[ORDER SPACING] Error in cancel_and_replace_order: {e}")

def run_bot():

    # Initialize the ML log file for LLM agent monitoring
    initialize_ml_log()

    # Declare global variables
    global exchange
    global bot_state
    global buy_prices
    global ohlcv_df
    global raw_trades_for_ohlcv_df
    # Ensure last_feature_order_price is initialized
    if 'last_feature_order_price' not in bot_state:
        bot_state['last_feature_order_price'] = None

    # Verify global buy_prices is initialized
    if not isinstance(buy_prices, dict):
        logger.error("Global buy_prices is not initialized as a dictionary, initializing now")
        globals()["buy_prices"] = {}

    # Initialize WebSocket manager
    websocket_manager = WebSocketManager(config.API_KEY, config.SECRET_KEY)
    coinbase_thread = websocket_manager.start()

    # Configure Coinbase exchange
    exchange = ccxt.coinbase(
        {
            "apiKey": config.API_KEY,
            "secret": config.SECRET_KEY,
            "enableRateLimit": True,
            "rateLimit": 200,
            "options": {"defaultType": "spot"},
        }
    )
    logger.debug(f"Exchange rate limit configured to {exchange.rateLimit} milliseconds")

    # Start CLI thread
    cli_thread = threading.Thread(target=start_cli, daemon=True)
    cli_thread.start()

    # Fetch initial market data
    current_price = 0.0
    try:
        ticker = exchange.fetch_ticker(config.SYMBOL)
        current_price = float(ticker["last"])
        logger.info(f"Initial market data fetched: Price={current_price:.2f}, GRID_SIZE={config.GRID_SIZE}")
        
        # Log startup activity to ML file for LLM agent monitoring
        try:
            timestamp = datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S")
            log_trade_to_ml_file(timestamp, "STARTUP", current_price, 0.0, 0.0)
            logger.info("Startup activity logged to ML file")
        except Exception as ml_log_error:
            logger.error(f"Failed to log startup to ML file: {ml_log_error}")
            
    except Exception as ticker_error:
        logger.error(f"Failed to fetch initial ticker data: {ticker_error}")
        shutdown_event.set()
        raise

    # Initialize data caches
    historical_data = None
    trade_data = None
    try:
        historical_data = fetch_historical_data(exchange, config.SYMBOL, limit=config.LOOKBACK)
        if historical_data is None or historical_data.empty:
            logger.error("Failed to fetch initial historical data, exiting...")
            shutdown_event.set()
            raise RuntimeError("No historical data available")

        # Compute features for historical_data (restored block)
        historical_data = historical_data.copy()
        historical_data["trades"] = 0
        historical_data["rsi"] = compute_rsi(historical_data["close"])
        historical_data["ema"] = compute_ema(historical_data["close"])
        historical_data["volatility"] = compute_volatility(historical_data, periods=config.VOLATILITY_WINDOW)
        historical_data["macd"], historical_data["macd_signal"] = compute_macd(historical_data["close"])
        historical_data["bollinger_upper"], historical_data["bollinger_lower"] = compute_bollinger(historical_data["close"])
        historical_data["momentum"] = compute_momentum(historical_data["close"])
        historical_data["volume_trend"] = compute_volume_trend(historical_data["volume"])
        historical_data["atr"] = compute_atr(
            historical_data["high"],
            historical_data["low"],
            historical_data["close"],
            periods=config.ATR_PERIOD,
        )
        historical_data["vwap"] = compute_vwap(historical_data, period=config.VWAP_PERIOD)
        historical_data["predicted_price"] = historical_data["close"].shift(-1).fillna(historical_data["close"].mean())
        historical_data["grid_level"] = 0
        historical_data = (
            historical_data.bfill()
            .ffill()
            .fillna(
                {
                    "rsi": 50.0,
                    "ema": historical_data["close"].mean(),
                    "volatility": 0.1,
                    "macd": 0.0,
                    "macd_signal": 0.0,
                    "bollinger_upper": historical_data["close"].mean(),
                    "bollinger_lower": historical_data["close"].mean(),
                    "momentum": 0.0,
                    "volume_trend": 0.0,
                    "atr": historical_data["close"].mean() * 0.01,
                    "vwap": historical_data["close"].mean(),
                    "predicted_price": historical_data["close"].mean(),
                    "grid_level": 0,
                    "trades": 0,
                }
            )
        )

        # Ensure historical_data timestamps are datetime
        if (
            "timestamp" not in historical_data
            or historical_data["timestamp"].isna().any()
            or not pd.api.types.is_datetime64_any_dtype(historical_data["timestamp"])
        ):
            logger.warning("Invalid or missing timestamps in historical_data, converting to datetime")
            historical_data["timestamp"] = pd.to_datetime(historical_data["timestamp"], errors="coerce", utc=True).fillna(
                pd.Timestamp.now(tz="UTC")
            )

        trade_data = fetch_trade_counts(
            exchange,
            config.SYMBOL,
            historical_data=historical_data,
            lookback_minutes=config.MIN_TRADE_MINUTES,
        )
        if trade_data is None or trade_data.empty:
            logger.warning("Failed to fetch initial trade data, using historical data only")
            trade_data = historical_data[["timestamp", "close", "volume", "high", "low"]].assign(trades=0)
    except Exception as data_error:
        logger.error(f"Failed to initialize data caches: {data_error}")
        shutdown_event.set()
        raise

    # Verify trade_data features
    if trade_data is not None and not trade_data.empty:
        trade_data = trade_data.copy()
        required_features = [
            "timestamp",
            "close",
            "volume",
            "trades",
            "rsi",
            "ema",
            "volatility",
            "macd",
            "macd_signal",
            "bollinger_upper",
            "bollinger_lower",
            "momentum",
            "volume_trend",
            "atr",
            "vwap",
            "predicted_price",
            "grid_level",
        ]
        missing_features = [f for f in required_features if f not in trade_data.columns]
        if missing_features:
            logger.warning(f"Missing features in trade_data: {missing_features}, computing them")
            trade_data["rsi"] = compute_rsi(trade_data["close"])
            trade_data["ema"] = compute_ema(trade_data["close"])
            trade_data["volatility"] = compute_volatility(trade_data, periods=config.VOLATILITY_WINDOW)
            trade_data["macd"], trade_data["macd_signal"] = compute_macd(trade_data["close"])
            trade_data["bollinger_upper"], trade_data["bollinger_lower"] = compute_bollinger(trade_data["close"])
            trade_data["momentum"] = compute_momentum(trade_data["close"])
            trade_data["volume_trend"] = compute_volume_trend(trade_data["volume"])
            trade_data["atr"] = compute_atr(
                trade_data.get("high", trade_data["close"]),
                trade_data.get("low", trade_data["close"]),
                trade_data["close"],
                periods=config.ATR_PERIOD,
            )
            trade_data["vwap"] = compute_vwap(trade_data, period=config.VWAP_PERIOD)
            trade_data["predicted_price"] = trade_data["close"].shift(-1).fillna(trade_data["close"].mean())
            trade_data["grid_level"] = 0
            trade_data = (
                trade_data.bfill()
                .ffill()
                .fillna(
                    {
                        "rsi": 50.0,
                        "ema": trade_data["close"].mean(),
                        "volatility": 0.1,
                        "macd": 0.0,
                        "macd_signal": 0.0,
                        "bollinger_upper": trade_data["close"].mean(),
                        "bollinger_lower": trade_data["close"].mean(),
                        "momentum": 0.0,
                        "volume_trend": 0.0,
                        "atr": trade_data["close"].mean() * 0.01,
                        "vwap": trade_data["close"].mean(),
                        "predicted_price": trade_data["close"].mean(),
                        "grid_level": 0,
                        "trades": 0,
                    }
                )
            )

    # Ensure trade_data timestamps are datetime
    if (
        "timestamp" not in trade_data
        or trade_data["timestamp"].isna().any()
        or not pd.api.types.is_datetime64_any_dtype(trade_data["timestamp"])
    ):
        logger.warning("Invalid or missing timestamps in trade_data, converting to datetime")
        trade_data["timestamp"] = pd.to_datetime(trade_data["timestamp"], errors="coerce", utc=True).fillna(
            pd.Timestamp.now(tz="UTC")
        )

    # Train machine learning models
    sklearn_rf_model = None
    sklearn_rf_scaler = None
    sklearn_sgd_model = None
    sklearn_sgd_scaler = None
    sklearn_lookback = config.SKLEARN_LOOKBACK  # e.g., 80
    pytorch_model = None
    pytorch_feature_scaler = None
    pytorch_target_scaler = None
    pytorch_lookback = config.PYTORCH_LOOKBACK  # e.g., 60
    xgb_model = None
    xgb_scaler = None
    xgb_lookback = config.XGB_LOOKBACK  # e.g., 60
    meta_model = None
    meta_scaler = None
    # Initialize prediction variables to avoid NameError
    last_sklearn_prediction = current_price
    last_pytorch_prediction = current_price
    last_xgb_prediction = current_price
    logger.debug(
        f"Initialized prediction variables: last_sklearn_prediction={last_sklearn_prediction:.2f}, "
        f"last_pytorch_prediction={last_pytorch_prediction:.2f}, last_xgb_prediction={last_xgb_prediction:.2f}"
    )

    # Initialize prediction_history
    prediction_history = pd.DataFrame(
        columns=[
            "sklearn_pred", "pytorch_pred", "xgb_pred",
            "current_price", "volatility", "actual_price"
        ]
    )
    logger.debug("Initialized empty prediction_history DataFrame")

    try:
        rf_model, rf_scaler, sgd_model, sgd_scaler, lookback = train_sklearn_predictor(historical_data, trade_data)
        if rf_model is not None and sgd_model is not None:
            sklearn_rf_model = rf_model
            sklearn_rf_scaler = rf_scaler
            sklearn_sgd_model = sgd_model
            sklearn_sgd_scaler = sgd_scaler
            sklearn_lookback = lookback
            bot_state["sklearn_rf_model"] = sklearn_rf_model
            bot_state["scaler_sklearn_rf"] = sklearn_rf_scaler
            bot_state["sklearn_sgd_model"] = sklearn_sgd_model
            bot_state["scaler_sklearn_sgd"] = sklearn_sgd_scaler
            bot_state["sklearn_lookback"] = sklearn_lookback
            logger.info("Sklearn RF and SGD models trained successfully")
            logger.debug(f"bot_state keys after sklearn training: {list(bot_state.keys())}")
            # --- SGD MSE Check ---
            if hasattr(sgd_model, 'mse_'):
                sgd_mse = sgd_model.mse_
            elif hasattr(sgd_model, 'loss_'):
                sgd_mse = sgd_model.loss_
            else:
                sgd_mse = None
            check_sgd_mse(sgd_mse)
        else:
            logger.error("Sklearn model training failed: RF or SGD model is None")
            logger.debug("Debug prompt: Check data quality - verify trade_data and historical_data are not empty")
            logger.debug("Debug prompt: Verify feature engineering - ensure sufficient features for model training")
            logger.debug("Optimization prompt: Review sklearn training parameters in train_sklearn_predictor()")
    except Exception as sklearn_error:
        logger.error(f"Failed to train Sklearn model: {sklearn_error}")
        logger.debug(f"Debug prompt: Sklearn exception details - {type(sklearn_error).__name__}: {str(sklearn_error)}")
        logger.debug("Debug prompt: Check data preprocessing pipeline and feature scaling")
        logger.debug("Optimization prompt: Verify sklearn version compatibility and parameter settings")

    try:
        pytorch_model, pytorch_feature_scaler, pytorch_target_scaler = train_pytorch_predictor(trade_data, historical_data)
        if pytorch_model is not None:
            bot_state["pytorch_model"] = pytorch_model
            bot_state["pytorch_scaler"] = pytorch_feature_scaler
            bot_state["pytorch_target_scaler"] = pytorch_target_scaler
            bot_state["pytorch_lookback"] = pytorch_lookback
            logger.info("PyTorch model trained successfully")
        else:
            logger.error("PyTorch model training failed")
            logger.debug("Debug prompt: Check PyTorch environment - verify CUDA availability if using GPU")
            logger.debug("Debug prompt: Validate data shapes and tensor conversions in train_pytorch_predictor()")
            logger.debug("Optimization prompt: Review learning rate, batch size, and network architecture")
    except Exception as pytorch_error:
        logger.error(f"Failed to train PyTorch model: {pytorch_error}")
        logger.debug(f"Debug prompt: PyTorch exception details - {type(pytorch_error).__name__}: {str(pytorch_error)}")
        logger.debug("Debug prompt: Check tensor shapes, device placement, and memory usage")
        logger.debug("Optimization prompt: Consider gradient clipping, learning rate scheduling, or model architecture changes")

    try:
        xgb_model, xgb_scaler, xgb_lookback = train_xgboost_predictor(historical_data, trade_data)
        if xgb_model is not None:
            bot_state["xgb_model"] = xgb_model
            bot_state["xgb_scaler"] = xgb_scaler
            bot_state["xgb_lookback"] = xgb_lookback
            logger.info("XGBoost model trained successfully")
        else:
            logger.error("XGBoost model training failed")
    except Exception as xgb_error:
        logger.error(f"Failed to train XGBoost model: {xgb_error}")

    # Populate prediction_history
    if trade_data is not None and not trade_data.empty:
        close_value = trade_data["close"].iloc[-1]
        volatility_value = (
            trade_data["volatility"].iloc[-1]
            if "volatility" in trade_data and not pd.isna(trade_data["volatility"].iloc[-1])
            else config.LOG_DEFAULT_VOLATILITY
        )
        actual_price = (
            trade_data["close"].iloc[-1]
            if "close" in trade_data and not pd.isna(trade_data["close"].iloc[-1])
            else close_value
        )
        prediction_history = pd.DataFrame(
            {
                "sklearn_pred": [last_sklearn_prediction],
                "pytorch_pred": [last_pytorch_prediction],
                "xgb_pred": [last_xgb_prediction],
                "current_price": [close_value],
                "volatility": [volatility_value],
                "actual_price": [actual_price],
            }
        )
        logger.debug(f"Initialized prediction_history with trade_data: {prediction_history.iloc[-1].to_dict()}")
    else:
        logger.warning("trade_data empty, initializing prediction_history with current_price")
        prediction_history = pd.DataFrame(
            {
                "sklearn_pred": [current_price],
                "pytorch_pred": [current_price],
                "xgb_pred": [current_price],
                "current_price": [current_price],
                "volatility": [config.LOG_DEFAULT_VOLATILITY],
                "actual_price": [current_price],
            }
        )
        logger.debug(f"Initialized prediction_history with fallback: {prediction_history.iloc[-1].to_dict()}")

    # Train meta-model
    try:
        meta_model, meta_scaler = train_meta_model(prediction_history, min_samples=1)
        if meta_model is not None:
            bot_state["meta_model"] = meta_model
            bot_state["meta_scaler"] = meta_scaler
            logger.info("Meta-model trained successfully")
            joblib.dump(meta_model, "client_meta_model.pkl")
            logger.info("Saved initial meta-model to client_meta_model.pkl")
            joblib.dump(meta_scaler, "client_meta_scaler.pkl")
            logger.info("Saved initial meta-model scaler to client_meta_scaler.pkl")
            # --- Meta-model Overfit Check ---
            train_mse = getattr(meta_model, 'mse_', None)
            test_mse = getattr(meta_model, 'test_mse_', None)
            check_meta_model_overfit(train_mse, test_mse)
        else:
            logger.error("Meta-model training failed")
    except Exception as meta_error:
        logger.error(f"Failed to train meta-model: {meta_error}")
        meta_model = None
        meta_scaler = None

    # Store prediction_history in bot_state
    bot_state["prediction_history"] = prediction_history
    logger.debug("Stored prediction_history in bot_state")

    # Require at least two models to proceed (including meta-model)
    valid_models = sum(1 for model in [sklearn_rf_model, sklearn_sgd_model, pytorch_model, xgb_model, meta_model] if model is not None)
    if valid_models < 2:
        logger.error(f"Only {valid_models} model(s) trained successfully, at least two required, exiting...")
        shutdown_event.set()
        raise RuntimeError("Insufficient valid ML models")

    logger.info(f"Trained {valid_models} ML models successfully")

    # Log training data
    if not historical_data.empty:
        last_timestamp = (
            historical_data["timestamp"].iloc[-1]
            if "timestamp" in historical_data and pd.notna(historical_data["timestamp"].iloc[-1])
            else pd.Timestamp.now(tz="UTC")
        )
        last_close = historical_data["close"].iloc[-1]
        last_volume = historical_data["volume"].iloc[-1]
        last_volatility = historical_data["volatility"].iloc[-1]
        last_rsi = historical_data["rsi"].iloc[-1]
        last_ema = historical_data["ema"].iloc[-1]
        last_atr = historical_data["atr"].iloc[-1]
        last_vwap = historical_data["vwap"].iloc[-1]
        logger.info(
            f"Initial Sklearn Training: timestamp={last_timestamp}, "
            f"close={last_close:.2f}, volume={last_volume:.2f}, "
            f"volatility={last_volatility:.2f}, rsi={last_rsi:.2f}, "
            f"ema={last_ema:.2f}, atr={last_atr:.2f}, vwap={last_vwap:.2f}"
        )
    if trade_data is not None and not trade_data.empty:
        last_timestamp = (
            trade_data["timestamp"].iloc[-1]
            if "timestamp" in trade_data and pd.notna(trade_data["timestamp"].iloc[-1])
            else pd.Timestamp.now(tz="UTC")
        )
        last_close = trade_data["close"].iloc[-1]
        last_trades = trade_data["trades"].iloc[-1]
        logger.info(
            f"Initial PyTorch Training: timestamp={last_timestamp}, "
            f"close={last_close:.2f}, trades={last_trades:.0f}"
        )

    # Set lookback period
    lookback = max(sklearn_lookback, pytorch_lookback, xgb_lookback)
    logger.debug(f"Lookback values: sklearn={sklearn_lookback}, pytorch={pytorch_lookback}, xgb={xgb_lookback}, max={lookback}")
    sklearn_features = [
        "timestamp",
        "close",
        "volume",
        "trades",
        "rsi",
        "ema",
        "volatility",
        "macd",
        "macd_signal",
        "bollinger_upper",
        "bollinger_lower",
        "momentum",
        "volume_trend",
        "high",
        "low",
        "atr",
        "vwap",
        "predicted_price",
        "grid_level",
    ]
    other_features = [
        "timestamp",
        "close",
        "volume",
        "trades",
        "rsi",
        "ema",
        "volatility",
        "macd",
        "macd_signal",
        "bollinger_upper",
        "bollinger_lower",
        "momentum",
        "volume_trend",
        "high",
        "low",
        "atr",
        "vwap",
        "predicted_price",
        "grid_level",
    ]

    # Initialize recent_data and perform initial WebSocket update
    try:
        ohlcv_df, raw_trades_for_ohlcv_df, updated = _update_live_data_from_websocket(
            websocket_manager, ohlcv_df, raw_trades_for_ohlcv_df
        )
        if updated and not ohlcv_df.empty:
            logger.info(f"Initial WebSocket update successful, using ohlcv_df for recent_data, rows={len(ohlcv_df)}")
            if "timestamp" in ohlcv_df:
                logger.debug(f"Raw ohlcv_df timestamps before conversion: {ohlcv_df['timestamp'].head().to_list()}")
                ohlcv_df["timestamp"] = pd.to_datetime(ohlcv_df["timestamp"], errors="coerce", utc=True)
                if ohlcv_df["timestamp"].isna().any():
                    logger.warning("Some timestamps in ohlcv_df are invalid, filling with current time")
                    ohlcv_df["timestamp"] = ohlcv_df["timestamp"].fillna(pd.Timestamp.now(tz="UTC"))
            else:
                logger.warning("No timestamp column in ohlcv_df, creating with current time")
                ohlcv_df["timestamp"] = pd.Timestamp.now(tz="UTC")
            logger.debug(
                f"ohlcv_df after timestamp fix: first_timestamp={ohlcv_df['timestamp'].iloc[0] if not ohlcv_df.empty else 'N/A'}, "
                f"last_timestamp={ohlcv_df['timestamp'].iloc[-1] if not ohlcv_df.empty else 'N/A'}, "
                f"last_close={ohlcv_df['close'].iloc[-1] if not ohlcv_df.empty else 'N/A'}"
            )
            if len(ohlcv_df) < lookback:
                logger.debug(f"Padding recent_data: ohlcv_df has {len(ohlcv_df)} rows, need {lookback}")
                historical_subset = historical_data[sklearn_features].tail(lookback - len(ohlcv_df)).copy()
                historical_subset["timestamp"] = pd.to_datetime(
                    historical_subset["timestamp"], errors="coerce", utc=True
                ).fillna(pd.Timestamp.now(tz="UTC"))
                recent_data = pd.concat([historical_subset, ohlcv_df[sklearn_features]], ignore_index=True).tail(
                    lookback
                )
                recent_trade_data = pd.concat(
                    [
                        historical_data[other_features].tail(lookback - len(ohlcv_df)),
                        ohlcv_df[other_features],
                    ],
                    ignore_index=True,
                ).tail(lookback)
            else:
                recent_data = ohlcv_df[sklearn_features].tail(lookback).copy()
                recent_trade_data = ohlcv_df[other_features].tail(lookback).copy()
            if "timestamp" in recent_data:
                recent_data["timestamp"] = pd.to_datetime(recent_data["timestamp"], errors="coerce", utc=True)
                if recent_data["timestamp"].isna().any():
                    logger.warning("Invalid timestamps in recent_data after construction, filling with current time")
                    recent_data["timestamp"] = recent_data["timestamp"].fillna(pd.Timestamp.now(tz="UTC"))
            else:
                logger.warning("No timestamp column in recent_data after construction, setting to current time")
                recent_data["timestamp"] = pd.Timestamp.now(tz="UTC")
            if "timestamp" in recent_trade_data:
                recent_trade_data["timestamp"] = pd.to_datetime(
                    recent_trade_data["timestamp"], errors="coerce", utc=True
                )
                if recent_trade_data["timestamp"].isna().any():
                    logger.warning(
                        "Invalid timestamps in recent_trade_data after construction, filling with current time"
                    )
                    recent_trade_data["timestamp"] = recent_trade_data["timestamp"].fillna(pd.Timestamp.now(tz="UTC"))
            else:
                logger.warning("No timestamp column in recent_trade_data after construction, setting to current time")
                recent_trade_data["timestamp"] = pd.Timestamp.now(tz="UTC")
        else:
            logger.warning("Initial WebSocket update failed or no data, using historical_data")
            recent_data = historical_data[sklearn_features].tail(lookback).copy()
            recent_trade_data = (
                trade_data[other_features].tail(lookback).copy()
                if trade_data is not None
                else recent_data[other_features].copy()
            )
            recent_data["timestamp"] = pd.to_datetime(recent_data["timestamp"], errors="coerce", utc=True).fillna(
                pd.Timestamp.now(tz="UTC")
            )
            recent_trade_data["timestamp"] = pd.to_datetime(
                recent_trade_data["timestamp"], errors="coerce", utc=True
            ).fillna(pd.Timestamp.now(tz="UTC"))
    except Exception as ws_error:
        logger.error(f"Error in initial WebSocket update: {ws_error}, using historical_data")
        recent_data = historical_data[sklearn_features].tail(lookback).copy()
        recent_trade_data = (
            trade_data[other_features].tail(lookback).copy()
            if trade_data is not None
            else recent_data[other_features].copy()
        )
        recent_data["timestamp"] = pd.to_datetime(recent_data["timestamp"], errors="coerce", utc=True).fillna(
            pd.Timestamp.now(tz="UTC")
        )
        recent_trade_data["timestamp"] = pd.to_datetime(
            recent_trade_data["timestamp"], errors="coerce", utc=True
        ).fillna(pd.Timestamp.now(tz="UTC"))

    # Initialize bot state
    bot_state["pytorch_model"] = LSTMPricePredictor(
        input_size=18,
        hidden_size=config.PYTORCH_HIDDEN_SIZE,
        num_layers=config.PYTORCH_NUM_LAYERS,
        dropout=config.PYTORCH_DROPOUT
    ).to(config.DEVICE)
    bot_state["buy_orders"] = []
    bot_state["sell_orders"] = []
    closed_orders = []
    last_trade_time = time.time()
    last_reset_time = time.time()
    last_retrain_time = time.time()
    last_retrain_check = time.time()
    last_sklearn_prediction = current_price
    last_pytorch_prediction = current_price
    last_xgb_prediction = current_price
    trade_counts = {}
    grid_base_price = current_price
    initial_grid_size = config.GRID_SIZE
    initial_position_size = config.POSITION_SIZE
    trade_data_lookback_minutes = config.MIN_TRADE_MINUTES
    eth_balance = 0.0
    usd_balance = 0.0
    eth_balance, usd_balance = sync_balances(exchange)
    bot_state["initial_eth"] = eth_balance
    bot_state["initial_usd"] = usd_balance
    bot_state["initial_buy_price"] = current_price
    prediction_accuracy = {"sklearn": [], "pytorch": [], "xgboost": []}

    # Store initial grid parameters for rebalancing
    initial_num_buy_grid_lines = config.NUM_BUY_GRID_LINES  # Save initial buy grid lines (20) for rebalancing reference
    initial_num_sell_grid_lines = config.NUM_SELL_GRID_LINES  # Save initial sell grid lines (20) for rebalancing reference

    # Perform initial ETH purchase
    target_eth = config.POSITION_SIZE * config.NUM_SELL_GRID_LINES * config.TARGET_ETH_BUFFER  # Calculate target ETH needed
    eth_balance = exchange.fetch_balance().get('ETH', {}).get('free', 0.0)  # Fetch available ETH balance
    logger.info(f"Checking ETH balance: {eth_balance:.6f} ETH, target: {target_eth:.6f} ETH")  # Log balance check
    initial_buy_order = None  # Initialize to avoid undefined variable error

    if eth_balance >= target_eth:
        logger.info(f"Sufficient ETH balance ({eth_balance:.6f} >= {target_eth:.6f}), skipping initial buy")  # Log sufficient balance
        grid_base_price = current_price  # Set grid base price to current market price
        bot_state["initial_buy_price"] = grid_base_price  # Store initial buy price for reference
        logger.info(f"Grid base price set to {grid_base_price:.2f}")  # Log grid base price
    else:
        total_usd_needed = usd_balance * config.INITIAL_ETH_PERCENTAGE  # Calculate USD needed for ETH purchase
        total_cost_usd = min(total_usd_needed, target_eth * current_price)  # Limit cost to available USD or target ETH
        logger.info(
            f"Submitting initial market buy for {total_cost_usd:.2f} USD (~{total_cost_usd / current_price:.6f} ETH)"
        )  # Log buy order submission
        try:
            if total_cost_usd <= 0:
                logger.error(f"Invalid total_cost_usd: {total_cost_usd}, skipping initial buy order")  # Log invalid cost error
                grid_base_price = current_price  # Fallback to current price
            else:
                initial_buy_order = exchange.create_market_buy_order(
                    config.SYMBOL,
                    total_cost_usd,
                    params={"createMarketBuyOrderRequiresPrice": False},
                )  # Place market buy order
                initial_buy_order = exchange.fetch_order(initial_buy_order["id"])  # Fetch order details
                filled_amount = float(initial_buy_order["filled"]) if initial_buy_order["filled"] else 0.0  # Get filled amount
                logger.info(f"Initial buy filled: {filled_amount:.6f} ETH, ID={initial_buy_order['id']}")  # Log buy order success
                grid_base_price = float(initial_buy_order["average"]) if initial_buy_order["average"] else current_price  # Use average price
                bot_state["initial_buy_price"] = grid_base_price  # Store initial buy price
                logger.info(f"Grid base price set to {grid_base_price:.2f}")  # Log grid base price
        except Exception as buy_error:
            logger.error(f"Initial buy order failed: {buy_error}, using current price as grid base")  # Log buy order failure
            grid_base_price = current_price  # Fallback to current price

    # Initialize orders to send
    initial_orders_to_send = [
        {
            "type": "order",
            "id": initial_buy_order["id"] if initial_buy_order else "initial_buy",
            "status": initial_buy_order["status"] if initial_buy_order else "failed",
            "side": "buy",
            "price": float(grid_base_price),
            "timestamp": int(time.time() * 1000),
        }
    ]  # Initialize WebSocket message list with initial buy order (if any)
    current_max_order_range = config.MAX_ORDER_RANGE * (config.GRID_SIZE / config.MIN_GRID_SIZE)  # Adjust range based on grid size
    current_stagnation_timeout = config.STAGNATION_TIMEOUT * (config.MIN_GRID_SIZE / config.GRID_SIZE)  # Adjust timeout based on grid size
    logger.info(
        f"Initial setup: MAX_ORDER_RANGE={current_max_order_range:.2f}, STAGNATION_TIMEOUT={current_stagnation_timeout:.0f}s, "
        f"NUM_BUY_GRID_LINES={initial_num_buy_grid_lines}, NUM_SELL_GRID_LINES={initial_num_sell_grid_lines}"
    )  # Enhanced: Log setup parameters for troubleshooting
    logger.info("Placing initial base grid orders and scanning for feature-based orders (RSI, Bollinger Bands, MACD, EMA, ATR, Volatility, VWAP) during initial order placement. LIVE ORDER PLACEMENT ENABLED.")

    eth_balance, usd_balance = sync_balances(exchange)  # Sync balances after potential initial buy
    logger.info(f"Initial balances for grid setup: ETH={eth_balance:.6f}, USD={usd_balance:.2f}")  # Log updated balances

    # Initialize feature_trade_state if not present
    if 'feature_trade_state' not in bot_state:
        bot_state['feature_trade_state'] = {
            'rsi': {'last_side': None, 'last_price': None},
            'bollinger': {'last_side': None, 'last_price': None},
            'macd': {'last_side': None, 'last_price': None},
        }
        logger.info("Initialized feature_trade_state for DCA tracking")

    # DCA logic to enforce buy/sell sequence
    def can_place_feature_order(feature, side, current_price):
        state = bot_state['feature_trade_state'][feature]
        if state['last_side'] is None:
            if side == 'buy':
                logger.info(f"[DCA {feature.upper()}] No previous trade, allowing buy order")
                return True
            else:
                logger.info(f"[DCA {feature.upper()}] No previous trade, sell order blocked (requires buy first)")
                return False
        if side == 'sell' and state['last_side'] == 'buy' and state['last_price']:
            can_sell = current_price >= state['last_price'] * 1.015  # 1.5% profit threshold
            logger.info(f"[DCA {feature.upper()}] Sell check: current_price={current_price:.2f}, "
                        f"last_buy_price={state['last_price']:.2f}, threshold={state['last_price'] * 1.015:.2f}, "
                        f"can_sell={can_sell}")
            return can_sell
        if side == 'buy' and state['last_side'] == 'sell' and state['last_price']:
            can_buy = current_price <= state['last_price'] * 0.98  # 2% lower threshold
            logger.info(f"[DCA {feature.upper()}] Buy check: current_price={current_price:.2f}, "
                        f"last_sell_price={state['last_price']:.2f}, threshold={state['last_price'] * 0.98:.2f}, "
                        f"can_buy={can_buy}")
            return can_buy
        logger.info(f"[DCA {feature.upper()}] Cannot place {side} order: last_side={state['last_side']}, "
                    f"last_price={state['last_price']}")
        return False

    # --- Place initial base grid orders (buy/sell) ---
    initial_orders_to_send = []
    for grid_index in range(config.NUM_BUY_GRID_LINES):
        if count_total_open_base_orders(bot_state) >= (config.NUM_BUY_GRID_LINES + config.NUM_SELL_GRID_LINES):
            logger.info(f"[BASE GRID] Skipping buy order placement: global base order cap reached ({config.NUM_BUY_GRID_LINES + config.NUM_SELL_GRID_LINES})")
            break
        price = grid_base_price - (config.GRID_SIZE * (grid_index + 1))
        price = float(exchange.price_to_precision(config.SYMBOL, price))
        if (grid_base_price - price) <= current_max_order_range:
            amount = exchange.amount_to_precision(config.SYMBOL, config.POSITION_SIZE)
            cost = float(amount) * price
            if usd_balance >= cost:
                try:
                    order = exchange.create_limit_buy_order(config.SYMBOL, amount, price, params={"post_only": True})
                    order = exchange.fetch_order(order["id"])
                    order_price = float(order["price"]) if order["price"] else price
                    if order["status"] == "open":
                        bot_state["buy_orders"].append({**order, "feature": "base"})
                        initial_orders_to_send.append({
                            "type": "order",
                            "id": order["id"],
                            "status": order["status"],
                            "side": order["side"],
                            "price": float(order_price),
                            "timestamp": int(time.time() * 1000),
                            "feature": "base"
                        })
                    logger.info(f"[BASE] Buy order {order['id']} placed at {price:.2f}, size={amount}, feature=base [LIVE]")
                except Exception as buy_order_error:
                    logger.error(f"[BASE] Failed to place buy order at {price:.2f}: {buy_order_error}")
            else:
                logger.warning(f"[BASE] Insufficient USD {usd_balance:.2f} for buy order at {price:.2f} (cost {cost:.2f})")
        time.sleep(exchange.rateLimit / 1000)
    logger.info(f"[BASE] Placed {len([o for o in bot_state['buy_orders'] if o['feature'] == 'base'])}/{config.NUM_BUY_GRID_LINES} base buy orders [LIVE]")

    eth_balance, _ = sync_balances(exchange)
    logger.info(f"[BASE] ETH balance before sell orders: {eth_balance:.6f}")
    for grid_index in range(config.NUM_SELL_GRID_LINES):
        if count_total_open_base_orders(bot_state) >= (config.NUM_BUY_GRID_LINES + config.NUM_SELL_GRID_LINES):
            logger.info(f"[BASE GRID] Skipping sell order placement: global base order cap reached ({config.NUM_BUY_GRID_LINES + config.NUM_SELL_GRID_LINES})")
            break
        price = grid_base_price + (config.GRID_SIZE * (grid_index + 1))
        price = float(exchange.price_to_precision(config.SYMBOL, price))
        if (price - grid_base_price) <= current_max_order_range:
            amount = min(
                eth_balance / max(1, config.NUM_SELL_GRID_LINES - grid_index),
                config.POSITION_SIZE,
         
   )
            amount = float(exchange.amount_to_precision(config.SYMBOL, amount))
            if eth_balance >= amount and amount >= config.MIN_POSITION_SIZE:
                try:
                    order = exchange.create_limit_sell_order(config.SYMBOL, amount, price, params={"post_only": True})
                    order = exchange.fetch_order(order["id"])
                    order_price = float(order["price"]) if order["price"] else price
                    if order["status"] == "open":
                        bot_state["sell_orders"].append({**order, "feature": "base"})
                        initial_orders_to_send.append({
                            "type": "order",
                            "id": order["id"],
                            "status": order["status"],
                            "side": order["side"],
                            "price": float(order_price),
                            "timestamp": int(time.time() * 1000),
                            "feature": "base"
                        })
                    eth_balance -= amount
                    buy_prices[order["id"]] = grid_base_price
                    logger.info(f"[BASE] Sell order {order['id']} placed at {price:.2f}, size={amount}, feature=base")
                except Exception as sell_order_error:
                    logger.error(f"[BASE] Failed to place sell order at {price:.2f}: {sell_order_error}")
            else:
                logger.warning(f"[BASE] Insufficient ETH {eth_balance:.6f} for sell order at {price:.2f} (required {amount:.6f})")
        time.sleep(exchange.rateLimit / 1000)
    logger.info(f"[BASE] Placed {len([o for o in bot_state['sell_orders'] if o['feature'] == 'base'])}/{config.NUM_SELL_GRID_LINES} base sell orders")
    logger.info(f"[BASE] Placed {len([o for o in bot_state['sell_orders'] if o['feature'] == 'base'])}/{config.NUM_SELL_GRID_LINES} base sell orders [LIVE]")

    # --- Place initial feature-based orders (RSI, Bollinger Bands, MACD) ---
    # --- Place initial feature-based orders (RSI, Bollinger Bands, MACD) ---
    # LIVE ORDER PLACEMENT ENABLED
    latest = historical_data.iloc[-1]
    try:
        orderbook = exchange.fetch_order_book(config.SYMBOL)
        best_bid = float(orderbook['bids'][0][0]) if orderbook['bids'] else None
        best_ask = float(orderbook['asks'][0][0]) if orderbook['asks'] else None
        min_tick = getattr(exchange, 'markets', {}).get(config.SYMBOL, {}).get('precision', {}).get('price', 0.01)
        min_tick = float(min_tick) if min_tick else 0.01
    except Exception as ob_err:
        logger.warning(f"[FEATURE] Could not fetch order book for post-only compliance: {ob_err}")
        best_bid = best_ask = None
        min_tick = 0.01

    feature_order_cap = getattr(config, "FEATURE_ORDER_CAP", 100)
    soft_feature_order_cap = getattr(config, "SOFT_FEATURE_ORDER_CAP", 50)

    # RSI
    logger.info(f"[RSI] Current RSI: {latest['rsi']:.2f}, Buy if < 30")
    min_feature_spacing = getattr(config, "MIN_FEATURE_ORDER_SPACING", 0.005)
    if (
        latest["rsi"] < 30
        and best_ask
        and count_total_open_feature_orders(bot_state) < feature_order_cap
        and can_place_feature_order("rsi", "buy", latest["close"])
        and can_place_feature_order_with_spacing(bot_state, latest["close"], min_feature_spacing)
    ):
        if soft_feature_order_cap is not None:
            feature_count = sum(1 for o in bot_state["buy_orders"] if o.get("feature") == "rsi" and o.get("status", "open") == "open")
            if feature_count >= soft_feature_order_cap:
                logger.info(f"[INIT] Soft cap reached for feature rsi ({soft_feature_order_cap}), but not strictly enforced.")
        retry_count = 0
        max_retries = 5
        price = float(exchange.price_to_precision(config.SYMBOL, min(latest["close"], best_ask - min_tick)))
        while retry_count < max_retries and price < best_ask:
            try:
                amount = exchange.amount_to_precision(config.SYMBOL, config.POSITION_SIZE)
                cost = float(amount) * price
                if usd_balance >= cost:
                    order = exchange.create_limit_buy_order(config.SYMBOL, amount, price, params={"post_only": True})
                    order = exchange.fetch_order(order["id"])
                    order_price = float(order["price"]) if order.get("price") is not None else price
                    bot_state["buy_orders"].append({**order, "feature": "rsi", "price": order_price})
                    initial_orders_to_send.append({
                        "type": "order",
                        "id": order["id"],
                        "status": order["status"],
                        "side": order["side"],
                        "price": order_price,
                        "timestamp": int(time.time() * 1000),
                        "feature": "rsi"
                    })
                    bot_state['feature_trade_state']['rsi']['last_side'] = 'buy'
                    bot_state['feature_trade_state']['rsi']['last_price'] = order_price
                    logger.info(f"[RSI] Buy order {order['id']} placed at {order_price:.2f}, size={amount}, feature=rsi, DCA updated")
                    break
                else:
                    logger.warning(f"[RSI] Insufficient USD {usd_balance:.2f} for buy order at {price:.2f} (cost {cost:.2f})")
                    break
            except Exception as e:
                if "INVALID_LIMIT_PRICE_POST_ONLY" in str(e):
                    retry_count += 1
                    price = float(exchange.price_to_precision(config.SYMBOL, price - min_tick))
                    logger.warning(f"[RSI] Buy order post-only error, retrying with lower price: {price:.2f} (attempt {retry_count}/{max_retries})")
                else:
                    logger.error(f"[RSI] Failed to place buy order: {e}")
                    break
        else:
            logger.warning(f"[RSI] Skipped buy: could not place post-only order after {max_retries} retries or price {price:.2f} >= best_ask {best_ask}")

    # Bollinger Bands
    logger.info(f"[BOLLINGER] Current Price: {latest['close']:.2f}, Lower Band: {latest['bollinger_lower']:.2f}, Upper Band: {latest['bollinger_upper']:.2f}")
    if (
        latest["close"] <= latest["bollinger_lower"]
        and best_ask
        and count_total_open_feature_orders(bot_state) < feature_order_cap
        and can_place_feature_order("bollinger", "buy", latest["close"])
        and can_place_feature_order_with_spacing(bot_state, latest["close"], min_feature_spacing)
    ):
        if soft_feature_order_cap is not None:
            feature_count = sum(1 for o in bot_state["buy_orders"] if o.get("feature") == "bollinger" and o.get("status", "open") == "open")
            if feature_count >= soft_feature_order_cap:
                logger.info(f"[INIT] Soft cap reached for feature bollinger ({soft_feature_order_cap}), but not strictly enforced.")
        retry_count = 0
        max_retries = 5
        price = float(exchange.price_to_precision(config.SYMBOL, min(latest["close"], best_ask - min_tick)))
        while retry_count < max_retries and price < best_ask:
            try:
                amount = exchange.amount_to_precision(config.SYMBOL, config.POSITION_SIZE)
                cost = float(amount) * price
                if usd_balance >= cost:
                    order = exchange.create_limit_buy_order(config.SYMBOL, amount, price, params={"post_only": True})
                    order = exchange.fetch_order(order["id"])
                    order_price = float(order["price"]) if order.get("price") is not None else price
                    bot_state["buy_orders"].append({**order, "feature": "bollinger", "price": order_price})
                    initial_orders_to_send.append({
                        "type": "order",
                        "id": order["id"],
                        "status": order["status"],
                        "side": order["side"],
                        "price": order_price,
                        "timestamp": int(time.time() * 1000),
                        "feature": "bollinger"
                    })
                    bot_state['feature_trade_state']['bollinger']['last_side'] = 'buy'
                    bot_state['feature_trade_state']['bollinger']['last_price'] = order_price
                    logger.info(f"[BOLLINGER] Buy order {order['id']} placed at {order_price:.2f}, size={amount}, feature=bollinger, DCA updated")
                    break
                else:
                    logger.warning(f"[BOLLINGER] Insufficient USD {usd_balance:.2f} for buy order at {price:.2f} (cost {cost:.2f})")
                    break
            except Exception as e:
                if "INVALID_LIMIT_PRICE_POST_ONLY" in str(e):
                    retry_count += 1
                    price = float(exchange.price_to_precision(config.SYMBOL, price - min_tick))
                    logger.warning(f"[BOLLINGER] Buy order post-only error, retrying with lower price: {price:.2f} (attempt {retry_count}/{max_retries})")
                else:
                    logger.error(f"[BOLLINGER] Failed to place buy order: {e}")
                    break
        else:
            logger.warning(f"[BOLLINGER] Skipped buy: could not place post-only order after {max_retries} retries or price {price:.2f} >= best_ask {best_ask}")

    # MACD
    logger.info(f"[MACD] Current MACD: {latest['macd']:.4f}, Signal: {latest['macd_signal']:.4f}, Buy if MACD > Signal")
    if (
        latest["macd"] > latest["macd_signal"]
        and best_ask
        and count_total_open_feature_orders(bot_state) < feature_order_cap
        and can_place_feature_order("macd", "buy", latest["close"])
        and can_place_feature_order_with_spacing(bot_state, latest["close"], min_feature_spacing)
    ):
        if soft_feature_order_cap is not None:
            feature_count = sum(1 for o in bot_state["buy_orders"] if o.get("feature") == "macd" and o.get("status", "open") == "open")
            if feature_count >= soft_feature_order_cap:
                logger.info(f"[INIT] Soft cap reached for feature macd ({soft_feature_order_cap}), but not strictly enforced.")
        retry_count = 0
        max_retries = 5
        price = float(exchange.price_to_precision(config.SYMBOL, min(latest["close"], best_ask - min_tick)))
        while retry_count < max_retries and price < best_ask:
            try:
                amount = exchange.amount_to_precision(config.SYMBOL, config.POSITION_SIZE)
                cost = float(amount) * price
                if usd_balance >= cost:
                    order = exchange.create_limit_buy_order(config.SYMBOL, amount, price, params={"post_only": True})
                    order = exchange.fetch_order(order["id"])
                    order_price = float(order["price"]) if order.get("price") is not None else price
                    bot_state["buy_orders"].append({**order, "feature": "macd", "price": order_price})
                    initial_orders_to_send.append({
                        "type": "order",
                        "id": order["id"],
                        "status": order["status"],
                        "side": order["side"],
                        "price": order_price,
                        "timestamp": int(time.time() * 1000),
                        "feature": "macd"
                    })
                    bot_state['feature_trade_state']['macd']['last_side'] = 'buy'
                    bot_state['feature_trade_state']['macd']['last_price'] = order_price
                    logger.info(f"[MACD] Buy order {order['id']} placed at {order_price:.2f}, size={amount}, feature=macd, DCA updated")
                    break
                else:
                    logger.warning(f"[MACD] Insufficient USD {usd_balance:.2f} for buy order at {price:.2f} (cost {cost:.2f})")
                    break
            except Exception as e:
                if "INVALID_LIMIT_PRICE_POST_ONLY" in str(e):
                    retry_count += 1
                    price = float(exchange.price_to_precision(config.SYMBOL, price - min_tick))
                    logger.warning(f"[MACD] Buy order post-only error, retrying with lower price: {price:.2f} (attempt {retry_count}/{max_retries})")
                else:
                    logger.error(f"[MACD] Failed to place buy order: {e}")
                    break
        else:
            logger.warning(f"[MACD] Skipped buy: could not place post-only order after {max_retries} retries or price {price:.2f} >= best_ask {best_ask}")

    # Send initial orders via WebSocket
    try:
        # Remove any cancelled/closed orders from initial_orders_to_send before sending
        filtered_orders_to_send = [o for o in initial_orders_to_send if o.get('status', 'open') == 'open']
        websocket_manager.send(json.dumps(filtered_orders_to_send))
        logger.info(f"[WEBSOCKET] Sent {len(filtered_orders_to_send)} initial orders (base and feature-based)")
    except Exception as websocket_error:
        logger.error(f"[WEBSOCKET] Error sending initial WebSocket orders: {websocket_error}")

    # Initialize main loop variables
    iteration_count = 0  # Initialize iteration counter
    previous_price = current_price  # Store current price for tracking
    last_heartbeat = time.time()  # Initialize heartbeat timestamp
    last_data_refresh = time.time()  # Initialize data refresh timestamp

    # Initialize ML scalers
    scaler_sklearn_rf = StandardScaler()  # For RandomForestRegressor
    scaler_pytorch = MinMaxScaler()
    scaler_xgb = StandardScaler()
    scaler_meta = StandardScaler()
    bot_state["sklearn_rf_scaler"] = scaler_sklearn_rf
    bot_state["pytorch_scaler"] = scaler_pytorch
    bot_state["xgb_scaler"] = scaler_xgb
    bot_state["meta_scaler"] = scaler_meta

    # Main loop
    last_trade_time = time.time()
    last_heartbeat = time.time()
    last_reset_time = time.time()
    iteration_count = 0
    previous_price = bot_state["current_price"]
    grid_base_price = 0
    initial_position_size = config.POSITION_SIZE
    initial_grid_size = config.GRID_SIZE
    last_sklearn_prediction = 0
    last_pytorch_prediction = 0
    last_xgb_prediction = 0
    trade_counts = collections.defaultdict(int)
    prediction_accuracy = {"sklearn": [], "pytorch": [], "xgboost": []}

    # Fit scalers and initialize models
    if not bot_state["feature_cache"].empty:
        features = [
            "close", "volume", "trades", "rsi", "ema", "volatility", "macd", "macd_signal",
            "bollinger_upper", "bollinger_lower", "momentum", "volume_trend", "atr", "vwap",
            "price_spread", "returns", "volume_change", "trade_intensity"
        ]
        # Fit scalers with DataFrame to preserve feature names
        X = bot_state["feature_cache"][features]
        scaler_sklearn_rf.fit(X)
        scaler_pytorch.fit(X)
        scaler_xgb.fit(X)
        logger.info("Scalers fitted with initial feature cache data")

        # Force initial retraining
        models = retrain_ml_models(
            bot_state["feature_cache"], scaler_sklearn_rf, scaler_pytorch, scaler_xgb, scaler_meta
        )
        if all(models):
            (bot_state["sklearn_rf_model"], bot_state["sklearn_sgd_model"]), \
            bot_state["pytorch_model"], bot_state["xgb_model"], bot_state["meta_model"] = models
            bot_state["last_retrain_time"] = time.time()
            logger.info("Initial model retraining completed")

    try:
        # Main loop for continuous trading
        while not shutdown_event.is_set():
            try:
                # Check if bot is paused
                if bot_state["paused"]:
                    logger.info("Bot is paused, skipping iteration")
                    time.sleep(config.CHECK_ORDER_FREQUENCY)
                    continue

                # Log periodic heartbeat to ML file for LLM agent monitoring
                current_time = time.time()
                if current_time - last_heartbeat >= 30:  # Every 30 seconds
                    try:
                        timestamp = datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S")
                        current_price = bot_state.get("current_price", 0)
                        log_trade_to_ml_file(timestamp, f"HEARTBEAT_{iteration_count}", current_price, 0.0, 0.0)
                        last_heartbeat = current_time
                    except Exception as heartbeat_error:
                        logger.error(f"Failed to log heartbeat to ML file: {heartbeat_error}")

                """ # --- ADDED: Breakout Logic ---
                # If a breakout position is active, manage it and skip the rest of the grid logic.
                if bot_state.get("breakout_position", {}).get("active"):
                    logger.info("[BREAKOUT] Active breakout position detected. Managing position...")
                    manage_breakout_position(exchange, websocket_manager)
                    time.sleep(config.CHECK_ORDER_FREQUENCY) # Wait before the next check
                    continue # Skip the rest of the main loop to focus on the breakout

                # If not in a breakout, check if conditions are right for a new one.
                check_for_breakout(exchange, websocket_manager)

                # If a breakout was just initiated, loop again to let manage_breakout_position take over.
                if bot_state.get("breakout_position", {}).get("active"):
                    logger.info("[BREAKOUT] New breakout trade initiated. Skipping grid logic for this cycle.")
                    time.sleep(config.CHECK_ORDER_FREQUENCY)
                    continue

                # If grid is paused (due to breakout), skip grid logic
                if bot_state.get("grid_paused"):
                    logger.info("[GRID PAUSED] Grid logic is paused due to an active breakout trade. Waiting for breakout to complete.")
                    time.sleep(config.CHECK_ORDER_FREQUENCY)
                    continue
                
                # --- ADDED: Grid Repositioning Logic ---
                if bot_state.get("needs_grid_reposition"):
                    reposition_grid_after_breakout(exchange, websocket_manager)
                    # The grid is now repositioned, continue to the next loop to start normal checks
                    continue
                # --- END: Grid Repositioning Logic ---
                # --- END: Breakout Logic ---

                # === BEGIN: Active Feature Calls and Logging === """
                try:
                    logger.info("[PROTECTION] Running uptrend protection and feature order spacing.")
                    if 'apply_uptrend_protection' in globals():
                        apply_uptrend_protection(ohlcv_df, config, bot_state)
                    if 'apply_feature_order_spacing' in globals():
                        apply_feature_order_spacing(bot_state, config)
                except Exception as e:
                    logger.error(f"[PROTECTION] Error in protection features: {e}")

                try:
                    logger.info("[LOGGING] Logging protection and feature trade events.")
                    if 'log_protection_event' in globals():
                        log_protection_event(bot_state, config)
                    if 'log_feature_trade' in globals():
                        log_feature_trade(time.time(), 'mainloop', 'mainloop', 'mainloop', bot_state.get('current_price', 0), 0, 'mainloop', 0)
                except Exception as e:
                    logger.error(f"[LOGGING] Error in logging features: {e}")

                try:
                    logger.info("[OPTIMIZATION] Running auto parameter optimization.")
                    if 'auto_optimize_params_bi' in globals() and 'apply_param_changes_bi' in globals():
                        perf_hist = bot_state.get('performance_history', [])
                        if perf_hist and len(perf_hist) >= 10:
                            changes = auto_optimize_params_bi(perf_hist[-20:], config)
                            if changes:
                                logger.info(f"[OPTIMIZATION] Proposed parameter changes: {changes}")
                                apply_param_changes_bi(config, changes)
                except Exception as e:
                    logger.error(f"[OPTIMIZATION] Error in auto parameter optimization: {e}")
                # === END: Active Feature Calls and Logging ===

                current_time = time.time()
                current_volume = ohlcv_df["volume"].iloc[-1] if not ohlcv_df.empty else 0.0

                # Update live data
                ohlcv_df, raw_trades_for_ohlcv_df, data_updated = _update_live_data_from_websocket(
                    websocket_manager, ohlcv_df, raw_trades_for_ohlcv_df
                )

                # Online Updates
                if len(recent_trade_data) >= config.PYTORCH_LOOKBACK:
                    try:
                        # Ensure all features
                        required_features = [
                            "close", "volume", "trades", "rsi", "ema", "volatility", "macd", "macd_signal",
                            "bollinger_upper", "bollinger_lower", "momentum", "volume_trend", "atr", "vwap",
                            "price_spread", "returns", "volume_change", "trade_intensity"
                        ]
                        if not all(f in recent_trade_data.columns for f in required_features):
                            logger.warning("Computing additional features for recent_trade_data")
                            recent_trade_data = compute_additional_features(recent_trade_data)
                            recent_trade_data = recent_trade_data.bfill().ffill().fillna({
                                "price_spread": 0.0, "returns": 0.0, "volume_change": 0.0, "trade_intensity": 0.0
                            })

                        log_ref_timestamp = recent_trade_data['timestamp'].iloc[-1] if not recent_trade_data.empty else pd.Timestamp.now(tz="UTC")
                        log_ref_current_close = recent_trade_data['close'].iloc[-1] if not recent_trade_data.empty else current_price

                        # Sklearn RF prediction
                        sklearn_predicted = last_sklearn_prediction
                        if bot_state.get("sklearn_rf_model"):
                            try:
                                sklearn_predicted = predict_sklearn_price(
                                    bot_state["sklearn_rf_model"], bot_state["scaler_sklearn_rf"],
                                    config.SKLEARN_LOOKBACK, recent_trade_data, current_price, last_sklearn_prediction
                                )
                                logger.info(f"Sklearn Prediction: timestamp={log_ref_timestamp}, predicted_price={sklearn_predicted:.2f}, current_close={log_ref_current_close:.2f}")
                            except Exception as e:
                                logger.error(f"Sklearn prediction failed: {e}")
                                sklearn_predicted = last_sklearn_prediction or current_price

                        # PyTorch prediction
                        pytorch_predicted = last_pytorch_prediction
                        if bot_state.get("pytorch_model"):
                            try:
                                pytorch_predicted = predict_pytorch_price(
                                    bot_state["pytorch_model"], bot_state["pytorch_scaler"], bot_state["pytorch_target_scaler"],
                                    config.PYTORCH_LOOKBACK, recent_trade_data, current_price, last_pytorch_prediction
                                )
                                logger.info(f"Pytorch Prediction: timestamp={log_ref_timestamp}, predicted_price={pytorch_predicted:.2f}, current_close={log_ref_current_close:.2f}")
                            except Exception as e:
                                logger.error(f"Pytorch prediction failed: {e}")
                                pytorch_predicted = last_pytorch_prediction or current_price

                        # XGBoost prediction
                        xgb_predicted = last_xgb_prediction
                        if bot_state.get("xgb_model"):
                            try:
                                xgb_predicted = predict_xgboost_price(
                                    bot_state["xgb_model"], bot_state["xgb_scaler"],
                                    60, recent_trade_data, current_price, last_xgb_prediction  # Fixed lookback=60
                                )
                                logger.info(f"XGBoost Prediction: timestamp={log_ref_timestamp}, predicted_price={xgb_predicted:.2f}, current_close={log_ref_current_close:.2f}")
                            except Exception as e:
                                logger.error(f"XGBoost prediction failed: {e}")
                                xgb_predicted = last_xgb_prediction or current_price

                        # Meta-model prediction
                        meta_predicted = log_ref_current_close
                        if bot_state.get("meta_model") and bot_state.get("meta_scaler") and all(np.isfinite([sklearn_predicted, pytorch_predicted, xgb_predicted])):
                            volatility = recent_trade_data["volatility"].iloc[-1] if not recent_trade_data.empty else 0.1
                            try:
                                meta_predicted = predict_meta_model(
                                    bot_state["meta_model"], bot_state["meta_scaler"],
                                    sklearn_predicted, pytorch_predicted, xgb_predicted, volatility, log_ref_current_close
                                )
                                logger.info(f"Meta-Model Prediction: timestamp={log_ref_timestamp}, predicted_price={meta_predicted:.2f}, current_close={log_ref_current_close:.2f}")
                            except Exception as e:
                                logger.error(f"Meta-model prediction failed: {e}")
                                valid_preds = [p for p in [sklearn_predicted, pytorch_predicted, xgb_predicted] if np.isfinite(p)]
                                meta_predicted = np.mean(valid_preds) if valid_preds else log_ref_current_close
                                logger.info(f"Meta-Model Fallback (mean of base predictions): timestamp={log_ref_timestamp}, predicted_price={meta_predicted:.2f}, current_close={log_ref_current_close:.2f}")

                        # Validate predictions
                        valid_predictions = [
                            p for p in [sklearn_predicted, pytorch_predicted, xgb_predicted, meta_predicted]
                            if np.isfinite(p) and 1000 < p < 10000
                        ]
                        if not valid_predictions:
                            logger.warning("No valid ML predictions, using current price")
                            sklearn_predicted = pytorch_predicted = xgb_predicted = meta_predicted = current_price
                        else:
                            avg_prediction = sum(valid_predictions) / len(valid_predictions)
                            sklearn_predicted = sklearn_predicted if np.isfinite(sklearn_predicted) and 1000 < sklearn_predicted < 10000 else avg_prediction
                            pytorch_predicted = pytorch_predicted if np.isfinite(pytorch_predicted) and 1000 < pytorch_predicted < 10000 else avg_prediction
                            xgb_predicted = xgb_predicted if np.isfinite(xgb_predicted) and 1000 < xgb_predicted < 10000 else avg_prediction
                            meta_predicted = meta_predicted if np.isfinite(meta_predicted) and 1000 < meta_predicted < 10000 else avg_prediction

                        last_sklearn_prediction = sklearn_predicted
                        last_pytorch_prediction = pytorch_predicted
                        last_xgb_prediction = xgb_predicted

                        # Online updates
                        if len(recent_trade_data) >= config.PYTORCH_LOOKBACK:
                            # Sklearn SGD update
                            X_sgd = recent_trade_data.tail(config.SKLEARN_LOOKBACK)[required_features]
                            y_sgd = recent_trade_data["close"].tail(config.SKLEARN_LOOKBACK).values
                            logger.debug(f"Sklearn SGD update: X shape={X_sgd.shape}, y shape={y_sgd.shape}")
                            if bot_state.get("sklearn_sgd_model"):
                                bot_state["sklearn_sgd_model"] = online_update_sklearn(
                                    bot_state["sklearn_sgd_model"], bot_state["scaler_sklearn_sgd"], X_sgd, y_sgd
                                )
                                logger.info("Sklearn SGD model updated online")
                            else:
                                logger.error("SGDRegressor model not initialized")

                            # PyTorch update
                            X_pytorch = recent_trade_data.tail(config.PYTORCH_LOOKBACK)[required_features]
                            y_pytorch = recent_trade_data["close"].tail(config.PYTORCH_LOOKBACK)
                            logger.debug(f"PyTorch update: X shape={X_pytorch.shape}, y shape={y_pytorch.shape}")

                            if bot_state.get("pytorch_model"):
                                if bot_state.get("pytorch_scaler") is not None and bot_state.get("pytorch_target_scaler") is not None:
                                    bot_state["pytorch_model"] = online_update_pytorch(
                                        bot_state["pytorch_model"], bot_state["pytorch_scaler"], bot_state["pytorch_target_scaler"],
                                        X_pytorch, y_pytorch, config.DEVICE
                                    )
                                    logger.info("PyTorch model updated online")
                                else:
                                    logger.error("PyTorch scaler or target scaler is None, skipping PyTorch online update")
                            else:
                                logger.error("PyTorch model not initialized")

                            logger.info("Performed online model updates")
                    except Exception as ml_error:
                        logger.error(f"Error in ML predictions or updates: {ml_error}")

                # 1. Fetch Market Data
                current_volume = 0.0
                try:
                    ticker = exchange.fetch_ticker(config.SYMBOL)
                    if not ticker or "last" not in ticker:
                        logger.error("Failed to fetch ticker data, skipping iteration")
                        time.sleep(config.CHECK_ORDER_FREQUENCY)
                        continue
                    current_price = float(ticker["last"])
                    current_volume = float(ticker["baseVolume"]) if ticker.get("baseVolume") else 0
                    minute_key = pd.Timestamp.now(tz="UTC").floor("1min")
                    trade_counts[minute_key] = trade_counts.get(minute_key, 0) + 1
                    logger.debug(f"Fetched market data: price={current_price:.2f}, volume={current_volume:.2f}")
                except Exception as market_error:
                    logger.error(f"Error fetching market data: {market_error}")
                    time.sleep(config.CHECK_ORDER_FREQUENCY)
                    continue

                # 2. Update Balances
                for attempt in range(3):
                    try:
                        eth_balance, usd_balance = sync_balances(exchange)
                        logger.debug(
                            f"Balance updated (attempt {attempt + 1}/3): ETH={eth_balance:.6f}, USD={usd_balance:.2f}"
                        )
                        break
                    except ccxt.RateLimitExceeded as rate_error:
                        logger.warning(f"Rate limit exceeded on balance sync attempt {attempt + 1}/3: {rate_error}")
                        time.sleep(1)
                    except Exception as balance_error:
                        logger.error(f"Balance sync attempt {attempt + 1}/3 failed: {balance_error}")
                        time.sleep(1)
                else:
                    logger.error("Failed to sync balances after 3 attempts, pausing bot")
                    bot_state["paused"] = True
                    time.sleep(config.CHECK_ORDER_FREQUENCY)
                    continue
                
                # 3. Refresh Data for ML Predictions
                try:
                    logger.info("Checking for data refresh: ohlcv_df_rows=%d, last_timestamp=%s",
                                len(ohlcv_df), ohlcv_df['timestamp'].iloc[-1] if not ohlcv_df.empty else 'None')

                    if not hasattr(_update_live_data_from_websocket, "last_ohlcv_update_log_time"):
                        _update_live_data_from_websocket.last_ohlcv_update_log_time = 0
                        _update_live_data_from_websocket.ohlcv_log_interval = 300  # 5 minutes
                        logger.info("Initialized WebSocket update logging: interval=%ds", _update_live_data_from_websocket.ohlcv_log_interval)

                    if current_time - last_data_refresh > 60:  # Refresh every 60 seconds
                        logger.info("Starting data refresh: current_time=%s", pd.Timestamp.now(tz="UTC"))
                        try:
                            # Prevent duplicate WebSocket updates
                            if hasattr(_update_live_data_from_websocket, "lock"):
                                if not _update_live_data_from_websocket.lock.acquire(blocking=False):
                                    logger.debug("Skipping WebSocket update due to active lock")
                                    return
                            else:
                                _update_live_data_from_websocket.lock = threading.Lock()
                                _update_live_data_from_websocket.lock.acquire()
                            logger.info("Acquired WebSocket update lock")

                            try:
                                if (current_time - getattr(_update_live_data_from_websocket, "last_update", 0) <
                                        config.WEBSOCKET_MIN_UPTIME):
                                    logger.debug("Skipping redundant WebSocket update, last_update=%.2fs ago",
                                                current_time - _update_live_data_from_websocket.last_update)
                                    return
                                _update_live_data_from_websocket.last_update = current_time
                                logger.info("Proceeding with WebSocket data update")

                                try:
                                    ohlcv_df, raw_trades_for_ohlcv_df, updated = _update_live_data_from_websocket(
                                        websocket_manager, ohlcv_df, raw_trades_for_ohlcv_df)
                                except Exception as ws_update_error:
                                    logger.error("Failed to update WebSocket data: %s", str(ws_update_error))
                                    raise

                                if updated and not ohlcv_df.empty:
                                    num_new_candles = len(ohlcv_df) - len(ohlcv_df.drop_duplicates())
                                    total_rows = len(ohlcv_df)
                                    if num_new_candles > 0:
                                        current_time_epoch = time.time()
                                        if (current_time_epoch - _update_live_data_from_websocket.last_ohlcv_update_log_time >=
                                                _update_live_data_from_websocket.ohlcv_log_interval):
                                            logger.info("Updated OHLCV data with %d new candle(s) from WebSocket. Total rows=%d, last_close=%.2f",
                                                        num_new_candles, total_rows, ohlcv_df['close'].iloc[-1])
                                            _update_live_data_from_websocket.last_ohlcv_update_log_time = current_time_epoch
                                        else:
                                            logger.debug("Added %d new candle(s) via WebSocket. Total rows=%d (INFO log throttled)",
                                                        num_new_candles, total_rows)
                                    else:
                                        logger.debug("Processed WebSocket data, 0 new candles added. Total rows=%d", total_rows)

                                    # Define dynamic defaults using WebSocket data
                                    close_mean = ohlcv_df["close"].mean() if not ohlcv_df.empty else current_price
                                    defaults = {
                                        "rsi": ohlcv_df["rsi"].mean() if not ohlcv_df.empty and "rsi" in ohlcv_df else config.LOG_DEFAULT_RSI,
                                        "ema": close_mean,
                                        "volatility": ohlcv_df["volatility"].mean() if not ohlcv_df.empty and "volatility" in ohlcv_df else config.LOG_DEFAULT_VOLATILITY,
                                        "macd": config.LOG_DEFAULT_MACD,
                                        "macd_signal": config.LOG_DEFAULT_MACD_SIGNAL,
                                        "bollinger_upper": close_mean,
                                        "bollinger_lower": close_mean,
                                        "momentum": config.LOG_DEFAULT_MOMENTUM,
                                        "volume_trend": config.LOG_DEFAULT_VOLUME_TREND,
                                        "atr": close_mean * config.LOG_DEFAULT_ATR,
                                        "vwap": close_mean,
                                        "predicted_price": ohlcv_df["close"].iloc[-1] if not ohlcv_df.empty else current_price,
                                        "grid_level": 0,
                                        "trades": int(ohlcv_df["trades"].mean()) if not ohlcv_df.empty and "trades" in ohlcv_df else 0,
                                        "close": close_mean,
                                        "high": ohlcv_df["high"].mean() if not ohlcv_df.empty and "high" in ohlcv_df else close_mean * 1.001,
                                        "low": ohlcv_df["low"].mean() if not ohlcv_df.empty and "low" in ohlcv_df else close_mean * 0.999,
                                        "open": close_mean,
                                        "volume": ohlcv_df["volume"].mean() if not ohlcv_df.empty and "volume" in ohlcv_df else 0.0,
                                        "price_spread": ohlcv_df["price_spread"].mean() if not ohlcv_df.empty and "price_spread" in ohlcv_df else 0.0,
                                        "returns": ohlcv_df["returns"].mean() if not ohlcv_df.empty and "returns" in ohlcv_df else 0.0,
                                        "volume_change": ohlcv_df["volume_change"].mean() if not ohlcv_df.empty and "volume_change" in ohlcv_df else 0.0,
                                        "trade_intensity": ohlcv_df["trade_intensity"].mean() if not ohlcv_df.empty and "trade_intensity" in ohlcv_df else 0.0
                                    }
                                    logger.info("Defined dynamic defaults: rsi=%.2f, volatility=%.4f, atr=%.2f, price_spread=%.2f",
                                                defaults["rsi"], defaults["volatility"], defaults["atr"], defaults["price_spread"])

                                    # Ensure ohlcv_df has all required columns
                                    required_columns = ["timestamp", "open", "high", "low", "close", "volume"]
                                    missing_columns = [c for c in required_columns if c not in ohlcv_df.columns]
                                    if missing_columns:
                                        logger.info("Missing columns in ohlcv_df: %s, adding with defaults", missing_columns)
                                        for col in missing_columns:
                                            if col == "timestamp":
                                                ohlcv_df["timestamp"] = pd.Timestamp.now(tz="UTC")
                                            elif col in ["open", "high", "low", "close"]:
                                                ohlcv_df[col] = close_mean
                                            elif col == "volume":
                                                ohlcv_df[col] = 0.0

                                    # Validate and fix timestamps
                                    if "timestamp" in ohlcv_df:
                                        logger.debug("Raw ohlcv_df timestamps before conversion: %s", ohlcv_df['timestamp'].head().to_list())
                                        ohlcv_df["timestamp"] = pd.to_datetime(ohlcv_df["timestamp"], errors="coerce", utc=True)
                                        if ohlcv_df["timestamp"].isna().any():
                                            logger.info("Found %d invalid timestamps in ohlcv_df, filling with current time",
                                                        ohlcv_df['timestamp'].isna().sum())
                                            ohlcv_df["timestamp"] = ohlcv_df["timestamp"].fillna(pd.Timestamp.now(tz="UTC"))
                                    else:
                                        logger.info("No timestamp column in ohlcv_df, setting to current time")
                                        ohlcv_df["timestamp"] = pd.Timestamp.now(tz="UTC")

                                    # Fill NaN values in OHLCV columns with forward/backward fill
                                    for col in ["open", "high", "low", "close", "volume"]:
                                        if ohlcv_df[col].isna().any():
                                            logger.info("Filling %d NaN values in ohlcv_df[%s] with forward/backward fill",
                                                        ohlcv_df[col].isna().sum(), col)
                                            ohlcv_df[col] = ohlcv_df[col].ffill().bfill()
                                            if ohlcv_df[col].isna().any():
                                                logger.info("Persisting %d NaNs in ohlcv_df[%s], filling with %s",
                                                            ohlcv_df[col].isna().sum(), col, defaults.get(col, 0.0))
                                                ohlcv_df[col] = ohlcv_df[col].fillna(defaults.get(col, 0.0))

                                    # Ensure ohlcv_df has all required features
                                    required_features = [
                                        "close", "volume", "trades", "rsi", "ema", "macd", "macd_signal",
                                        "bollinger_upper", "bollinger_lower", "momentum", "volume_trend",
                                        "atr", "vwap", "predicted_price", "grid_level", "volatility",
                                        "high", "low", "price_spread", "returns", "volume_change", "trade_intensity"
                                    ]
                                    missing_features = [f for f in required_features if f not in ohlcv_df.columns]
                                    if missing_features:
                                        logger.info("Missing features in ohlcv_df: %s, adding with defaults", missing_features)
                                        for feature in missing_features:
                                            ohlcv_df[feature] = defaults.get(feature, 0.0)

                                    # Compute technical indicators with validation
                                    try:
                                        logger.info("Computing technical indicators for ohlcv_df")
                                        ohlcv_df["rsi"] = compute_rsi(ohlcv_df["close"], periods=config.RSI_PERIOD)
                                        ohlcv_df["ema"] = compute_ema(ohlcv_df["close"], span=config.EMA_SPAN)
                                        ohlcv_df["volatility"] = compute_volatility(ohlcv_df, periods=config.VOLATILITY_WINDOW)
                                        ohlcv_df["macd"], ohlcv_df["macd_signal"] = compute_macd(
                                            ohlcv_df["close"],
                                            fast=config.MACD_FAST,
                                            slow=config.MACD_SLOW,
                                            signal=config.MACD_SIGNAL,
                                        )
                                        ohlcv_df["bollinger_upper"], ohlcv_df["bollinger_lower"] = compute_bollinger(
                                            ohlcv_df["close"],
                                            window=config.BOLLINGER_WINDOW,
                                            num_std=config.BOLLINGER_NUM_STD,
                                        )
                                        ohlcv_df["momentum"] = compute_momentum(ohlcv_df["close"], periods=config.MOMENTUM_PERIOD)
                                        ohlcv_df["volume_trend"] = compute_volume_trend(ohlcv_df["volume"], window=5)
                                        ohlcv_df["atr"] = compute_atr(
                                            ohlcv_df["high"],
                                            ohlcv_df["low"],
                                            ohlcv_df["close"],
                                            periods=config.ATR_PERIOD,
                                        )
                                        ohlcv_df["vwap"] = compute_vwap(ohlcv_df, period=config.VWAP_PERIOD)
                                        ohlcv_df["predicted_price"] = ohlcv_df["close"].shift(-1).fillna(ohlcv_df["close"].iloc[-1])
                                        ohlcv_df["grid_level"] = 0
                                        logger.info("Computed indicators: atr=%.2f, vwap=%.2f, rsi=%.2f, volatility=%.4f, price_spread=%.2f",
                                                    ohlcv_df['atr'].iloc[-1], ohlcv_df['vwap'].iloc[-1],
                                                    ohlcv_df['rsi'].iloc[-1], ohlcv_df['volatility'].iloc[-1],
                                                    ohlcv_df['price_spread'].iloc[-1])
                                    except Exception as e:
                                        logger.error("Error computing indicators for ohlcv_df: %s, using defaults", str(e))
                                        for feature in required_features:
                                            ohlcv_df[feature] = defaults.get(feature, 0.0)

                                    # Fill NaN values in computed features with forward/backward fill
                                    for feature in required_features:
                                        if feature in ohlcv_df and ohlcv_df[feature].isna().any():
                                            logger.info("Filling %d NaN values in ohlcv_df[%s] with forward/backward fill",
                                                        ohlcv_df[feature].isna().sum(), feature)
                                            ohlcv_df[feature] = ohlcv_df[feature].ffill().bfill()
                                            if ohlcv_df[feature].isna().any():
                                                logger.info("Persisting %d NaNs in ohlcv_df[%s], filling with %s",
                                                            ohlcv_df[feature].isna().sum(), feature, defaults.get(feature, 0.0))
                                                ohlcv_df[feature] = ohlcv_df[feature].fillna(defaults.get(feature, 0.0))
                                        elif feature in ohlcv_df:
                                            logger.debug("No NaN values in ohlcv_df[%s]: min=%.2f, max=%.2f",
                                                        feature, ohlcv_df[feature].min(), ohlcv_df[feature].max())

                                    # Assign to recent_data and recent_trade_data
                                    logger.info("Assigning data to recent_data and recent_trade_data: lookback=%d", lookback)
                                    if len(ohlcv_df) < lookback:
                                        logger.info("Padding recent_data: ohlcv_df_rows=%d, needed=%d", len(ohlcv_df), lookback)
                                        for feature in required_features:
                                            if feature not in historical_data.columns:
                                                close_mean_hist = historical_data["close"].mean() if "close" in historical_data else current_price
                                                historical_data[feature] = defaults.get(
                                                    feature,
                                                    close_mean_hist if feature in ["ema", "vwap", "bollinger_upper",
                                                                                "bollinger_lower", "high", "low"] else 0.0)
                                        historical_subset = historical_data[sklearn_features].tail(lookback - len(ohlcv_df)).copy()
                                        historical_subset["timestamp"] = pd.to_datetime(
                                            historical_subset["timestamp"], errors="coerce", utc=True).fillna(pd.Timestamp.now(tz="UTC"))
                                        other_features = [
                                            "timestamp", "close", "volume", "trades", "rsi", "ema", "volatility", "macd",
                                            "macd_signal", "bollinger_upper", "bollinger_lower", "momentum", "volume_trend",
                                            "high", "low", "atr", "vwap", "predicted_price", "grid_level",
                                            "price_spread", "returns", "volume_change", "trade_intensity"
                                        ]
                                        recent_data = pd.concat([historical_subset, ohlcv_df[sklearn_features]], ignore_index=True).tail(lookback)
                                        recent_trade_data = pd.concat(
                                            [historical_data[other_features].tail(lookback - len(ohlcv_df)), ohlcv_df[other_features]],
                                            ignore_index=True).tail(lookback)
                                    else:
                                        other_features = [
                                            "timestamp", "close", "volume", "trades", "rsi", "ema", "volatility", "macd",
                                            "macd_signal", "bollinger_upper", "bollinger_lower", "momentum", "volume_trend",
                                            "high", "low", "atr", "vwap", "predicted_price", "grid_level",
                                            "price_spread", "returns", "volume_change", "trade_intensity"
                                        ]
                                        recent_data = ohlcv_df[sklearn_features].tail(lookback).copy()
                                        recent_trade_data = ohlcv_df[other_features].tail(lookback).copy()
                                        logger.info("Assigned recent_trade_data: rows=%d, last_timestamp=%s, last_close=%.2f, last_atr=%.2f, last_price_spread=%.2f",
                                                    len(recent_trade_data), recent_trade_data["timestamp"].iloc[-1],
                                                    recent_trade_data["close"].iloc[-1], recent_trade_data["atr"].iloc[-1],
                                                    recent_trade_data["price_spread"].iloc[-1])

                                    # Validate timestamps
                                    logger.info("Validating timestamps for recent_data and recent_trade_data")
                                    for dataset, name in [(recent_data, "recent_data"), (recent_trade_data, "recent_trade_data")]:
                                        if ("timestamp" not in dataset or dataset["timestamp"].isna().all() or
                                                not pd.api.types.is_datetime64_any_dtype(dataset["timestamp"])):
                                            logger.info("Invalid timestamps in %s, setting to current time", name)
                                            dataset["timestamp"] = pd.Series([pd.Timestamp.now(tz="UTC")] * len(dataset), index=dataset.index)
                                        dataset["timestamp"] = pd.to_datetime(dataset["timestamp"], errors="coerce", utc=True)
                                        if dataset["timestamp"].isna().any():
                                            logger.info("Found %d invalid timestamps in %s, filling with current time",
                                                        dataset["timestamp"].isna().sum(), name)
                                            dataset["timestamp"] = dataset["timestamp"].fillna(pd.Timestamp.now(tz="UTC"))

                                    # Initialize prediction_history if empty
                                    if "prediction_history" not in bot_state or bot_state["prediction_history"].empty:
                                        logger.info("Initializing prediction_history")
                                        close_value = recent_trade_data["close"].iloc[-1] if not recent_trade_data.empty and "close" in recent_trade_data else current_price
                                        volatility_value = recent_trade_data["volatility"].iloc[-1] if not recent_trade_data.empty and "volatility" in recent_trade_data else defaults["volatility"]
                                        actual_price = recent_trade_data["predicted_price"].iloc[-1] if not recent_trade_data.empty and "predicted_price" in recent_trade_data else current_price
                                        bot_state["prediction_history"] = pd.DataFrame({
                                            "sklearn_pred": [last_sklearn_prediction or close_value],
                                            "pytorch_pred": [last_pytorch_prediction or close_value],
                                            "xgb_pred": [last_xgb_prediction or close_value],
                                            "current_price": [close_value],
                                            "volatility": [volatility_value],
                                            "actual_price": [actual_price]
                                        })
                                        logger.info("Initialized prediction_history: rows=1, close=%.2f, volatility=%.4f",
                                                    close_value, volatility_value)
                                        logger.debug("Prediction_history details: %s", bot_state['prediction_history'].iloc[-1].to_dict())

                                    logger.debug("Recent_data columns: %s", recent_data.columns.tolist())

                            finally:
                                _update_live_data_from_websocket.lock.release()
                                logger.info("Released WebSocket update lock")

                        except Exception as ws_error:
                                logger.error("Error updating WebSocket data: %s, falling back to historical data", str(ws_error))
                                try:
                                    for feature in required_features:
                                        if feature not in historical_data.columns:
                                            close_mean = historical_data["close"].mean() if "close" in historical_data else current_price
                                            historical_data[feature] = defaults.get(
                                                feature,
                                                close_mean if feature in ["ema", "vwap", "bollinger_upper", "bollinger_lower", "high", "low"] else 0.0)
                                    recent_data = historical_data[sklearn_features].tail(lookback).copy()
                                    recent_trade_data = trade_data[other_features].tail(lookback).copy() if trade_data is not None else recent_data[other_features].copy()
                                    recent_data["timestamp"] = pd.to_datetime(recent_data["timestamp"], errors="coerce", utc=True).fillna(pd.Timestamp.now(tz="UTC"))
                                    recent_trade_data["timestamp"] = pd.to_datetime(recent_trade_data["timestamp"], errors="coerce", utc=True).fillna(pd.Timestamp.now(tz="UTC"))
                                    logger.info("Assigned fallback data: recent_data_rows=%d, recent_trade_data_rows=%d",
                                                len(recent_data), len(recent_trade_data))
                                    logger.debug("Recent_data columns (fallback): %s", recent_data.columns.tolist())
                                    logger.debug("Recent_trade_data columns (fallback): %s", recent_trade_data.columns.tolist())

                                    # Initialize prediction_history in error case
                                    if "prediction_history" not in bot_state or bot_state["prediction_history"].empty:
                                        logger.info("Initializing prediction_history (fallback)")
                                        close_value = recent_trade_data["close"].iloc[-1] if not recent_trade_data.empty and "close" in recent_trade_data else current_price
                                        volatility_value = recent_trade_data["volatility"].iloc[-1] if not recent_trade_data.empty and "volatility" in recent_trade_data else defaults["volatility"]
                                        actual_price = recent_trade_data["predicted_price"].iloc[-1] if not recent_trade_data.empty and "predicted_price" in recent_trade_data else current_price
                                        bot_state["prediction_history"] = pd.DataFrame({
                                            "sklearn_pred": [last_sklearn_prediction or close_value],
                                            "pytorch_pred": [last_pytorch_prediction or close_value],
                                            "xgb_pred": [last_xgb_prediction or close_value],
                                            "current_price": [close_value],
                                            "volatility": [volatility_value],
                                            "actual_price": [actual_price]
                                        })
                                        logger.info("Initialized prediction_history (fallback): rows=1, close=%.2f, volatility=%.4f",
                                                    close_value, volatility_value)
                                        logger.debug("Prediction_history details (fallback): %s", bot_state['prediction_history'].iloc[-1].to_dict())
                                except Exception as fallback_error:
                                    logger.error("Error assigning fallback data: %s", str(fallback_error))

                        except Exception as ws_error:
                            logger.error("Error in WebSocket update process: %s", str(ws_error))

                        last_data_refresh = current_time
                        logger.info("Refreshed historical and trade data for ML predictions: duration=%.2fs",
                                    time.time() - current_time)
                except Exception as refresh_error:
                    logger.error("Error refreshing data for ML predictions: %s", str(refresh_error))

                # 4. Retraining
                try:
                    logger.info("Checking ML model retraining conditions: current_time=%s", pd.Timestamp.now(tz="UTC"))
                    if current_time - last_retrain_check > config.RETRAIN_INTERVAL:  # Check every 1200 seconds
                        logger.info("RETRAIN_INTERVAL met: time_since_last_check=%.2fs, interval=%ds", 
                                    current_time - last_retrain_check, config.RETRAIN_INTERVAL)
                        time_since_last_retrain = current_time - last_retrain_time
                        min_data_rows = max(1, config.LOOKBACK // 600 + 80)  # ~80 rows
                        should_retrain = time_since_last_retrain >= config.MAX_RETRAIN_INTERVAL or (
                            time_since_last_retrain >= config.MIN_RETRAIN_INTERVAL and len(recent_data) >= min_data_rows
                        )

                        if should_retrain:
                            logger.info("Retraining triggered: time_since_last_retrain=%.2fs, recent_data_rows=%d, min_data_rows=%d",
                                        time_since_last_retrain, len(recent_data), min_data_rows)
                            # Use WebSocket ohlcv_df if sufficient, else fetch additional data
                            if len(recent_data) < 61 or len(recent_trade_data) < 61:
                                logger.info("Insufficient data for retraining: recent_data=%d, recent_trade_data=%d, needed=61",
                                            len(recent_data), len(recent_trade_data))
                                try:
                                    logger.info("Attempting WebSocket data update: ohlcv_df_rows=%d", len(ohlcv_df))
                                    # Ensure ohlcv_df has enough rows
                                    if len(ohlcv_df) < 2 * config.LOOKBACK:
                                        logger.info("Forcing WebSocket data update: ohlcv_df_rows=%d < needed=%d",
                                                    len(ohlcv_df), 2 * config.LOOKBACK)
                                        ohlcv_df, raw_trades_for_ohlcv_df, updated = _update_live_data_from_websocket(
                                            websocket_manager,
                                            ohlcv_df,
                                            raw_trades_for_ohlcv_df,
                                        )
                                        if updated and not ohlcv_df.empty:
                                            logger.info("Updated ohlcv_df via WebSocket: new_rows=%d, last_close=%.2f",
                                                        len(ohlcv_df), ohlcv_df['close'].iloc[-1])

                                    logger.info("Defining dynamic defaults for retraining")
                                    # Define dynamic defaults using WebSocket data
                                    close_mean = ohlcv_df["close"].mean() if not ohlcv_df.empty else current_price
                                    defaults = {
                                        "rsi": ohlcv_df["rsi"].mean() if not ohlcv_df.empty and "rsi" in ohlcv_df else 50.0,
                                        "ema": close_mean,
                                        "volatility": ohlcv_df["volatility"].mean() if not ohlcv_df.empty and "volatility" in ohlcv_df else 0.1,
                                        "macd": 0.0,
                                        "macd_signal": 0.0,
                                        "bollinger_upper": close_mean,
                                        "bollinger_lower": close_mean,
                                        "momentum": 0.0,
                                        "volume_trend": 0.0,
                                        "atr": close_mean * 0.01,
                                        "vwap": close_mean,
                                        "predicted_price": ohlcv_df["close"].iloc[-1] if not ohlcv_df.empty else current_price,
                                        "grid_level": 0,
                                        "trades": int(ohlcv_df["trades"].mean()) if not ohlcv_df.empty and "trades" in ohlcv_df else 0,
                                        "close": close_mean,
                                        "high": ohlcv_df["high"].mean() if not ohlcv_df.empty and "high" in ohlcv_df else close_mean * 1.001,
                                        "low": ohlcv_df["low"].mean() if not ohlcv_df.empty and "low" in ohlcv_df else close_mean * 0.999,
                                        "open": close_mean,
                                        "volume": ohlcv_df["volume"].mean() if not ohlcv_df.empty and "volume" in ohlcv_df else 0.0,
                                        "price_spread": ohlcv_df["price_spread"].mean() if not ohlcv_df.empty and "price_spread" in ohlcv_df else 0.0,
                                        "returns": ohlcv_df["returns"].mean() if not ohlcv_df.empty and "returns" in ohlcv_df else 0.0,
                                        "volume_change": ohlcv_df["volume_change"].mean() if not ohlcv_df.empty and "volume_change" in ohlcv_df else 0.0,
                                        "trade_intensity": ohlcv_df["trade_intensity"].mean() if not ohlcv_df.empty and "trade_intensity" in ohlcv_df else 0.0
                                    }
                                    logger.info("Defined dynamic defaults: rsi=%.2f, volatility=%.4f, atr=%.2f, price_spread=%.2f",
                                                defaults["rsi"], defaults["volatility"], defaults["atr"], defaults["price_spread"])

                                    # Use WebSocket data if sufficient
                                    if len(ohlcv_df) >= 2 * config.LOOKBACK:
                                        logger.info("Using WebSocket ohlcv_df for retraining: ohlcv_df_rows=%d", len(ohlcv_df))
                                        recent_data = ohlcv_df[sklearn_features].tail(2 * config.LOOKBACK).copy()
                                        recent_trade_data = ohlcv_df[
                                            [
                                                "timestamp", "close", "volume", "trades", "rsi", "ema",
                                                "volatility", "macd", "macd_signal", "bollinger_upper",
                                                "bollinger_lower", "momentum", "volume_trend", "high",
                                                "low", "atr", "vwap", "predicted_price", "grid_level",
                                                "price_spread", "returns", "volume_change", "trade_intensity"
                                            ]
                                        ].tail(2 * config.LOOKBACK).copy()
                                        logger.info("Assigned WebSocket data: recent_data_rows=%d, recent_trade_data_rows=%d",
                                                    len(recent_data), len(recent_trade_data))
                                    else:
                                        logger.info("Fetching additional trade data via API: ohlcv_df_rows=%d < needed=%d",
                                                    len(ohlcv_df), 2 * config.LOOKBACK)
                                        trade_counts_total = pd.DataFrame()
                                        try:
                                            for attempt in range(7):
                                                since = int(
                                                    (current_time - (2 * config.LOOKBACK + attempt * 600) * 60) * 1000
                                                )
                                                until = int(current_time * 1000)
                                                logger.info("Fetching trades: attempt=%d, since=%s, until=%s",
                                                            attempt + 1, pd.Timestamp(since, unit="ms", tz="UTC"), pd.Timestamp(until, unit="ms", tz="UTC"))
                                                trades = exchange.fetch_trades(
                                                    config.SYMBOL,
                                                    limit=1000,
                                                    since=since,
                                                    params={"until": until},
                                                )
                                                if trades:
                                                    trade_data = pd.DataFrame(trades)
                                                    if "timestamp" in trade_data:
                                                        trade_data["timestamp"] = pd.to_datetime(
                                                            trade_data["timestamp"],
                                                            unit="ms",
                                                            utc=True,
                                                            errors="coerce",
                                                        )
                                                        trade_data["timestamp"] = trade_data["timestamp"].fillna(
                                                            pd.Timestamp.now(tz="UTC")
                                                        )
                                                        trade_data["price"] = trade_data["price"].astype(float)
                                                        trade_data["amount"] = trade_data["amount"].astype(float)
                                                        trade_counts = (
                                                            trade_data.groupby(pd.Grouper(key="timestamp", freq="1min"))
                                                            .agg(
                                                                {
                                                                    "price": "mean",
                                                                    "amount": "sum",
                                                                    "id": "count",
                                                                }
                                                            )
                                                            .reset_index()
                                                        )
                                                        trade_counts = trade_counts.rename(
                                                            columns={
                                                                "id": "trades",
                                                                "amount": "volume",
                                                                "price": "close",
                                                            }
                                                        )
                                                        trade_counts_total = pd.concat(
                                                            [trade_counts_total, trade_counts],
                                                            ignore_index=True,
                                                        )
                                                        logger.info("Fetched %d trade records in attempt %d, total_records=%d",
                                                                    len(trade_counts), attempt + 1, len(trade_counts_total))
                                                        if len(trade_counts_total) >= 122:
                                                            break
                                                time.sleep(exchange.rateLimit / 1000)
                                        except Exception as trade_error:
                                            logger.error("Failed to fetch trades: %s", str(trade_error))

                                        if not trade_counts_total.empty:
                                            logger.info("Processing fetched trade data: total_records=%d", len(trade_counts_total))
                                            trade_counts_total = trade_counts_total.drop_duplicates(
                                                subset=["timestamp"], keep="last"
                                            ).sort_values("timestamp")
                                            if not recent_trade_data.empty:
                                                recent_trade_data = pd.concat(
                                                    [recent_trade_data, trade_counts_total],
                                                    ignore_index=True,
                                                )
                                                recent_trade_data = (
                                                    recent_trade_data.drop_duplicates(subset=["timestamp"], keep="last")
                                                    .sort_values("timestamp")
                                                    .tail(2 * config.LOOKBACK)
                                                )
                                            else:
                                                recent_trade_data = trade_counts_total.tail(2 * config.LOOKBACK)
                                            logger.info("Updated recent_trade_data with trade data: rows=%d", len(recent_trade_data))
                                        else:
                                            logger.warning("No trades fetched, using OHLCV-derived trade counts")
                                            if not ohlcv_df.empty:
                                                recent_trade_data = (
                                                    ohlcv_df[
                                                        [
                                                            "timestamp",
                                                            "close",
                                                            "volume",
                                                            "trades",
                                                        ]
                                                    ]
                                                    .tail(2 * config.LOOKBACK)
                                                    .copy()
                                                )
                                                recent_trade_data["trades"] = recent_trade_data["trades"].fillna(
                                                    defaults["trades"]
                                                )
                                                logger.info("Updated recent_trade_data with OHLCV trades: rows=%d", len(recent_trade_data))

                                        logger.info("Fetching additional OHLCV data via API")
                                        try:
                                            timeframe = "1m"
                                            since = int(
                                                (current_time - (2 * config.LOOKBACK + 40) * 60) * 1000
                                            )
                                            logger.info("Fetching OHLCV: timeframe=%s, since=%s, limit=%d",
                                                        timeframe, pd.Timestamp(since, unit="ms", tz="UTC"), 2 * config.LOOKBACK)
                                            ohlcv = exchange.fetch_ohlcv(
                                                config.SYMBOL,
                                                timeframe,
                                                since=since,
                                                limit=2 * config.LOOKBACK,
                                            )
                                            if ohlcv:
                                                ohlcv_df_new = pd.DataFrame(
                                                    ohlcv,
                                                    columns=[
                                                        "timestamp",
                                                        "open",
                                                        "high",
                                                        "low",
                                                        "close",
                                                        "volume",
                                                    ],
                                                )
                                                ohlcv_df_new["timestamp"] = pd.to_datetime(
                                                    ohlcv_df_new["timestamp"],
                                                    unit="ms",
                                                    utc=True,
                                                    errors="coerce",
                                                )
                                                ohlcv_df_new["timestamp"] = ohlcv_df_new["timestamp"].fillna(
                                                    pd.Timestamp.now(tz="UTC")
                                                )
                                                logger.info("Fetched %d OHLCV records, computing indicators", len(ohlcv_df_new))
                                                if len(ohlcv_df_new) < config.BOLLINGER_WINDOW:
                                                    logger.warning(
                                                        "Insufficient OHLCV data: rows=%d, need=%d", len(ohlcv_df_new), config.BOLLINGER_WINDOW
                                                    )
                                                for col in ["open", "high", "low", "close", "volume"]:
                                                    ohlcv_df_new[col] = ohlcv_df_new[col].ffill().bfill()
                                                    if ohlcv_df_new[col].isna().any():
                                                        ohlcv_df_new[col] = ohlcv_df_new[col].fillna(defaults.get(col, 0.0))
                                                ohlcv_df_new["rsi"] = compute_rsi(
                                                    ohlcv_df_new["close"],
                                                    periods=config.RSI_PERIOD,
                                                ).fillna(defaults["rsi"])
                                                ohlcv_df_new["ema"] = compute_ema(
                                                    ohlcv_df_new["close"], span=config.EMA_SPAN
                                                ).fillna(defaults["ema"])
                                                ohlcv_df_new["volatility"] = compute_volatility(
                                                    ohlcv_df_new, periods=config.VOLATILITY_WINDOW
                                                ).fillna(defaults["volatility"])
                                                (
                                                    ohlcv_df_new["macd"],
                                                    ohlcv_df_new["macd_signal"],
                                                ) = compute_macd(
                                                    ohlcv_df_new["close"],
                                                    fast=config.MACD_FAST,
                                                    slow=config.MACD_SLOW,
                                                    signal=config.MACD_SIGNAL,
                                                )
                                                ohlcv_df_new["macd"] = ohlcv_df_new["macd"].fillna(defaults["macd"])
                                                ohlcv_df_new["macd_signal"] = ohlcv_df_new["macd_signal"].fillna(
                                                    defaults["macd_signal"]
                                                )
                                                (
                                                    ohlcv_df_new["bollinger_upper"],
                                                    ohlcv_df_new["bollinger_lower"],
                                                ) = compute_bollinger(
                                                    ohlcv_df_new["close"],
                                                    window=config.BOLLINGER_WINDOW,
                                                    num_std=config.BOLLINGER_NUM_STD,
                                                )
                                                ohlcv_df_new["bollinger_upper"] = ohlcv_df_new["bollinger_upper"].fillna(
                                                    defaults["bollinger_upper"]
                                                )
                                                ohlcv_df_new["bollinger_lower"] = ohlcv_df_new["bollinger_lower"].fillna(
                                                    defaults["bollinger_lower"]
                                                )
                                                ohlcv_df_new["momentum"] = compute_momentum(
                                                    ohlcv_df_new["close"],
                                                    periods=config.MOMENTUM_PERIOD,
                                                ).fillna(defaults["momentum"])
                                                ohlcv_df_new["volume_trend"] = compute_volume_trend(
                                                    ohlcv_df_new["volume"], window=5
                                                ).fillna(defaults["volume_trend"])
                                                ohlcv_df_new["atr"] = compute_atr(
                                                    ohlcv_df_new["high"],
                                                    ohlcv_df_new["low"],
                                                    ohlcv_df_new["close"],
                                                    periods=config.ATR_PERIOD,
                                                ).fillna(defaults["atr"])
                                                ohlcv_df_new["vwap"] = compute_vwap(
                                                    ohlcv_df_new, period=config.VWAP_PERIOD
                                                ).fillna(defaults["vwap"])
                                                ohlcv_df_new["predicted_price"] = (
                                                    ohlcv_df_new["close"].shift(-1).fillna(defaults["predicted_price"])
                                                )
                                                ohlcv_df_new["grid_level"] = defaults["grid_level"]
                                                ohlcv_df_new["trades"] = (
                                                    recent_trade_data["trades"]
                                                    .iloc[-len(ohlcv_df_new) :]
                                                    .reindex(ohlcv_df_new.index)
                                                    .fillna(defaults["trades"])
                                                    if "trades" in recent_trade_data
                                                    else defaults["trades"]
                                                )
                                                ohlcv_df_new["price_spread"] = (ohlcv_df_new["high"] - ohlcv_df_new["low"]).fillna(defaults["price_spread"])
                                                ohlcv_df_new["returns"] = ohlcv_df_new["close"].pct_change().fillna(defaults["returns"])
                                                ohlcv_df_new["volume_change"] = ohlcv_df_new["volume"].diff().fillna(defaults["volume_change"])
                                                ohlcv_df_new["trade_intensity"] = (ohlcv_df_new["trades"] / ohlcv_df_new["volume"].replace(0, 1)).fillna(defaults["trade_intensity"])
                                                nan_counts = {f: ohlcv_df_new[f].isna().sum() for f in required_features}
                                                if any(nan_counts.values()):
                                                    logger.warning("NaN values in ohlcv_df_new: %s", nan_counts)
                                                    for feature in required_features:
                                                        ohlcv_df_new[feature] = ohlcv_df_new[feature].fillna(defaults.get(feature, 0.0))
                                                if not ohlcv_df.empty:
                                                    ohlcv_df = pd.concat(
                                                        [ohlcv_df, ohlcv_df_new],
                                                        ignore_index=True,
                                                    )
                                                    ohlcv_df = (
                                                        ohlcv_df.drop_duplicates(subset=["timestamp"], keep="last")
                                                        .sort_values("timestamp")
                                                        .tail(2 * config.LOOKBACK)
                                                    )
                                                else:
                                                    ohlcv_df = ohlcv_df_new.tail(2 * config.LOOKBACK)
                                                recent_data = ohlcv_df[sklearn_features].tail(2 * config.LOOKBACK).copy()
                                                recent_trade_data = ohlcv_df[
                                                    [
                                                        "timestamp", "close", "volume", "trades", "rsi", "ema",
                                                        "volatility", "macd", "macd_signal", "bollinger_upper",
                                                        "bollinger_lower", "momentum", "volume_trend", "high",
                                                        "low", "atr", "vwap", "predicted_price", "grid_level",
                                                        "price_spread", "returns", "volume_change", "trade_intensity"
                                                    ]
                                                ].tail(2 * config.LOOKBACK).copy()
                                                logger.info("Updated data with OHLCV: ohlcv_df_rows=%d, recent_data_rows=%d, recent_trade_data_rows=%d",
                                                            len(ohlcv_df), len(recent_data), len(recent_trade_data))
                                            else:
                                                logger.warning("No OHLCV data fetched, proceeding with existing ohlcv_df")
                                            time.sleep(exchange.rateLimit / 1000)
                                        except Exception as ohlcv_error:
                                            logger.error("Error fetching/updating OHLCV data: %s", str(ohlcv_error))
                                            time.sleep(exchange.rateLimit / 1000)
                                finally:
                                    if hasattr(_update_live_data_from_websocket, "lock") and _update_live_data_from_websocket.lock.locked():
                                        logger.info("Releasing WebSocket update lock")
                                        _update_live_data_from_websocket.lock.release()

                            # Verify data after preparation
                            logger.info("Verifying data for retraining: recent_data_rows=%d, recent_trade_data_rows=%d, needed=61",
                                        len(recent_data), len(recent_trade_data))
                            if len(recent_data) < 61 or len(recent_trade_data) < 61:
                                logger.warning("Insufficient data after preparation: recent_data=%d, recent_trade_data=%d",
                                            len(recent_data), len(recent_trade_data))
                                last_retrain_check = current_time
                                return

                            logger.info("Starting ML model retraining: data_rows=%d, lookback=%d",
                                        len(recent_data), config.LOOKBACK // 100)
                            try:
                                start_time = time.time()
                                sklearn_model_new, sklearn_scaler_new = None, None
                                pytorch_model_new, pytorch_scaler_new = None, None
                                xgb_model_new, xgb_scaler_new = None, None
                                meta_model_new, meta_scaler_new = None, None

                                # Prepare features for MSE computation
                                features = [
                                    "close", "volume", "trades", "rsi", "ema", "volatility", "macd", "macd_signal",
                                    "bollinger_upper", "bollinger_lower", "momentum", "volume_trend", "atr", "vwap",
                                    "price_spread", "returns", "volume_change", "trade_intensity"
                                ]
                                lookback = config.LOOKBACK // 100
                                pytorch_lookback = min(lookback, 32)  # Smaller lookback for PyTorch
                                logger.info("Prepared features for retraining: feature_count=%d, lookback=%d, pytorch_lookback=%d",
                                            len(features), lookback, pytorch_lookback)

                                # Load or initialize models and scalers
                                logger.info("Loading Sklearn model and scaler")
                                try:
                                    if (
                                        bot_state.get("sklearn_model")
                                        and os.path.exists("client_sklearn_model.pkl")
                                        and os.path.exists("client_sklearn_scaler.pkl")
                                    ):
                                        sklearn_model_new = joblib.load("client_sklearn_model.pkl")
                                        sklearn_scaler_new = joblib.load("client_sklearn_scaler.pkl")
                                        logger.info("Loaded saved Sklearn model and scaler")
                                    else:
                                        logger.info("No saved Sklearn model/scaler found")
                                except Exception as e:
                                    logger.error("Failed to load Sklearn model/scaler: %s", str(e))
                                    sklearn_model_new, sklearn_scaler_new = None, None

                                logger.info("Training Sklearn RF and SGD models")
                                try:
                                    # Load scalers if available
                                    if os.path.exists("client_sklearn_rf_scaler.pkl"):
                                        bot_state["scaler_sklearn_rf"] = joblib.load("client_sklearn_rf_scaler.pkl")
                                        logger.info("Loaded RF scaler from client_sklearn_rf_scaler.pkl")
                                    else:
                                        bot_state["scaler_sklearn_rf"] = StandardScaler()
                                        logger.info("Initialized new RF scaler")
                                    if os.path.exists("client_sklearn_sgd_scaler.pkl"):
                                        bot_state["scaler_sklearn_sgd"] = joblib.load("client_sklearn_sgd_scaler.pkl")
                                        logger.info("Loaded SGD scaler from client_sklearn_sgd_scaler.pkl")
                                    else:
                                        bot_state["scaler_sklearn_sgd"] = StandardScaler()
                                        logger.info("Initialized new SGD scaler")

                                    # Train sklearn models
                                    rf_model, rf_scaler, sgd_model, sgd_scaler, sklearn_lookback = train_sklearn_predictor(historical_data, trade_data)
                                    if rf_model is not None and sgd_model is not None:
                                        bot_state["sklearn_rf_model"] = rf_model
                                        bot_state["scaler_sklearn_rf"] = rf_scaler
                                        bot_state["sklearn_sgd_model"] = sgd_model
                                        bot_state["scaler_sklearn_sgd"] = sgd_scaler
                                        bot_state["sklearn_lookback"] = sklearn_lookback
                                        logger.info("Sklearn RF and SGD models trained successfully: lookback=%d", sklearn_lookback)
                                    else:
                                        logger.error("Sklearn model training failed: RF or SGD model is None")
                                except Exception as sklearn_error:
                                    logger.error("Failed to train Sklearn model: %s", str(sklearn_error))

                                logger.info("Loading PyTorch model and scaler")
                                try:
                                    if bot_state.get("pytorch_model"):
                                        bot_state["pytorch_model"] = LSTMPricePredictor(input_size=len(features))
                                        if os.path.exists("client_pytorch_model.pth") and os.path.exists(
                                            "client_pytorch_scaler.pkl"
                                        ):
                                            bot_state["pytorch_model"].load_state_dict(
                                                torch.load("client_pytorch_model.pth")
                                            )
                                            pytorch_scaler_new = joblib.load("client_pytorch_scaler.pkl")
                                            pytorch_model_new = bot_state["pytorch_model"]
                                            logger.info("Loaded saved PyTorch model and scaler")
                                        else:
                                            pytorch_model_new = bot_state["pytorch_model"]
                                            pytorch_scaler_new = None
                                            logger.info("No saved PyTorch model/scaler found")
                                except Exception as e:
                                    logger.error("Failed to load PyTorch model/scaler: %s", str(e))
                                    pytorch_model_new, pytorch_scaler_new = None, None

                                logger.info("Training XGBoost model")
                                try:
                                    # Load XGBoost scaler if available
                                    if os.path.exists("client_xgb_scaler.pkl"):
                                        bot_state["xgb_scaler"] = joblib.load("client_xgb_scaler.pkl")
                                        logger.info("Loaded XGBoost scaler from client_xgb_scaler.pkl")
                                    else:
                                        bot_state["xgb_scaler"] = StandardScaler()
                                        logger.info("Initialized new XGBoost scaler")

                                    # Train XGBoost model
                                    xgb_model, xgb_scaler, xgb_lookback = train_xgboost_predictor(historical_data, trade_data)
                                    if xgb_model is not None:
                                        bot_state["xgb_model"] = xgb_model
                                        bot_state["xgb_scaler"] = xgb_scaler
                                        logger.info("XGBoost model trained successfully: lookback=%d", xgb_lookback)
                                    else:
                                        logger.error("XGBoost model training failed")
                                except Exception as xgb_error:
                                    logger.error("Failed to train XGBoost model: %s", str(xgb_error))

                                logger.info("Loading Meta-Model and scaler")
                                try:
                                    if (
                                        bot_state.get("meta_model")
                                        and os.path.exists("client_meta_model.pkl")
                                        and os.path.exists("client_meta_scaler.pkl")
                                    ):
                                        meta_model_new = joblib.load("client_meta_model.pkl")
                                        meta_scaler_new = joblib.load("client_meta_scaler.pkl")
                                        logger.info("Loaded saved Meta-Model and scaler")
                                    else:
                                        logger.info("No saved Meta-Model/scaler found")
                                except Exception as e:
                                    logger.error("Failed to load Meta-Model/scaler: %s", str(e))
                                    meta_model_new, meta_scaler_new = None, None

                                if bot_state.get("sklearn_model") and (
                                    sklearn_model_new is None or sklearn_scaler_new is None
                                ):
                                    logger.info("Retraining Sklearn model: recent_data_rows=%d", len(recent_data))
                                    try:
                                        (
                                            sklearn_model_new,
                                            sklearn_scaler_new,
                                            sklearn_lookback,
                                        ) = train_sklearn_predictor(
                                            recent_data,
                                            recent_trade_data,
                                            lookback=lookback,
                                        )
                                        if sklearn_model_new and sklearn_scaler_new:
                                            scaled_features = sklearn_scaler_new.transform(recent_data[features])
                                            X, y = [], []
                                            for i in range(lookback, len(scaled_features)):
                                                X.append(scaled_features[i - lookback : i].flatten())
                                                y.append(scaled_features[i, features.index("close")])
                                            X = np.array(X)
                                            y = np.array(y)
                                            train_size = int(len(X) * 0.8)
                                            if train_size >= 5 and len(X) - train_size >= 2:
                                                X_train, X_test = X[:train_size], X[train_size:]
                                                y_train, y_test = y[:train_size], y[train_size:]
                                                y_train_pred = sklearn_model_new.predict(X_train)
                                                y_test_pred = sklearn_model_new.predict(X_test)
                                                train_mse = mean_squared_error(y_train, y_train_pred)
                                                test_mse = mean_squared_error(y_test, y_test_pred)
                                                sklearn_model_new.mse_train = train_mse
                                                sklearn_model_new.mse_test = test_mse
                                                logger.info("Sklearn model retrained: train_samples=%d, test_samples=%d, train_mse=%.6f, test_mse=%.6f",
                                                            len(X_train), len(X_test), train_mse, test_mse)
                                            else:
                                                train_mse = "N/A"
                                                test_mse = "N/A"
                                                logger.warning("Insufficient Sklearn split: train=%d, test=%d", train_size, len(X) - train_size)
                                            joblib.dump(sklearn_scaler_new, "client_sklearn_scaler.pkl")
                                            joblib.dump(sklearn_model_new, "client_sklearn_model.pkl")
                                            logger.info("Saved Sklearn scaler and model to client_sklearn_scaler.pkl and client_sklearn_model.pkl")
                                        else:
                                            logger.warning("Sklearn retraining returned None")
                                    except Exception as sklearn_error:
                                        logger.error("Sklearn retraining failed: %s", str(sklearn_error))

                                if bot_state.get("pytorch_model") and (
                                    pytorch_model_new is None or pytorch_scaler_new is None or pytorch_target_scaler_new is None
                                ):
                                    logger.info("Retraining PyTorch model: recent_trade_data_rows=%d, lookback=%d",
                                                len(recent_trade_data), pytorch_lookback)
                                    try:
                                        if len(recent_trade_data) < 10:
                                            raise ValueError(
                                                "Insufficient data for PyTorch: rows=%d, need=10" % len(recent_trade_data)
                                            )
                                        num_epochs = getattr(config, "MIN_PYTORCH_NUM_EPOCHS", 200)
                                        if not isinstance(num_epochs, int) or num_epochs <= 0:
                                            logger.warning("Invalid MIN_PYTORCH_NUM_EPOCHS=%s, using default=200", num_epochs)
                                            num_epochs = 200
                                        logger.info("Training PyTorch model: epochs=%d", num_epochs)
                                        (
                                            pytorch_model_new,
                                            pytorch_scaler_new,
                                            pytorch_target_scaler_new
                                        ) = train_pytorch_predictor(recent_trade_data, recent_data, lookback_minutes=pytorch_lookback)
                                        if not (pytorch_model_new and pytorch_scaler_new and pytorch_target_scaler_new):
                                            logger.warning("PyTorch retraining returned None, falling back to initial training function")
                                            pytorch_model_new, pytorch_scaler_new, pytorch_target_scaler_new = train_pytorch_predictor(historical_data, historical_data, lookback_minutes=pytorch_lookback, num_epochs=num_epochs)
                                            if pytorch_model_new and pytorch_scaler_new and pytorch_target_scaler_new:
                                                logger.info("Fallback initial PyTorch training succeeded after retrain failure")
                                            else:
                                                logger.error("Fallback initial PyTorch training also failed")
                                        if pytorch_model_new and pytorch_scaler_new and pytorch_target_scaler_new:
                                            scaled_features = pytorch_scaler_new.transform(recent_trade_data[features])
                                            X, y = [], []
                                            for i in range(pytorch_lookback, len(scaled_features)):
                                                X.append(scaled_features[i - pytorch_lookback : i])
                                                y.append(scaled_features[i, features.index("close")])
                                            X = np.array(X)
                                            y = np.array(y)
                                            train_size = int(len(X) * 0.8)
                                            if train_size >= 8 and len(X) - train_size >= 2:
                                                X_train, X_test = X[:train_size], X[train_size:]
                                                y_train, y_test = y[:train_size], y[train_size:]
                                                X_train = torch.FloatTensor(X_train).to(config.DEVICE)
                                                y_train = torch.FloatTensor(y_train).view(-1, 1).to(config.DEVICE)
                                                X_test = torch.FloatTensor(X_test).to(config.DEVICE)
                                                y_test = torch.FloatTensor(y_test).view(-1, 1).to(config.DEVICE)
                                                pytorch_model_new.eval()
                                                with torch.no_grad():
                                                    train_outputs = pytorch_model_new(X_train)
                                                    test_outputs = pytorch_model_new(X_test)
                                                    criterion = nn.MSELoss()
                                                    train_loss = criterion(train_outputs, y_train).item()
                                                    test_loss = criterion(test_outputs, y_test).item()
                                                pytorch_model_new.train_loss = train_loss
                                                pytorch_model_new.test_loss = test_loss
                                                logger.info("PyTorch model retrained: train_samples=%d, test_samples=%d, train_loss=%.6f, test_loss=%.6f",
                                                            len(X_train), len(X_test), train_loss, test_loss)
                                            else:
                                                logger.warning("Insufficient PyTorch split: train=%d, test=%d", train_size, len(X) - train_size)
                                                train_loss = "N/A"
                                                test_loss = "N/A"
                                            joblib.dump(pytorch_scaler_new, "client_pytorch_scaler.pkl")
                                            joblib.dump(pytorch_target_scaler_new, "client_pytorch_target_scaler.pkl")
                                            torch.save(pytorch_model_new.state_dict(), "client_pytorch_model.pth")
                                            logger.info("Saved PyTorch scaler, target scaler, and model to client_pytorch_scaler.pkl, client_pytorch_target_scaler.pkl, client_pytorch_model.pth")
                                        else:
                                            logger.error("PyTorch retraining and fallback both failed; scalers/models not updated")
                                    except Exception as pytorch_error:
                                        logger.error("PyTorch retraining failed: %s", str(pytorch_error))

                                logger.info("Optimizing config parameters")
                                try:
                                    current_params = {
                                        "ML_TREND_WEIGHT": config.ML_TREND_WEIGHT,
                                        "ML_CONFIDENCE_THRESHOLD": config.ML_CONFIDENCE_THRESHOLD,
                                    }
                                    bounds = {
                                        "ML_TREND_WEIGHT": (0.3, 1.5),
                                        "ML_CONFIDENCE_THRESHOLD": (0.5, 0.95),
                                    }
                                    ensemble_pred = (
                                        prediction_history[["sklearn_pred", "pytorch_pred", "xgb_pred"]].mean(axis=1).values
                                    )
                                    y_meta = prediction_history["actual_price"].values
                                    new_params = optimize_config_parameters(
                                        ensemble_pred, y_meta, current_params, bounds
                                    )
                                    config.ML_TREND_WEIGHT = new_params["ML_TREND_WEIGHT"]
                                    config.ML_CONFIDENCE_THRESHOLD = new_params["ML_CONFIDENCE_THRESHOLD"]
                                    logger.info("Updated config parameters: ML_TREND_WEIGHT=%.2f, ML_CONFIDENCE_THRESHOLD=%.2f",
                                                config.ML_TREND_WEIGHT, config.ML_CONFIDENCE_THRESHOLD)
                                except Exception as opt_error:
                                    logger.error("Parameter optimization failed: %s, retaining current parameters", str(opt_error))

                                logger.info("Updating prediction history: current_rows=%d", len(prediction_history))
                                try:
                                    new_entry = pd.DataFrame(
                                        {
                                            "sklearn_pred": [last_sklearn_prediction or current_price],
                                            "pytorch_pred": [last_pytorch_prediction or current_price],
                                            "xgb_pred": [last_xgb_prediction or current_price],
                                            "current_price": [current_price],
                                            "volatility": [
                                                recent_trade_data["volatility"].iloc[-1]
                                                if "volatility" in recent_trade_data
                                                and not pd.isna(recent_trade_data["volatility"].iloc[-1])
                                                else defaults["volatility"]
                                            ],
                                            "actual_price": [
                                                recent_trade_data["predicted_price"].iloc[-1]
                                                if "predicted_price" in recent_trade_data
                                                and not pd.isna(recent_trade_data["predicted_price"].iloc[-1])
                                                else current_price
                                            ],
                                        }
                                    )
                                    prediction_history = pd.concat([prediction_history, new_entry], ignore_index=True).tail(
                                        1000
                                    )
                                    bot_state["prediction_history"] = prediction_history
                                    logger.info("Updated prediction_history: new_rows=%d, last_price=%.2f",
                                                len(prediction_history), prediction_history["current_price"].iloc[-1])
                                except Exception as history_error:
                                    logger.error("Failed to update prediction history: %s", str(history_error))

                                logger.info("Updating bot_state with retrained models")
                                try:
                                    if sklearn_model_new or pytorch_model_new or xgb_model_new or meta_model_new:
                                        if sklearn_model_new:
                                            bot_state["sklearn_model"] = sklearn_model_new
                                            bot_state["sklearn_scaler"] = sklearn_scaler_new
                                            logger.info("Updated bot_state with Sklearn model")
                                        if pytorch_model_new:
                                            bot_state["pytorch_model"] = pytorch_model_new
                                            bot_state["pytorch_scaler"] = pytorch_scaler_new
                                            logger.info("Updated bot_state with PyTorch model")
                                        if xgb_model_new:
                                            bot_state["xgb_model"] = xgb_model_new
                                            bot_state["xgb_scaler"] = xgb_scaler_new
                                            logger.info("Updated bot_state with XGBoost model")
                                        if meta_model_new:
                                            bot_state["meta_model"] = meta_model_new
                                            bot_state["meta_scaler"] = meta_scaler_new
                                            logger.info("Updated bot_state with Meta-Model")
                                        last_retrain_time = current_time
                                        lookback = max(
                                            sklearn_lookback or 0,
                                            pytorch_lookback or 0,
                                            xgb_lookback or 0,
                                        )
                                        logger.info("Retraining completed successfully: duration=%.2fs, final_lookback=%d",
                                                    time.time() - start_time, lookback)
                                    else:
                                        logger.error("Retraining failed: no models could be trained")
                                except Exception as state_error:
                                    logger.error("Failed to update bot_state: %s", str(state_error))
                            except Exception as retrain_error:
                                logger.error("Error during ML retraining: %s, continuing with existing models", str(retrain_error))
                            last_retrain_check = current_time
                        else:
                            logger.info("Skipping retraining: time_since_last_retrain=%.2fs, recent_data_rows=%d, min_data_rows=%d",
                                        time_since_last_retrain, len(recent_data), min_data_rows)
                except Exception as check_error:
                    logger.error("Error checking retraining conditions: %s", str(check_error))
                    last_retrain_check = current_time

                # 5. Check Orders
                # --- Ensure volatility_val and price_val are always defined ---
                volatility_val = None
                price_val = None
                # Try to extract from recent data if available
                try:
                    if 'recent_trade_data' in locals() and not recent_trade_data.empty:
                        if 'volatility' in recent_trade_data.columns:
                            volatility_val = recent_trade_data['volatility'].iloc[-1]
                        if 'close' in recent_trade_data.columns:
                            price_val = recent_trade_data['close'].iloc[-1]
                except Exception as e:
                    logger.warning(f"[PARAM RECOMMEND] Could not extract volatility_val or price_val: {e}")

                # --- Call dynamic parameter recommendation at the start of Section #5 ---
                meta_conf = None
                # Try classifier-style confidence first
                if 'meta_model' in bot_state and hasattr(bot_state['meta_model'], 'predict_proba') and len(recent_data) > 0:
                    try:
                        meta_conf = float(bot_state['meta_model'].predict_proba([recent_data.iloc[-1][[f for f in recent_data.columns if f != 'timestamp']]])[0][1])
                        logger.info(f"[PARAM RECOMMEND] meta_conf from meta_model.predict_proba: {meta_conf}")
                    except Exception as e:
                        logger.warning(f"[PARAM RECOMMEND] Could not compute meta-model confidence (predict_proba): {e}")
                # Fallback: use regression confidence (inverse error or prediction variance)
                if meta_conf is None:
                    try:
                        # Use recent prediction errors if available
                        if 'prediction_history' in bot_state and len(bot_state['prediction_history']) > 5:
                            preds = bot_state['prediction_history']['meta_pred'].tail(20) if 'meta_pred' in bot_state['prediction_history'] else None
                            actuals = bot_state['prediction_history']['actual_price'].tail(20)
                            if preds is not None and len(preds) == len(actuals):
                                errors = abs(preds.values - actuals.values) / (actuals.values + 1e-9)
                                mean_error = float(errors.mean())
                                meta_conf = 1.0 - mean_error
                                logger.info(f"[PARAM RECOMMEND] meta_conf from meta-model inverse error: {meta_conf}")
                        # If not, use prediction variance as a proxy
                        elif 'prediction_history' in bot_state and 'meta_pred' in bot_state['prediction_history']:
                            preds = bot_state['prediction_history']['meta_pred'].tail(20)
                            if len(preds) > 1:
                                pred_var = float(preds.var())
                                meta_conf = 1.0 / (1.0 + pred_var)
                                logger.info(f"[PARAM RECOMMEND] meta_conf from meta-model prediction variance: {meta_conf}")
                    except Exception as e:
                        logger.warning(f"[PARAM RECOMMEND] Could not compute meta-model regression confidence: {e}")
                # Fallback: use PyTorch model confidence if available and more accurate
                if meta_conf is None or (meta_conf is not None and meta_conf < 0.5):
                    try:
                        if 'prediction_history' in bot_state and 'pytorch_pred' in bot_state['prediction_history'] and len(bot_state['prediction_history']) > 5:
                            preds = bot_state['prediction_history']['pytorch_pred'].tail(20)
                            actuals = bot_state['prediction_history']['actual_price'].tail(20)
                            if len(preds) == len(actuals):
                                errors = abs(preds.values - actuals.values) / (actuals.values + 1e-9)
                                mean_error = float(errors.mean())
                                pytorch_conf = 1.0 - mean_error
                                logger.info(f"[PARAM RECOMMEND] pytorch_conf from PyTorch inverse error: {pytorch_conf}")
                                if meta_conf is None or pytorch_conf > meta_conf:
                                    meta_conf = pytorch_conf
                    except Exception as e:
                        logger.warning(f"[PARAM RECOMMEND] Could not compute PyTorch regression confidence: {e}")
                if meta_conf is None:
                    logger.info(f"[PARAM RECOMMEND] meta_conf could not be determined (no classifier, no recent predictions)")
                # Optionally, compute recent accuracy (e.g., last 20 predictions)
                recent_accuracy = None
                if 'prediction_history' in bot_state and len(bot_state['prediction_history']) > 20:
                    try:
                        preds = bot_state['prediction_history']['sklearn_pred'].tail(20)
                        actuals = bot_state['prediction_history']['actual_price'].tail(20)
                        if len(preds) == len(actuals):
                            errors = abs(preds.values - actuals.values) / (actuals.values + 1e-9)
                            recent_accuracy = float((1 - errors).mean())
                    except Exception as e:
                        logger.warning(f"[PARAM RECOMMEND] Could not compute recent accuracy: {e}")
                # Get balances for recommendation (always use latest from bot_state if available)
                if 'get_balances' in globals():
                    eth_balance, usd_balance = get_balances()
                elif 'eth_balance' in bot_state and 'usd_balance' in bot_state:
                    eth_balance = bot_state['eth_balance']
                    usd_balance = bot_state['usd_balance']
                else:
                    eth_balance, usd_balance = 0, 0
                logger.info(f"[PARAM RECOMMEND] Using balances for recommendation: USD={usd_balance:.2f}, ETH={eth_balance:.6f}")
                # Warn if balances are zero but open orders or recent updates show otherwise
                if (usd_balance == 0 or eth_balance == 0) and (len(bot_state.get('buy_orders', [])) > 0 or len(bot_state.get('sell_orders', [])) > 0):
                    logger.warning(f"[PARAM RECOMMEND] WARNING: Zero balances detected but open orders exist. Check balance update logic.")
                recs = recommend_dynamic_params(usd_balance, eth_balance, volatility_val, meta_conf, recent_accuracy, price_val=price_val)

                # --- Dynamically update all actionable parameters (including SHORT_* overrides) ---
                param_map = {
                    "EMA_SPAN": "SHORT_EMA_SPAN",
                    "ATR_PERIOD": "SHORT_ATR_PERIOD",
                    "VOLATILITY_WINDOW": "SHORT_VOLATILITY_WINDOW",
                    "VWAP_PERIOD": "SHORT_VWAP_PERIOD",
                    "BOLLINGER_WINDOW": "SHORT_BOLLINGER_WINDOW",
                    "MACD_FAST": "SHORT_MACD_FAST",
                    "MACD_SLOW": "SHORT_MACD_SLOW",
                    "MACD_SIGNAL": "SHORT_MACD_SIGNAL",
                    "VOLATILITY_THRESHOLD": "LOWER_VOLATILITY_THRESHOLD"
                }
                for param, short_var in param_map.items():
                    if param in recs:
                        globals()[short_var] = recs[param]
                        logger.info(f"[PARAM AUTO-ADJUST] {short_var} set to {recs[param]} (from {param}) for this run (not persistent)")
                # Also update original config values if present
                for param in ["EMA_SPAN", "ATR_PERIOD", "VWAP_PERIOD", "BOLLINGER_WINDOW", "VOLATILITY_THRESHOLD"]:
                    if param in recs and hasattr(config, param):
                        setattr(config, param, recs[param])
                        logger.info(f"[PARAM AUTO-ADJUST] config.{param} set to {recs[param]} for this run (not persistent)")

                # --- Dynamic Feature Order Triggering in Active Markets ---
                # If market is highly active (high volatility or volume), allow more aggressive feature order placement within caps
                market_is_active = False
                active_volatility_threshold = recs.get("VOLATILITY_THRESHOLD", 0.1) * 2
                active_volume_threshold = 2 * (recent_trade_data["volume"].rolling(10).mean().iloc[-1] if "volume" in recent_trade_data and len(recent_trade_data["volume"]) >= 10 else 0)
                current_volume = recent_trade_data["volume"].iloc[-1] if "volume" in recent_trade_data else 0
                if volatility_val is not None and volatility_val > active_volatility_threshold:
                    market_is_active = True
                if current_volume > active_volume_threshold and active_volume_threshold > 0:
                    market_is_active = True
                if market_is_active:
                    logger.info(f"[DYNAMIC TRIGGER] Market is ACTIVE (volatility={volatility_val}, volume={current_volume}), relaxing feature order logic within caps.")
                    # Lower DCA min spacing and allow more frequent feature orders, but never exceed caps
                    DCA_min_spacing = min(DCA_min_spacing, 0.002) if 'DCA_min_spacing' in locals() else 0.002
                    # Optionally, increase soft_feature_order_cap if not at hard cap
                    if 'soft_feature_order_cap' in locals() and 'feature_order_cap' in locals():
                        soft_feature_order_cap = min(feature_order_cap, int(soft_feature_order_cap * 1.5) + 1)
                        logger.info(f"[DYNAMIC TRIGGER] Increased soft_feature_order_cap to {soft_feature_order_cap} due to active market.")
                else:
                    logger.info(f"[DYNAMIC TRIGGER] Market is not highly active (volatility={volatility_val}, volume={current_volume}), using default feature order logic.")

                # --- Simple Risk Management: Gradual, Trend-Based Order Adjustments (3% over 5min) ---
                # Only cancel relevant side's orders, no cancel-all, and log actions. Gradual, 1-min increments, ready for reversals.
                try:
                    trend_window = 5  # 5 minutes (assuming 1min candles)
                    trend_threshold = 0.03  # 3% move up or down
                    if "close" in recent_trade_data and len(recent_trade_data["close"]) > trend_window:
                        closes = recent_trade_data["close"].tail(trend_window)
                        pct_change = (closes.iloc[-1] - closes.iloc[0]) / closes.iloc[0]
                        # Only act if there are open orders and sufficient balance
                        if pct_change > trend_threshold and len(bot_state.get("sell_orders", [])) > 0 and eth_balance > 0:
                            logger.info(f"[RISK MGMT] Uptrend detected (+{pct_change*100:.2f}% over {trend_window}min). Gradually cancelling sell orders to protect ETH.")
                            # Cancel one sell order per minute in uptrend
                            sell_orders = list(bot_state.get("sell_orders", []))
                            if sell_orders:
                                order = sell_orders[0]
                                try:
                                    exchange.cancel_order(order["id"], config.SYMBOL)
                                    logger.info(f"[RISK MGMT] Gradually canceled sell order {order['id']} due to uptrend.")
                                except Exception as e:
                                    logger.warning(f"[RISK MGMT] Failed to cancel sell order {order.get('id', '?')}: {e}")
                        elif pct_change < -trend_threshold and len(bot_state.get("buy_orders", [])) > 0 and usd_balance > 0:
                            logger.info(f"[RISK MGMT] Downtrend detected ({pct_change*100:.2f}% over {trend_window}min). Gradually cancelling buy orders to protect USD.")
                            # Cancel one buy order per minute in downtrend
                            buy_orders = list(bot_state.get("buy_orders", []))
                            if buy_orders:
                                order = buy_orders[0]
                                try:
                                    exchange.cancel_order(order["id"], config.SYMBOL)
                                    logger.info(f"[RISK MGMT] Gradually canceled buy order {order['id']} due to downtrend.")
                                except Exception as e:
                                    logger.warning(f"[RISK MGMT] Failed to cancel buy order {order.get('id', '?')}: {e}")
                        else:
                            logger.info(f"[RISK MGMT] No significant trend or no open orders to adjust. pct_change={pct_change*100:.2f}% over {trend_window}min.")
                except Exception as risk_error:
                    logger.error(f"[RISK MGMT] Error in risk management logic: {risk_error}")

                # === Section #5: Enhanced Feature-Based Order Logic Enhancements ===
                # --- SUMMARY TABLE & ACTIONABLE STEPS ---
                # | Feature    | Trigger Logic                | Threshold/Period | Logging | Would-be Trigger | Actionable Steps |
                # |------------|------------------------------|------------------|---------|------------------|-----------------|
                # | EMA        | Price cross EMA              | Lower period     | Yes     | Yes              | Tune period     |
                # | ATR        | Price move > ATR             | Lower period     | Yes     | Yes              | Tune period     |
                # | Volatility | Vol > threshold              | Lower threshold  | Yes     | Yes              | Tune threshold  |
                # | VWAP       | Price cross VWAP             | Lower period     | Yes     | Yes              | Tune period     |
                # | RSI        | RSI < 30 (buy), > 70 (sell)  | 30/70            | Yes     | Yes              | Tune levels     |
                # | Bollinger  | Price cross bands            | std=2, window=10 | Yes     | Yes              | Tune window     |
                # | MACD       | MACD cross signal            | fast/slow=6/13   | Yes     | Yes              | Tune params     |
                #
                # Actionable Steps:
                # 1. All feature triggers log actual values, reasons for not firing, and would-be triggers.
                # 2. Lowered thresholds/periods for ATR, Volatility, EMA, VWAP (see config overrides below).
                # 3. DCA/order cap logic relaxed: allow more frequent alternation, lower min_spacing.
                # 4. Would-be triggers are logged for parameter tuning/backtesting.
                # 5. All logic is maintainable and parameterized for future tuning.

                # --- Config overrides for shorter periods/lower thresholds ---
                SHORT_EMA_SPAN = 6
                SHORT_ATR_PERIOD = 7
                SHORT_VOLATILITY_WINDOW = 7
                SHORT_VWAP_PERIOD = 7
                SHORT_BOLLINGER_WINDOW = 10
                SHORT_MACD_FAST = 6
                SHORT_MACD_SLOW = 13
                SHORT_MACD_SIGNAL = 5
                LOWER_VOLATILITY_THRESHOLD = 0.12

                # --- Logging all feature triggers and would-be triggers ---
                would_be_triggers = {}
                # EMA
                if 'price_val' in locals() and 'ema_val' in locals() and price_val is not None and ema_val is not None:
                    would_be_triggers['ema_buy'] = price_val > ema_val
                    would_be_triggers['ema_sell'] = price_val < ema_val
                    logger.info(f"[EMA] Would-be BUY: {would_be_triggers['ema_buy']} (price {price_val:.2f} > ema {ema_val:.2f}), Would-be SELL: {would_be_triggers['ema_sell']} (price {price_val:.2f} < ema {ema_val:.2f})")
                # ATR
                if 'price_val' in locals() and 'atr_val' in locals() and price_val is not None and atr_val is not None:
                    last_close = None
                    if "close" in recent_trade_data and len(recent_trade_data["close"]) > 1:
                        last_close = recent_trade_data["close"].iloc[-2]
                    if last_close is not None:
                        would_be_triggers['atr_buy'] = price_val < last_close - atr_val
                        would_be_triggers['atr_sell'] = price_val > last_close + atr_val
                        logger.info(f"[ATR] Would-be BUY: {would_be_triggers['atr_buy']} (price {price_val:.2f} < last_close - atr {last_close - atr_val:.2f}), Would-be SELL: {would_be_triggers['atr_sell']} (price {price_val:.2f} > last_close + atr {last_close + atr_val:.2f})")
                # Volatility
                if 'volatility_val' in locals() and volatility_val is not None:
                    would_be_triggers['volatility_buy'] = volatility_val > LOWER_VOLATILITY_THRESHOLD
                    would_be_triggers['volatility_sell'] = volatility_val < LOWER_VOLATILITY_THRESHOLD
                    logger.info(f"[VOLATILITY] Would-be BUY: {would_be_triggers['volatility_buy']} (vol {volatility_val:.4f} > threshold {LOWER_VOLATILITY_THRESHOLD}), Would-be SELL: {would_be_triggers['volatility_sell']} (vol {volatility_val:.4f} < threshold {LOWER_VOLATILITY_THRESHOLD})")
                # VWAP
                if 'price_val' in locals() and 'vwap_val' in locals() and price_val is not None and vwap_val is not None:
                    would_be_triggers['vwap_buy'] = price_val > vwap_val
                    would_be_triggers['vwap_sell'] = price_val < vwap_val
                    logger.info(f"[VWAP] Would-be BUY: {would_be_triggers['vwap_buy']} (price {price_val:.2f} > vwap {vwap_val:.2f}), Would-be SELL: {would_be_triggers['vwap_sell']} (price {price_val:.2f} < vwap {vwap_val:.2f})")
                # RSI
                if 'rsi_val' in locals() and rsi_val is not None:
                    would_be_triggers['rsi_buy'] = rsi_val < 30
                    would_be_triggers['rsi_sell'] = rsi_val > 70
                    logger.info(f"[RSI] Would-be BUY: {would_be_triggers['rsi_buy']} (rsi {rsi_val:.2f} < 30), Would-be SELL: {would_be_triggers['rsi_sell']} (rsi {rsi_val:.2f} > 70)")
                # Bollinger
                if 'price_val' in locals() and 'lower_val' in locals() and 'upper_val' in locals() and price_val is not None and lower_val is not None and upper_val is not None:
                    would_be_triggers['bollinger_buy'] = price_val <= lower_val
                    would_be_triggers['bollinger_sell'] = price_val >= upper_val
                    logger.info(f"[BOLLINGER] Would-be BUY: {would_be_triggers['bollinger_buy']} (price {price_val:.2f} <= lower {lower_val:.2f}), Would-be SELL: {would_be_triggers['bollinger_sell']} (price {price_val:.2f} >= upper {upper_val:.2f})")
                # MACD
                if 'macd_val' in locals() and 'macd_signal_val' in locals() and macd_val is not None and macd_signal_val is not None:
                    would_be_triggers['macd_buy'] = macd_val > macd_signal_val
                    would_be_triggers['macd_sell'] = macd_val < macd_signal_val
                    logger.info(f"[MACD] Would-be BUY: {would_be_triggers['macd_buy']} (macd {macd_val:.4f} > signal {macd_signal_val:.4f}), Would-be SELL: {would_be_triggers['macd_sell']} (macd {macd_val:.4f} < signal {macd_signal_val:.4f})")

                # --- Relaxed DCA/order cap logic: lower min_spacing for DCA alternation ---
                # (If you want to relax DCA further, set min_spacing in can_place_feature_order to 0.003 or lower)

                # --- END ENHANCEMENTS ---

                # --- Enforce $2 order spacing for buys and sells ---
                # Example: use ATR as dynamic grid spacing if available, else fallback to $2
                enforce_order_spacing(bot_state, min_spacing=2.0, feature_name='atr', recent_trade_data=recent_trade_data)

                try:
                    check_orders_start_time = time.time()
                    logger.info("Starting order checking process")

                    orders_to_send = []
                    closed_order_ids = []
                    current_price = get_current_price()
                    logger.info(f"Retrieved current price: {current_price:.2f}")
                    max_order_range = config.MAX_ORDER_RANGE  # 2500 above/below current price
                    logger.info(f"Max order range set to: {max_order_range:.2f}")

                    for order_list, is_buy in [
                        (bot_state["buy_orders"], True),
                        (bot_state["sell_orders"], False),
                    ]:
                        order_type = "buy" if is_buy else "sell"
                        logger.info(f"Checking {order_type} orders: {len(order_list)} orders in list")

                        # --- Feature-based order scan and logging ---
                        # Get latest feature values
                        rsi_val = recent_trade_data["rsi"].iloc[-1] if "rsi" in recent_trade_data else None
                        price_val = recent_trade_data["close"].iloc[-1] if "close" in recent_trade_data else None
                        lower_val = recent_trade_data["bollinger_lower"].iloc[-1] if "bollinger_lower" in recent_trade_data else None
                        upper_val = recent_trade_data["bollinger_upper"].iloc[-1] if "bollinger_upper" in recent_trade_data else None
                        macd_val = recent_trade_data["macd"].iloc[-1] if "macd" in recent_trade_data else None
                        macd_signal_val = recent_trade_data["macd_signal"].iloc[-1] if "macd_signal" in recent_trade_data else None
                        ema_val = recent_trade_data["ema"].iloc[-1] if "ema" in recent_trade_data else None
                        atr_val = recent_trade_data["atr"].iloc[-1] if "atr" in recent_trade_data else None
                        volatility_val = recent_trade_data["volatility"].iloc[-1] if "volatility" in recent_trade_data else None
                        vwap_val = recent_trade_data["vwap"].iloc[-1] if "vwap" in recent_trade_data else None
                        logger.info(
                            f"Feature values for {order_type} - "
                            f"RSI: {rsi_val if rsi_val is not None else 'N/A'}, "
                            f"Price: {price_val if price_val is not None else 'N/A'}, "
                            f"Bollinger Lower: {lower_val if lower_val is not None else 'N/A'}, "
                            f"Bollinger Upper: {upper_val if upper_val is not None else 'N/A'}, "
                            f"MACD: {macd_val if macd_val is not None else 'N/A'}, "
                            f"MACD Signal: {macd_signal_val if macd_signal_val is not None else 'N/A'}, "
                            f"EMA: {ema_val if ema_val is not None else 'N/A'}, "
                            f"ATR: {atr_val if atr_val is not None else 'N/A'}, "
                            f"Volatility: {volatility_val if volatility_val is not None else 'N/A'}, "
                            f"VWAP: {vwap_val if vwap_val is not None else 'N/A'}"
                        )

                        # --- Automated feature-based order placement (max 8 per feature) ---
                        # --- Dynamic Feature Order Cap Logic ---
                        expanded_feature_list = ["rsi", "bollinger", "macd", "ema", "atr", "volatility", "vwap"]
                        max_total_orders = getattr(config, "MAX_TOTAL_ORDERS", 240)
                        num_base_orders = getattr(config, "NUM_BUY_GRID_LINES", 20) + getattr(config, "NUM_SELL_GRID_LINES", 20)
                        feature_order_cap = getattr(config, "FEATURE_ORDER_CAP", max_total_orders - num_base_orders)
                        soft_feature_order_cap = getattr(config, "SOFT_FEATURE_ORDER_CAP", max(2, feature_order_cap // len(expanded_feature_list)))
                        total_open_orders = len(bot_state['buy_orders']) + len(bot_state['sell_orders'])
                        base_order_count = sum(1 for o in order_list if o.get("feature", "base") == "base")
                        feature_order_counts = {f: sum(1 for o in order_list if isinstance(o, dict) and o.get("feature") == f) for f in expanded_feature_list}
                        total_open_feature_orders = sum(feature_order_counts.values())
                        available_feature_slots = max(0, feature_order_cap - total_open_feature_orders)
                        logger.info(f"[ORDER MGMT] Dynamic feature cap: {feature_order_cap}, soft per-feature cap: {soft_feature_order_cap}")
                        logger.info(f"[ORDER MGMT] {order_type.upper()} base orders: {base_order_count}, feature order counts: {feature_order_counts}")
                        logger.info(f"[ORDER MGMT] Total open orders: {total_open_orders} (Buy: {len(bot_state['buy_orders'])}, Sell: {len(bot_state['sell_orders'])}), available feature slots: {available_feature_slots}")

                        def can_place_feature_order_dynamic(feature):
                            if total_open_feature_orders < feature_order_cap:
                                if feature_order_counts[feature] < soft_feature_order_cap:
                                    return True
                                # Allow exceeding soft cap if slots are available
                                logger.info(f"[DYNAMIC CAP] Feature '{feature}' exceeding soft cap ({soft_feature_order_cap}), but slots available (total open: {total_open_feature_orders}/{feature_order_cap})")
                                return True
                            logger.info(f"[DYNAMIC CAP] Feature '{feature}' cannot place more orders: open={feature_order_counts[feature]}, total open={total_open_feature_orders}, cap={feature_order_cap}")
                            return False

                        # Helper: check if we can place more orders (single robust implementation, always in scope)
                        def can_place_more_orders():
                            total_orders = len(bot_state["buy_orders"]) + len(bot_state["sell_orders"])
                            if total_orders >= max_total_orders:
                                logger.info(f"[ORDER MGMT] Total order cap reached: {total_orders} >= {max_total_orders}")
                                return False
                            # Also check if there are available slots for feature orders
                            if (max_total_orders - total_orders) <= 0:
                                logger.info(f"[ORDER MGMT] No available slots for new feature orders (total_open_orders={total_orders}, max_total_orders={max_total_orders})")
                                return False
                            return True

                        # --- DCA-style feature-based trade logic ---
                        if 'feature_trade_state' not in bot_state:
                            bot_state['feature_trade_state'] = {f: {'last_side': None, 'last_price': None} for f in expanded_feature_list}
                            logger.info("[ORDER MGMT] Initialized feature_trade_state in bot_state for all features")
                        # --- EMA ---
                        ema_val = recent_trade_data["ema"].iloc[-1] if "ema" in recent_trade_data else None
                        if ema_val is not None:
                            if 'get_balances' in globals():
                                eth_balance, usd_balance = get_balances()
                            else:
                                eth_balance, usd_balance = 0, 0
                            position_size = getattr(config, "POSITION_SIZE", 0.0018)
                            # Buy: price crosses above EMA
                            if is_buy and price_val is not None and price_val > ema_val and can_place_feature_order_dynamic("ema") and can_place_more_orders() and usd_balance > price_val * position_size:
                                if can_place_feature_order('ema', 'buy', price_val):
                                    logger.info(f"[EMA] Buy condition MET: price={price_val:.2f} > EMA={ema_val:.2f}")
                                    # Place order (copy MACD logic)
                                    try:
                                        amount = exchange.amount_to_precision(config.SYMBOL, config.POSITION_SIZE)
                                        price = float(exchange.price_to_precision(config.SYMBOL, price_val))
                                        order = exchange.create_limit_buy_order(config.SYMBOL, amount, price, params={"post_only": True})
                                        order = exchange.fetch_order(order["id"])
                                        bot_state["buy_orders"].append({**order, "feature": "ema"})
                                        log_feature_trade(
                                            timestamp=datetime.now(timezone.utc).isoformat(),
                                            feature="ema",
                                            order_type="buy",
                                            order_id=order["id"],
                                            price=price,
                                            size=amount,
                                            status="open",
                                            profit=""
                                        )
                                        logger.info(f"[EMA] Logged feature trade for BUY order: id={order['id']}")
                                    except Exception as e:
                                        logger.error(f"[EMA] Failed to place automated BUY order: {e}")
                                else:
                                    logger.info(f"[EMA] Buy condition MET but DCA logic does NOT allow. Waiting for sell.")
                            # Sell: price crosses below EMA
                            elif not is_buy and price_val is not None and price_val < ema_val and can_place_feature_order_dynamic("ema") and can_place_more_orders() and eth_balance > position_size:
                                if can_place_feature_order('ema', 'sell', price_val):
                                    logger.info(f"[EMA] Sell condition MET: price={price_val:.2f} < EMA={ema_val:.2f}")
                                    try:
                                        amount = exchange.amount_to_precision(config.SYMBOL, config.POSITION_SIZE)
                                        price = float(exchange.price_to_precision(config.SYMBOL, price_val))
                                        order = exchange.create_limit_sell_order(config.SYMBOL, amount, price, params={"post_only": True})
                                        order = exchange.fetch_order(order["id"])
                                        bot_state["sell_orders"].append({**order, "feature": "ema"})
                                        log_feature_trade(
                                            timestamp=datetime.now(timezone.utc).isoformat(),
                                            feature="ema",
                                            order_type="sell",
                                            order_id=order["id"],
                                            price=price,
                                            size=amount,
                                            status="open",
                                            profit=""
                                        )
                                        logger.info(f"[EMA] Logged feature trade for SELL order: id={order['id']}")
                                    except Exception as e:
                                        logger.error(f"[EMA] Failed to place automated SELL order: {e}")
                                else:
                                    logger.info(f"[EMA] Sell condition MET but DCA logic does NOT allow. Waiting for buy.")

                        # --- ATR ---
                        atr_val = recent_trade_data["atr"].iloc[-1] if "atr" in recent_trade_data else None
                        if atr_val is not None:
                            if 'get_balances' in globals():
                                eth_balance, usd_balance = get_balances()
                            else:
                                eth_balance, usd_balance = 0, 0
                            position_size = getattr(config, "POSITION_SIZE", 0.0018)
                            # Buy: price drops by more than ATR from last close
                            last_close = recent_trade_data["close"].iloc[-2] if "close" in recent_trade_data and len(recent_trade_data["close"]) > 1 else None
                            if is_buy and last_close is not None and price_val is not None and price_val < last_close - atr_val and can_place_feature_order_dynamic("atr") and can_place_more_orders() and usd_balance > price_val * position_size:
                                if can_place_feature_order('atr', 'buy', price_val):
                                    logger.info(f"[ATR] Buy condition MET: price={price_val:.2f} < last_close - ATR={last_close - atr_val:.2f}")
                                    try:
                                        amount = exchange.amount_to_precision(config.SYMBOL, config.POSITION_SIZE)
                                        price = float(exchange.price_to_precision(config.SYMBOL, price_val))
                                        order = exchange.create_limit_buy_order(config.SYMBOL, amount, price, params={"post_only": True})
                                        order = exchange.fetch_order(order["id"])
                                        bot_state["buy_orders"].append({**order, "feature": "atr"})
                                        log_feature_trade(
                                            timestamp=datetime.now(timezone.utc).isoformat(),
                                            feature="atr",
                                            order_type="buy",
                                            order_id=order["id"],
                                            price=price,
                                            size=amount,
                                            status="open",
                                            profit=""
                                        )
                                        logger.info(f"[ATR] Logged feature trade for BUY order: id={order['id']}")
                                    except Exception as e:
                                        logger.error(f"[ATR] Failed to place automated BUY order: {e}")
                                else:
                                    logger.info(f"[ATR] Buy condition MET but DCA logic does NOT allow. Waiting for sell.")
                            # Sell: price rises by more than ATR from last close
                            elif not is_buy and last_close is not None and price_val is not None and price_val > last_close + atr_val and can_place_feature_order_dynamic("atr") and can_place_more_orders() and eth_balance > position_size:
                                if can_place_feature_order('atr', 'sell', price_val):
                                    logger.info(f"[ATR] Sell condition MET: price={price_val:.2f} > last_close + ATR={last_close + atr_val:.2f}")
                                    try:
                                        amount = exchange.amount_to_precision(config.SYMBOL, config.POSITION_SIZE)
                                        price = float(exchange.price_to_precision(config.SYMBOL, price_val))
                                        order = exchange.create_limit_sell_order(config.SYMBOL, amount, price, params={"post_only": True})
                                        order = exchange.fetch_order(order["id"])
                                        bot_state["sell_orders"].append({**order, "feature": "atr"})
                                        log_feature_trade(
                                            timestamp=datetime.now(timezone.utc).isoformat(),
                                            feature="atr",
                                            order_type="sell",
                                            order_id=order["id"],
                                            price=price,
                                            size=amount,
                                            status="open",
                                            profit=""
                                        )
                                        logger.info(f"[ATR] Logged feature trade for SELL order: id={order['id']}")
                                    except Exception as e:
                                        logger.error(f"[ATR] Failed to place automated SELL order: {e}")
                                else:
                                    logger.info(f"[ATR] Sell condition MET but DCA logic does NOT allow. Waiting for buy.")

                        # --- Volatility ---
                        volatility_val = recent_trade_data["volatility"].iloc[-1] if "volatility" in recent_trade_data else None
                        if volatility_val is not None:
                            if 'get_balances' in globals():
                                eth_balance, usd_balance = get_balances()
                            else:
                                eth_balance, usd_balance = 0, 0
                            position_size = getattr(config, "POSITION_SIZE", 0.0018)
                            # Buy: volatility spikes above threshold
                            volatility_threshold = getattr(config, "VOLATILITY_THRESHOLD", 0.2)
                            if is_buy and volatility_val > volatility_threshold and can_place_feature_order_dynamic("volatility") and can_place_more_orders() and usd_balance > price_val * position_size:
                                if can_place_feature_order('volatility', 'buy', price_val):
                                    logger.info(f"[VOLATILITY] Buy condition MET: volatility={volatility_val:.4f} > threshold={volatility_threshold:.4f}")
                                    try:
                                        amount = exchange.amount_to_precision(config.SYMBOL, config.POSITION_SIZE)
                                        price = float(exchange.price_to_precision(config.SYMBOL, price_val))
                                        order = exchange.create_limit_buy_order(config.SYMBOL, amount, price, params={"post_only": True})
                                        order = exchange.fetch_order(order["id"])
                                        bot_state["buy_orders"].append({**order, "feature": "volatility"})
                                        log_feature_trade(
                                            timestamp=datetime.now(timezone.utc).isoformat(),
                                            feature="volatility",
                                            order_type="buy",
                                            order_id=order["id"],
                                            price=price,
                                            size=amount,
                                            status="open",
                                            profit=""
                                        )
                                        logger.info(f"[VOLATILITY] Logged feature trade for BUY order: id={order['id']}")
                                    except Exception as e:
                                        logger.error(f"[VOLATILITY] Failed to place automated BUY order: {e}")
                                else:
                                    logger.info(f"[VOLATILITY] Buy condition MET but DCA logic does NOT allow. Waiting for sell.")
                            # Sell: volatility drops below threshold
                            elif not is_buy and volatility_val < volatility_threshold and can_place_feature_order_dynamic("volatility") and can_place_more_orders() and eth_balance > position_size:
                                if can_place_feature_order('volatility', 'sell', price_val):
                                    logger.info(f"[VOLATILITY] Sell condition MET: volatility={volatility_val:.4f} < threshold={volatility_threshold:.4f}")
                                    try:
                                        amount = exchange.amount_to_precision(config.SYMBOL, config.POSITION_SIZE)
                                        price = float(exchange.price_to_precision(config.SYMBOL, price_val))
                                        order = exchange.create_limit_sell_order(config.SYMBOL, amount, price, params={"post_only": True})
                                        order = exchange.fetch_order(order["id"])
                                        bot_state["sell_orders"].append({**order, "feature": "volatility"})
                                        log_feature_trade(
                                            timestamp=datetime.now(timezone.utc).isoformat(),
                                            feature="volatility",
                                            order_type="sell",
                                            order_id=order["id"],
                                            price=price,
                                            size=amount,
                                            status="open",
                                            profit=""
                                        )
                                        logger.info(f"[VOLATILITY] Logged feature trade for SELL order: id={order['id']}")
                                    except Exception as e:
                                        logger.error(f"[VOLATILITY] Failed to place automated SELL order: {e}")
                                else:
                                    logger.info(f"[VOLATILITY] Sell condition MET but DCA logic does NOT allow. Waiting for buy.")

                        def can_place_feature_order(feature, side, current_price):
                            state = bot_state['feature_trade_state'][feature]
                            # Add patience: require more spacing for buys/sells
                            min_spacing = 0.005  # 0.5% default, can be tuned
                            if state['last_side'] is None:
                                if side == 'buy':
                                    logger.info(f"[DCA {feature.upper()}] No previous trade, allowing buy order")
                                    return True
                                else:
                                    logger.info(f"[DCA {feature.upper()}] No previous trade, sell order blocked (requires buy first)")
                                    return False
                            if side == 'sell' and state['last_side'] == 'buy' and state['last_price']:
                                can_sell = current_price >= state['last_price'] * (1 + min_spacing)
                                logger.info(f"[DCA {feature.upper()}] Sell check: current_price={current_price:.2f}, "
                                            f"last_buy_price={state['last_price']:.2f}, threshold={state['last_price'] * (1 + min_spacing):.2f}, "
                                            f"can_sell={can_sell}")
                                return can_sell
                            if side == 'buy' and state['last_side'] == 'sell' and state['last_price']:
                                can_buy = current_price <= state['last_price'] * (1 - min_spacing)
                                logger.info(f"[DCA {feature.upper()}] Buy check: current_price={current_price:.2f}, "
                                            f"last_sell_price={state['last_price']:.2f}, threshold={state['last_price'] * (1 - min_spacing):.2f}, "
                                            f"can_buy={can_buy}")
                                return can_buy
                            logger.info(f"[DCA {feature.upper()}] Cannot place {side} order: last_side={state['last_side']}, "
                                        f"last_price={state['last_price']}")
                            return False

                        # --- VWAP ---
                        vwap_val = recent_trade_data["vwap"].iloc[-1] if "vwap" in recent_trade_data else None
                        if vwap_val is not None:
                            if 'get_balances' in globals():
                                eth_balance, usd_balance = get_balances()
                            else:
                                eth_balance, usd_balance = 0, 0
                            position_size = getattr(config, "POSITION_SIZE", 0.0018)
                            # Buy: price crosses above VWAP
                            if is_buy and price_val is not None and price_val > vwap_val and can_place_feature_order_dynamic("vwap") and can_place_more_orders() and usd_balance > price_val * position_size:
                                if can_place_feature_order('vwap', 'buy', price_val):
                                    logger.info(f"[VWAP] Buy condition MET: price={price_val:.2f} > VWAP={vwap_val:.2f}")
                                    try:
                                        amount = exchange.amount_to_precision(config.SYMBOL, config.POSITION_SIZE)
                                        price = float(exchange.price_to_precision(config.SYMBOL, price_val))
                                        order = exchange.create_limit_buy_order(config.SYMBOL, amount, price, params={"post_only": True})
                                        order = exchange.fetch_order(order["id"])
                                        bot_state["buy_orders"].append({**order, "feature": "vwap"})
                                        log_feature_trade(
                                            timestamp=datetime.now(timezone.utc).isoformat(),
                                            feature="vwap",
                                            order_type="buy",
                                            order_id=order["id"],
                                            price=price,
                                            size=amount,
                                            status="open",
                                            profit=""
                                        )
                                        logger.info(f"[VWAP] Logged feature trade for BUY order: id={order['id']}")
                                    except Exception as e:
                                        logger.error(f"[VWAP] Failed to place automated BUY order: {e}")
                                else:
                                    logger.info(f"[VWAP] Buy condition MET but DCA logic does NOT allow. Waiting for sell.")
                            # Sell: price crosses below VWAP
                            elif not is_buy and price_val is not None and price_val < vwap_val and can_place_feature_order_dynamic("vwap") and can_place_more_orders() and eth_balance > position_size:
                                if can_place_feature_order('vwap', 'sell', price_val):
                                    logger.info(f"[VWAP] Sell condition MET: price={price_val:.2f} < VWAP={vwap_val:.2f}")
                                    try:
                                        amount = exchange.amount_to_precision(config.SYMBOL, config.POSITION_SIZE)
                                        price = float(exchange.price_to_precision(config.SYMBOL, price_val))
                                        order = exchange.create_limit_sell_order(config.SYMBOL, amount, price, params={"post_only": True})
                                        order = exchange.fetch_order(order["id"])
                                        bot_state["sell_orders"].append({**order, "feature": "vwap"})
                                        log_feature_trade(
                                            timestamp=datetime.now(timezone.utc).isoformat(),
                                            feature="vwap",
                                            order_type="sell",
                                            order_id=order["id"],
                                            price=price,
                                            size=amount,
                                            status="open",
                                            profit=""
                                        )
                                        logger.info(f"[VWAP] Logged feature trade for SELL order: id={order['id']}")
                                    except Exception as e:
                                        logger.error(f"[VWAP] Failed to place automated SELL order: {e}")
                                else:
                                    logger.info(f"[VWAP] Sell condition MET but DCA logic does NOT allow. Waiting for buy.")

                        # Remove any duplicate can_place_more_orders definitions below this point (if present)

                        # Helper: get balances
                        def get_balances():
                            try:
                                eth_balance, usd_balance = sync_balances(exchange)
                                return eth_balance, usd_balance
                            except Exception as e:
                                logger.error(f"[ORDER MGMT] Failed to fetch balances: {e}")
                                return 0, 0

                        # --- RSI ---
                        if rsi_val is not None:
                            eth_balance, usd_balance = get_balances()
                            position_size = getattr(config, "POSITION_SIZE", 0.0018)
                            # Buy: only if enough USD, not at cap, and total order cap not reached
                            if is_buy and rsi_val < 30 and can_place_feature_order_dynamic("rsi") and can_place_more_orders() and usd_balance > price_val * position_size:
                                if can_place_feature_order('rsi', 'buy', price_val):
                                    logger.info(f"[RSI DCA] Buy condition MET and DCA logic allows: RSI={rsi_val:.2f}, open_orders={feature_order_counts['rsi']}")
                                    retry_count = 0
                                    max_retries = 5
                                    min_tick = getattr(exchange, 'markets', {}).get(config.SYMBOL, {}).get('precision', {}).get('price', 0.01)
                                    min_tick = float(min_tick) if min_tick else 0.01
                                    price = float(exchange.price_to_precision(config.SYMBOL, price_val))
                                    while retry_count < max_retries:
                                        try:
                                            orderbook = exchange.fetch_order_book(config.SYMBOL)
                                            best_bid = float(orderbook['bids'][0][0]) if orderbook['bids'] else None
                                            best_ask = float(orderbook['asks'][0][0]) if orderbook['asks'] else None
                                            logger.info(f"[RSI] Order book: best_bid={best_bid if best_bid else 'N/A'}, best_ask={best_ask if best_ask else 'N/A'}")
                                            if best_bid is not None and best_ask is not None:
                                                mid_price = (best_bid + best_ask) / 2
                                                new_price = min(mid_price - min_tick, best_ask - min_tick)
                                                price = float(exchange.price_to_precision(config.SYMBOL, new_price))
                                            if best_ask is not None and price < best_ask:
                                                amount = exchange.amount_to_precision(config.SYMBOL, config.POSITION_SIZE)
                                                logger.info(f"[RSI] Preparing BUY order: price={price:.2f}, amount={amount}")
                                                order = exchange.create_limit_buy_order(config.SYMBOL, amount, price, params={"post_only": True})
                                                order = exchange.fetch_order(order["id"])
                                                bot_state["buy_orders"].append({**order, "feature": "rsi"})
                                                logger.info(f"[RSI] Automated BUY order placed: id={order['id']}, price={price:.2f}, size={amount}, feature=rsi")
                                                if count_open_feature_orders(bot_state, "rsi", "buy") >= 8:
                                                    logger.info(f"[RSI] Skipping BUY: open feature orders at cap (8)")
                                                    break
                                                log_feature_trade(
                                                    timestamp=datetime.now(timezone.utc).isoformat(),
                                                    feature="rsi",
                                                    order_type="buy",
                                                    order_id=order["id"],
                                                    price=price,
                                                    size=amount,
                                                    status="open",
                                                    profit=""
                                                )
                                                logger.info(f"[RSI] Logged feature trade for BUY order: id={order['id']}")
                                                break
                                            else:
                                                logger.warning(f"[RSI] Buy price {price:.2f} not below best ask {best_ask if best_ask else 'N/A'}, retrying...")
                                                time.sleep(5)
                                            retry_count += 1
                                        except Exception as e:
                                            if "INVALID_LIMIT_PRICE_POST_ONLY" in str(e):
                                                retry_count += 1
                                                logger.warning(f"[RSI] Buy order post-only error, retrying with lower price: {price:.2f} (attempt {retry_count}/{max_retries})")
                                                time.sleep(5)
                                            else:
                                                logger.error(f"[RSI] Failed to place automated BUY order: {e}")
                                                break
                                    else:
                                        logger.warning(f"[RSI] Skipped buy: could not place post-only order after {max_retries} retries. Last attempted price: {price:.2f}")
                                else:
                                    logger.info(f"[RSI DCA] Buy condition MET but DCA logic does NOT allow: RSI={rsi_val:.2f}. Waiting for sell.")
                            elif not is_buy and rsi_val > 70 and can_place_feature_order_dynamic("rsi") and can_place_more_orders() and eth_balance > position_size:
                                if can_place_feature_order('rsi', 'sell', price_val):
                                    logger.info(f"[RSI DCA] Sell condition MET and DCA logic allows: RSI={rsi_val:.2f}, open_orders={feature_order_counts['rsi']}")
                                    retry_count = 0
                                    max_retries = 5
                                    min_tick = getattr(exchange, 'markets', {}).get(config.SYMBOL, {}).get('precision', {}).get('price', 0.01)
                                    min_tick = float(min_tick) if min_tick else 0.01
                                    price = float(exchange.price_to_precision(config.SYMBOL, price_val))
                                    while retry_count < max_retries:
                                        try:
                                            orderbook = exchange.fetch_order_book(config.SYMBOL)
                                            best_bid = float(orderbook['bids'][0][0]) if orderbook['bids'] else None
                                            best_ask = float(orderbook['asks'][0][0]) if orderbook['asks'] else None
                                            logger.info(f"[RSI] Order book: best_bid={best_bid if best_bid else 'N/A'}, best_ask={best_ask if best_ask else 'N/A'}")
                                            if best_bid is not None and best_ask is not None:
                                                mid_price = (best_bid + best_ask) / 2
                                                new_price = max(mid_price + min_tick, best_bid + min_tick)
                                                price = float(exchange.price_to_precision(config.SYMBOL, new_price))
                                            if best_bid is not None and price >= best_bid + min_tick:
                                                amount = exchange.amount_to_precision(config.SYMBOL, config.POSITION_SIZE)
                                                logger.info(f"[RSI] Preparing SELL order: price={price:.2f}, amount={amount}")
                                                order = exchange.create_limit_sell_order(config.SYMBOL, amount, price, params={"post_only": True})
                                                order = exchange.fetch_order(order["id"])
                                                bot_state["sell_orders"].append({**order, "feature": "rsi"})
                                                logger.info(f"[RSI] Automated SELL order placed: id={order['id']}, price={price:.2f}, size={amount}, feature=rsi")
                                                if count_open_feature_orders(bot_state, "rsi", "sell") >= 8:
                                                    logger.info(f"[RSI] Skipping SELL: open feature orders at cap (8)")
                                                    break
                                                log_feature_trade(
                                                    timestamp=datetime.now(timezone.utc).isoformat(),
                                                    feature="rsi",
                                                    order_type="sell",
                                                    order_id=order["id"],
                                                    price=price,
                                                    size=amount,
                                                    status="open",
                                                    profit=""
                                                )
                                                logger.info(f"[RSI] Logged feature trade for SELL order: id={order['id']}")
                                                break
                                            else:
                                                logger.warning(f"[RSI] Sell price {price:.2f} not above best bid {best_bid if best_bid else 'N/A'} + min_tick, retrying...")
                                                time.sleep(5)
                                                if retry_count >= 2 and best_bid is not None and best_ask is not None:
                                                    mid_price = (best_bid + best_ask) / 2
                                                    new_price = max(mid_price + min_tick, best_bid + min_tick)
                                                    price = float(exchange.price_to_precision(config.SYMBOL, new_price))
                                                    logger.warning(f"[RSI] Readjusting sell price to mid: {mid_price:.2f}, new price: {price:.2f}")
                                            retry_count += 1
                                        except Exception as e:
                                            if "INVALID_LIMIT_PRICE_POST_ONLY" in str(e):
                                                retry_count += 1
                                                price = float(exchange.price_to_precision(config.SYMBOL, price + min_tick))
                                                logger.warning(f"[RSI] Sell order post-only error, retrying with higher price: {price:.2f} (attempt {retry_count}/{max_retries})")
                                                time.sleep(5)
                                            else:
                                                logger.error(f"[RSI] Failed to place automated SELL order: {e}")
                                                break
                                    else:
                                        logger.warning(f"[RSI] Skipped sell: could not place post-only order after {max_retries} retries. Last attempted price: {price:.2f}")
                                else:
                                    logger.info(f"[RSI DCA] Sell condition MET but DCA logic does NOT allow: RSI={rsi_val:.2f}. Waiting for buy.")

                        # --- Bollinger ---
                        if price_val is not None and lower_val is not None and upper_val is not None:
                            eth_balance, usd_balance = get_balances()
                            position_size = getattr(config, "POSITION_SIZE", 0.0018)
                            if is_buy and price_val <= lower_val and can_place_feature_order_dynamic("bollinger") and can_place_more_orders() and usd_balance > price_val * position_size:
                                if can_place_feature_order('bollinger', 'buy', price_val):
                                    logger.info(f"[BOLLINGER DCA] Buy condition MET and DCA logic allows: Price={price_val:.2f}, Lower={lower_val:.2f}, open_orders={feature_order_counts['bollinger']}")
                                    retry_count = 0
                                    max_retries = 5
                                    min_tick = getattr(exchange, 'markets', {}).get(config.SYMBOL, {}).get('precision', {}).get('price', 0.01)
                                    min_tick = float(min_tick) if min_tick else 0.01
                                    price = float(exchange.price_to_precision(config.SYMBOL, price_val))
                                    while retry_count < max_retries:
                                        try:
                                            orderbook = exchange.fetch_order_book(config.SYMBOL)
                                            best_bid = float(orderbook['bids'][0][0]) if orderbook['bids'] else None
                                            best_ask = float(orderbook['asks'][0][0]) if orderbook['asks'] else None
                                            logger.info(f"[BOLLINGER] Order book: best_bid={best_bid if best_bid else 'N/A'}, best_ask={best_ask if best_ask else 'N/A'}")
                                            if best_bid is not None and best_ask is not None:
                                                mid_price = (best_bid + best_ask) / 2
                                                new_price = min(mid_price - min_tick, best_ask - min_tick)
                                                price = float(exchange.price_to_precision(config.SYMBOL, new_price))
                                            if best_ask is not None and price < best_ask:
                                                amount = exchange.amount_to_precision(config.SYMBOL, config.POSITION_SIZE)
                                                logger.info(f"[BOLLINGER] Preparing BUY order: price={price:.2f}, amount={amount}")
                                                order = exchange.create_limit_buy_order(config.SYMBOL, amount, price, params={"post_only": True})
                                                order = exchange.fetch_order(order["id"])
                                                bot_state["buy_orders"].append({**order, "feature": "bollinger"})
                                                logger.info(f"[BOLLINGER] Automated BUY order placed: id={order['id']}, price={price:.2f}, size={amount}, feature=bollinger")
                                                if count_open_feature_orders(bot_state, "bollinger", "buy") >= 8:
                                                    logger.info(f"[BOLLINGER] Skipping BUY: open feature orders at cap (8)")
                                                    break
                                                log_feature_trade(
                                                    timestamp=datetime.now(timezone.utc).isoformat(),
                                                    feature="bollinger",
                                                    order_type="buy",
                                                    order_id=order["id"],
                                                    price=price,
                                                    size=amount,
                                                    status="open",
                                                    profit=""
                                                )
                                                logger.info(f"[BOLLINGER] Logged feature trade for BUY order: id={order['id']}")
                                                break
                                            else:
                                                logger.warning(f"[BOLLINGER] Buy price {price:.2f} not below best ask {best_ask if best_ask else 'N/A'}, retrying...")
                                                time.sleep(1)
                                                if retry_count >= 2 and best_bid is not None and best_ask is not None:
                                                    mid_price = (best_bid + best_ask) / 2
                                                    new_price = min(mid_price - min_tick, best_ask - min_tick)
                                                    price = float(exchange.price_to_precision(config.SYMBOL, new_price))
                                                    logger.warning(f"[BOLLINGER] Readjusting buy price to mid: {mid_price:.2f}, new price: {price:.2f}")
                                            retry_count += 1
                                        except Exception as e:
                                            if "INVALID_LIMIT_PRICE_POST_ONLY" in str(e):
                                                retry_count += 1
                                                price = float(exchange.price_to_precision(config.SYMBOL, price - min_tick))
                                                logger.warning(f"[BOLLINGER] Buy order post-only error, retrying with lower price: {price:.2f} (attempt {retry_count}/{max_retries})")
                                                time.sleep(1)
                                            else:
                                                logger.error(f"[BOLLINGER] Failed to place automated BUY order: {e}")
                                                break
                                    else:
                                        logger.warning(f"[BOLLINGER] Skipped buy: could not place post-only order after {max_retries} retries. Last attempted price: {price:.2f}")
                                else:
                                    logger.info(f"[BOLLINGER DCA] Buy condition MET but DCA logic does NOT allow: Price={price_val:.2f}. Waiting for sell.")
                            elif not is_buy and price_val >= upper_val and can_place_feature_order_dynamic("bollinger") and can_place_more_orders() and eth_balance > position_size:
                                if can_place_feature_order('bollinger', 'sell', price_val):
                                    logger.info(f"[BOLLINGER DCA] Sell condition MET and DCA logic allows: Price={price_val:.2f}, Upper={upper_val:.2f}, open_orders={feature_order_counts['bollinger']}")
                                    retry_count = 0
                                    max_retries = 5
                                    min_tick = getattr(exchange, 'markets', {}).get(config.SYMBOL, {}).get('precision', {}).get('price', 0.01)
                                    min_tick = float(min_tick) if min_tick else 0.01
                                    price = float(exchange.price_to_precision(config.SYMBOL, price_val))
                                    while retry_count < max_retries:
                                        try:
                                            orderbook = exchange.fetch_order_book(config.SYMBOL)
                                            best_bid = float(orderbook['bids'][0][0]) if orderbook['bids'] else None
                                            best_ask = float(orderbook['asks'][0][0]) if orderbook['asks'] else None
                                            logger.info(f"[BOLLINGER] Order book: best_bid={best_bid if best_bid else 'N/A'}, best_ask={best_ask if best_ask else 'N/A'}")
                                            if best_bid is not None and best_ask is not None:
                                                mid_price = (best_bid + best_ask) / 2
                                                new_price = max(mid_price + min_tick, best_bid + min_tick)
                                                price = float(exchange.price_to_precision(config.SYMBOL, new_price))
                                            if best_bid is not None and price >= best_bid + min_tick:
                                                amount = exchange.amount_to_precision(config.SYMBOL, config.POSITION_SIZE)
                                                logger.info(f"[BOLLINGER] Preparing SELL order: price={price:.2f}, amount={amount}")
                                                order = exchange.create_limit_sell_order(config.SYMBOL, amount, price, params={"post_only": True})
                                                order = exchange.fetch_order(order["id"])
                                                bot_state["sell_orders"].append({**order, "feature": "bollinger"})
                                                logger.info(f"[BOLLINGER] Automated SELL order placed: id={order['id']}, price={price:.2f}, size={amount}, feature=bollinger")
                                                if count_open_feature_orders(bot_state, "bollinger", "sell") >= 8:
                                                    logger.info(f"[BOLLINGER] Skipping SELL: open feature orders at cap (8)")
                                                    break
                                                log_feature_trade(
                                                    timestamp=datetime.now(timezone.utc).isoformat(),
                                                    feature="bollinger",
                                                    order_type="sell",
                                                    order_id=order["id"],
                                                    price=price,
                                                    size=amount,
                                                    status="open",
                                                    profit=""
                                                )
                                                logger.info(f"[BOLLINGER] Logged feature trade for SELL order: id={order['id']}")
                                                break
                                            else:
                                                logger.warning(f"[BOLLINGER] Sell price {price:.2f} not above best bid {best_bid if best_bid else 'N/A'} + min_tick, retrying...")
                                                time.sleep(1)
                                                if retry_count >= 2 and best_bid is not None and best_ask is not None:
                                                    mid_price = (best_bid + best_ask) / 2
                                                    new_price = max(mid_price + min_tick, best_bid + min_tick)
                                                    price = float(exchange.price_to_precision(config.SYMBOL, new_price))
                                                    logger.warning(f"[BOLLINGER] Readjusting sell price to mid: {mid_price:.2f}, new price: {price:.2f}")
                                            retry_count += 1
                                        except Exception as e:
                                            if "INVALID_LIMIT_PRICE_POST_ONLY" in str(e):
                                                retry_count += 1
                                                price = float(exchange.price_to_precision(config.SYMBOL, price + min_tick))
                                                logger.warning(f"[BOLLINGER] Sell order post-only error, retrying with higher price: {price:.2f} (attempt {retry_count}/{max_retries})")
                                                time.sleep(1)
                                            else:
                                                logger.error(f"[BOLLINGER] Failed to place automated SELL order: {e}")
                                                break
                                    else:
                                        logger.warning(f"[BOLLINGER] Skipped sell: could not place post-only order after {max_retries} retries. Last attempted price: {price:.2f}")
                                else:
                                    logger.info(f"[BOLLINGER DCA] Sell condition MET but DCA logic does NOT allow: Price={price_val:.2f}. Waiting for buy.")

                        # --- MACD ---
                        if macd_val is not None and macd_signal_val is not None:
                            eth_balance, usd_balance = get_balances()
                            position_size = getattr(config, "POSITION_SIZE", 0.0018)
                            if is_buy and macd_val > macd_signal_val and can_place_feature_order_dynamic("macd") and can_place_more_orders() and usd_balance > price_val * position_size:
                                if can_place_feature_order('macd', 'buy', price_val):
                                    logger.info(f"[MACD DCA] Buy condition MET and DCA logic allows: MACD={macd_val:.4f}, Signal={macd_signal_val:.4f}, open_orders={feature_order_counts['macd']}")
                                    retry_count = 0
                                    max_retries = 5
                                    min_tick = getattr(exchange, 'markets', {}).get(config.SYMBOL, {}).get('precision', {}).get('price', 0.01)
                                    min_tick = float(min_tick) if min_tick else 0.01
                                    price = float(exchange.price_to_precision(config.SYMBOL, price_val))
                                    while retry_count < max_retries:
                                        try:
                                            orderbook = exchange.fetch_order_book(config.SYMBOL)
                                            best_bid = float(orderbook['bids'][0][0]) if orderbook['bids'] else None
                                            best_ask = float(orderbook['asks'][0][0]) if orderbook['asks'] else None
                                            logger.info(f"[MACD] Order book: best_bid={best_bid if best_bid else 'N/A'}, best_ask={best_ask if best_ask else 'N/A'}")
                                            if best_bid is not None and best_ask is not None:
                                                mid_price = (best_bid + best_ask) / 2
                                                new_price = min(mid_price - min_tick, best_ask - min_tick)
                                                price = float(exchange.price_to_precision(config.SYMBOL, new_price))
                                            if best_ask is not None and price < best_ask:
                                                amount = exchange.amount_to_precision(config.SYMBOL, config.POSITION_SIZE)
                                                logger.info(f"[MACD] Preparing BUY order: price={price:.2f}, amount={amount}")
                                                order = exchange.create_limit_buy_order(config.SYMBOL, amount, price, params={"post_only": True})
                                                order = exchange.fetch_order(order["id"])
                                                bot_state["buy_orders"].append({**order, "feature": "macd"})
                                                logger.info(f"[MACD] Automated BUY order placed: id={order['id']}, price={price:.2f}, size={amount}, feature=macd")
                                                if count_open_feature_orders(bot_state, "macd", "buy") >= 8:
                                                    logger.info(f"[MACD] Skipping BUY: open feature orders at cap (8)")
                                                    break
                                                log_feature_trade(
                                                    timestamp=datetime.now(timezone.utc).isoformat(),
                                                    feature="macd",
                                                    order_type="buy",
                                                    order_id=order["id"],
                                                    price=price,
                                                    size=amount,
                                                    status="open",
                                                    profit=""
                                                )
                                                logger.info(f"[MACD] Logged feature trade for BUY order: id={order['id']}")
                                                break
                                            else:
                                                logger.warning(f"[MACD] Buy price {price:.2f} not below best ask {best_ask if best_ask else 'N/A'}, retrying...")
                                                time.sleep(1)
                                                if retry_count >= 2 and best_bid is not None and best_ask is not None:
                                                    mid_price = (best_bid + best_ask) / 2
                                                    new_price = min(mid_price - min_tick, best_ask - min_tick)
                                                    price = float(exchange.price_to_precision(config.SYMBOL, new_price))
                                                    logger.warning(f"[MACD] Readjusting buy price to mid: {mid_price:.2f}, new price: {price:.2f}")
                                            retry_count += 1
                                        except Exception as e:
                                            if "INVALID_LIMIT_PRICE_POST_ONLY" in str(e):
                                                retry_count += 1
                                                price = float(exchange.price_to_precision(config.SYMBOL, price - min_tick))
                                                logger.warning(f"[MACD] Buy order post-only error, retrying with lower price: {price:.2f} (attempt {retry_count}/{max_retries})")
                                                time.sleep(1)
                                            else:
                                                logger.error(f"[MACD] Failed to place automated BUY order: {e}")
                                                break
                                    else:
                                        logger.warning(f"[MACD] Skipped buy: could not place post-only order after {max_retries} retries. Last attempted price: {price:.2f}")
                                else:
                                    logger.info(f"[MACD DCA] Buy condition MET but DCA logic does NOT allow: MACD={macd_val:.4f}. Waiting for sell.")
                            elif not is_buy and macd_val < macd_signal_val and can_place_feature_order_dynamic("macd") and can_place_more_orders() and eth_balance > position_size:
                                if can_place_feature_order('macd', 'sell', price_val):
                                    logger.info(f"[MACD DCA] Sell condition MET and DCA logic allows: MACD={macd_val:.4f}, Signal={macd_signal_val:.4f}, open_orders={feature_order_counts['macd']}")
                                    retry_count = 0
                                    max_retries = 5
                                    min_tick = getattr(exchange, 'markets', {}).get(config.SYMBOL, {}).get('precision', {}).get('price', 0.01)
                                    min_tick = float(min_tick) if min_tick else 0.01
                                    price = float(exchange.price_to_precision(config.SYMBOL, price_val))
                                    while retry_count < max_retries:
                                        try:
                                            orderbook = exchange.fetch_order_book(config.SYMBOL)
                                            best_bid = float(orderbook['bids'][0][0]) if orderbook['bids'] else None
                                            best_ask = float(orderbook['asks'][0][0]) if orderbook['asks'] else None
                                            logger.info(f"[MACD] Order book: best_bid={best_bid if best_bid else 'N/A'}, best_ask={best_ask if best_ask else 'N/A'}")
                                            if best_bid is not None and best_ask is not None:
                                                mid_price = (best_bid + best_ask) / 2
                                                new_price = max(mid_price + min_tick, best_bid + min_tick)
                                                price = float(exchange.price_to_precision(config.SYMBOL, new_price))
                                            if best_bid is not None and price >= best_bid + min_tick:
                                                amount = exchange.amount_to_precision(config.SYMBOL, config.POSITION_SIZE)
                                                logger.info(f"[MACD] Preparing SELL order: price={price:.2f}, amount={amount}")
                                                order = exchange.create_limit_sell_order(config.SYMBOL, amount, price, params={"post_only": True})
                                                order = exchange.fetch_order(order["id"])
                                                bot_state["sell_orders"].append({**order, "feature": "macd"})
                                                logger.info(f"[MACD] Automated SELL order placed: id={order['id']}, price={price:.2f}, size={amount}, feature=macd")
                                                if count_open_feature_orders(bot_state, "macd", "sell") >= 8:
                                                    logger.info(f"[MACD] Skipping SELL: open feature orders at cap (8)")
                                                    break
                                                log_feature_trade(
                                                    timestamp=datetime.now(timezone.utc).isoformat(),
                                                    feature="macd",
                                                    order_type="sell",
                                                    order_id=order["id"],
                                                    price=price,
                                                    size=amount,
                                                    status="open",
                                                    profit=""
                                                )
                                                logger.info(f"[MACD] Logged feature trade for SELL order: id={order['id']}")
                                                break
                                            else:
                                                logger.warning(f"[MACD] Sell price {price:.2f} not above best bid {best_bid if best_bid else 'N/A'} + min_tick, retrying...")
                                                time.sleep(1)
                                                if retry_count >= 2 and best_bid is not None and best_ask is not None:
                                                    mid_price = (best_bid + best_ask) / 2
                                                    new_price = max(mid_price + min_tick, best_bid + min_tick)
                                                    price = float(exchange.price_to_precision(config.SYMBOL, new_price))
                                                    logger.warning(f"[MACD] Readjusting sell price to mid: {mid_price:.2f}, new price: {price:.2f}")
                                            retry_count += 1
                                        except Exception as e:
                                            if "INVALID_LIMIT_PRICE_POST_ONLY" in str(e):
                                                retry_count += 1
                                                price = float(exchange.price_to_precision(config.SYMBOL, price + min_tick))
                                                logger.warning(f"[MACD] Sell order post-only error, retrying with higher price: {price:.2f} (attempt {retry_count}/{max_retries})")
                                                time.sleep(1)
                                            else:
                                                logger.error(f"[MACD] Failed to place automated SELL order: {e}")
                                                break
                                    else:
                                        logger.warning(f"[MACD] Skipped sell: could not place post-only order after {max_retries} retries. Last attempted price: {price:.2f}")
                                else:
                                    logger.info(f"[MACD DCA] Sell condition MET but DCA logic does NOT allow: MACD={macd_val:.4f}. Waiting for buy.")

                        check_order_type_start_time = time.time()
                        orders_to_remove = []
                        feature_orders_to_remove = []
                        base_orders_to_remove = []
                        for index, order in enumerate(order_list):
                            grid_level = index + 1
                            if not order or "id" not in order or not isinstance(order, dict):
                                logger.warning(f"[ORDER MGMT] Skipping invalid {order_type} order at grid_level {grid_level}: {order}")
                                orders_to_remove.append(order)
                                continue

                            feature_label = order.get("feature", "base")
                            if feature_label == "base":
                                logger.info(f"[BASE ORDER] Processing {order_type} order: id={order['id']}, grid_level={grid_level}")
                            else:
                                logger.info(f"[FEATURE ORDER] Processing {order_type} order: id={order['id']}, grid_level={grid_level}, feature={feature_label}")

                            # Log feature-based order check details
                            if feature_label == "rsi":
                                rsi_val = recent_trade_data["rsi"].iloc[-1] if "rsi" in recent_trade_data else None
                                logger.info(f"[RSI CHECK] Order id={order['id']}, grid_level={grid_level}, RSI={rsi_val if rsi_val is not None else 'N/A'}, Buy if < 30, Sell if > 70")
                            elif feature_label == "bollinger":
                                price_val = recent_trade_data["close"].iloc[-1] if "close" in recent_trade_data else None
                                lower_val = recent_trade_data["bollinger_lower"].iloc[-1] if "bollinger_lower" in recent_trade_data else None
                                upper_val = recent_trade_data["bollinger_upper"].iloc[-1] if "bollinger_upper" in recent_trade_data else None
                                logger.info(f"[BOLLINGER CHECK] Order id={order['id']}, grid_level={grid_level}, Price={price_val if price_val is not None else 'N/A'}, Lower Band={lower_val if lower_val is not None else 'N/A'}, Upper Band={upper_val if upper_val is not None else 'N/A'}")
                            elif feature_label == "macd":
                                macd_val = recent_trade_data["macd"].iloc[-1] if "macd" in recent_trade_data else None
                                macd_signal_val = recent_trade_data["macd_signal"].iloc[-1] if "macd_signal" in recent_trade_data else None
                                logger.info(f"[MACD CHECK] Order id={order['id']}, grid_level={grid_level}, MACD={macd_val if macd_val is not None else 'N/A'}, Signal={macd_signal_val if macd_signal_val is not None else 'N/A'}, Buy if MACD > Signal, Sell if MACD < Signal")
                            else:
                                logger.info(f"[BASE CHECK] Order id={order['id']}, grid_level={grid_level}, price={order.get('price', 'N/A')}")

                            # Profit and order closed logging for all features
                            if order.get("status") == config.CHECK_ORDER_STATUS:
                                profit = None
                                if feature_label in ["rsi", "bollinger", "macd"]:
                                    if order_type == "sell":
                                        buy_price = buy_prices.get(order["id"])
                                        if buy_price is not None:
                                            profit = float(order.get("price", 0)) - float(buy_price)
                                            logger.info(f"[PROFIT] {feature_label.upper()} sell order id={order['id']} closed. Buy price={buy_price:.2f}, Sell price={order.get('price', 0):.2f}, Profit={profit:.2f}")
                                            bot_state['feature_trade_state'][feature_label]['last_side'] = 'sell'
                                            bot_state['feature_trade_state'][feature_label]['last_price'] = float(order.get("price", 0))
                                            logger.info(f"[DCA {feature_label.upper()}] Updated feature_trade_state: last_side=sell, last_price={float(order.get('price', 0)):.2f}")
                                        else:
                                            logger.info(f"[PROFIT] {feature_label.upper()} sell order id={order['id']} closed. Buy price unknown.")
                                    else:
                                        logger.info(f"[ORDER CLOSED] {feature_label.upper()} buy order id={order['id']} closed at price={order.get('price', 0):.2f}")
                                        bot_state['feature_trade_state'][feature_label]['last_side'] = 'buy'
                                        bot_state['feature_trade_state'][feature_label]['last_price'] = float(order.get("price", 0))
                                        logger.info(f"[DCA {feature_label.upper()}] Updated feature_trade_state: last_side=buy, last_price={float(order.get('price', 0)):.2f}")

                            try:
                                order_update = exchange.fetch_order(order["id"], config.SYMBOL)
                                if not order_update or not isinstance(order_update, dict):
                                    logger.error(f"Failed to fetch {order_type} order {order['id']}, feature={feature_label}, marking for removal")
                                    orders_to_remove.append(order)
                                    continue

                                order_price = float(order_update.get("price", order.get("price", current_price)))
                                logger.info(f"{order_type.capitalize()} order status: id={order['id']}, status={order_update['status']}, "
                                            f"price={order_price:.2f}, grid_level={grid_level}, feature={feature_label}")

                                if order_update["status"] != order.get("status"):
                                    logger.info(f"{order_type.capitalize()} order status updated: id={order['id']}, "
                                                f"old_status={order.get('status', 'unknown')}, new_status={order_update['status']}, "
                                                f"price={order_price:.2f}, feature={feature_label}")
                                    order["status"] = order_update["status"]

                                if order_update["status"] == "canceled":
                                    logger.info(f"{order_type.capitalize()} order already canceled: id={order['id']}, price={order_price:.4f}, "
                                                f"grid_level={grid_level}, feature={feature_label}")
                                    orders_to_remove.append(order)
                                    orders_to_send.append(
                                        {
                                            "type": "order",
                                            "id": order["id"],
                                            "status": "canceled",
                                            "side": order_type,
                                            "price": float(order_price),
                                            "timestamp": int(current_time * 1000),
                                        }
                                    )
                                    logger.info(f"Added canceled {order_type} order to WebSocket: id={order['id']}, status=canceled, feature={feature_label}")
                                    continue

                                # Check if order is out of range using current_price
                                price_diff = current_price - order_price if is_buy else order_price - current_price
                                logger.info(f"Out-of-range check: {order_type} order id={order['id']}, price={order_price:.2f}, "
                                            f"current_price={current_price:.2f}, grid_base_price={grid_base_price:.2f}, "
                                            f"price_diff={price_diff:.2f}, max_order_range={max_order_range:.2f}")
                                if price_diff > max_order_range:
                                    try:
                                        cancelled = cancel_orders(exchange, [order], config.SYMBOL)
                                        if cancelled:
                                            logger.info(f"Canceled out-of-range {order_type} order: id={order['id']}, "
                                                        f"price={order_price:.2f}, grid_level={grid_level}, feature={feature_label}, "
                                                        f"max_order_range={max_order_range:.2f}")
                                            orders_to_remove.append(order)
                                            orders_to_send.append(
                                                {
                                                    "type": "order",
                                                    "id": order["id"],
                                                    "status": "canceled",
                                                    "side": order_type,
                                                    "price": float(order_price),
                                                    "timestamp": int(current_time * 1000),
                                                }
                                            )
                                            logger.info(f"Added canceled out-of-range {order_type} order to WebSocket: id={order['id']}, "
                                                        f"status=canceled, feature={feature_label}")
                                        else:
                                            logger.warning(f"Failed to cancel out-of-range {order_type} order: id={order['id']}, "
                                                        f"feature={feature_label}")
                                    except Exception as cancel_error:
                                        logger.error(f"Failed to cancel out-of-range {order_type} order {order['id']}, "
                                                    f"feature={feature_label}: {cancel_error}")
                                    continue

                                # Handle executed orders
                                if order_update["status"] == config.CHECK_ORDER_STATUS:
                                    closed_orders.append(order_update)
                                    closed_order_ids.append(order["id"])
                                    size = float(order_update.get("filled", config.POSITION_SIZE))
                                    logger.info(f"{order_type.capitalize()} order executed: id={order['id']}, price={order_price:.2f}, "
                                                f"size={size:.6f}, grid_level={grid_level}, feature={feature_label}")

                                    if is_buy:
                                        buy_prices[order["id"]] = order_price
                                        eth_balance, _ = sync_balances(exchange)
                                        logger.info(f"Post-buy balance: ETH={eth_balance:.6f}")
                                        sell_size = min(eth_balance, config.POSITION_SIZE)
                                        feature = order.get("feature", "base")
                                        feature_order_id = order.get("feature_order_id") if feature != "base" else None
                                        new_sell_order = place_sell_after_buy(
                                            exchange, order["id"], order_price, sell_size,
                                            feature=feature, feature_order_id=feature_order_id
                                        )
                                        if new_sell_order and isinstance(new_sell_order, dict):
                                            bot_state["sell_orders"].append(new_sell_order)
                                            orders_to_send.append(
                                                {
                                                    "type": "order",
                                                    "id": new_sell_order["id"],
                                                    "status": new_sell_order["status"],
                                                    "side": "sell",
                                                    "price": float(new_sell_order["price"]),
                                                    "timestamp": int(current_time * 1000),
                                                }
                                            )
                                            logger.info(f"Placed new sell order after buy: id={new_sell_order['id']}, "
                                                        f"price={float(new_sell_order['price']):.2f}, size={sell_size:.6f}, "
                                                        f"grid_level={len(bot_state['sell_orders'])}, feature={new_sell_order.get('feature', 'base')}, feature_order_id={new_sell_order.get('feature_order_id')}")
                                        else:
                                            logger.warning(f"Failed to place sell order after buy: buy_order_id={order['id']}, "
                                                        f"attempted_size={sell_size:.6f}, feature={feature_label}")
                                        last_trade_time = current_time
                                        logger.info(f"Updated last_trade_time to {last_trade_time:.0f} after buy execution")
                                    else:
                                        buy_price = buy_prices.pop(order["id"], None)
                                        if buy_price:
                                            log_trade_profit(buy_price, order_price, size, order["id"])
                                            logger.info(f"Logged trade profit for sell order: id={order['id']}, "
                                                        f"buy_price={buy_price:.2f}, sell_price={order_price:.2f}, size={size:.6f}, "
                                                        f"feature={feature_label}")
                                        else:
                                            logger.warning(f"No buy price mapped for sell order: id={order['id']}, "
                                                        f"price={order_price:.2f}, feature={feature_label}")
                                        eth_balance, usd_balance = sync_balances(exchange)
                                        logger.info(f"Post-sell balance: ETH={eth_balance:.6f}, USD={usd_balance:.2f}")
                                        required_usd = size * order_price
                                        if usd_balance < required_usd:
                                            logger.warning(f"Insufficient USD to place buy after sell: available={usd_balance:.2f}, "
                                                        f"required={required_usd:.2f}, feature={feature_label}")
                                        else:
                                            feature = order.get("feature", "base")
                                            feature_order_id = order.get("feature_order_id") if feature != "base" else None
                                            new_buy_order = place_buy_after_sell(
                                                exchange, order["id"], order_price, size,
                                                feature=feature, feature_order_id=feature_order_id
                                            )
                                            if new_buy_order and isinstance(new_buy_order, dict):
                                                bot_state["buy_orders"].append(new_buy_order)
                                                orders_to_send.append(
                                                    {
                                                        "type": "order",
                                                        "id": new_buy_order["id"],
                                                        "status": new_buy_order["status"],
                                                        "side": "buy",
                                                        "price": float(new_buy_order["price"]),
                                                        "timestamp": int(current_time * 1000),
                                                    }
                                                )
                                                logger.info(f"Placed new buy order after sell: id={new_buy_order['id']}, "
                                                            f"price={float(new_buy_order['price']):.2f}, size={size:.6f}, "
                                                            f"grid_level={len(bot_state['buy_orders'])}, feature={new_buy_order.get('feature', 'base')}, feature_order_id={new_buy_order.get('feature_order_id')}")
                                            else:
                                                logger.warning(f"Failed to place buy order after sell: sell_order_id={order['id']}, "
                                                            f"attempted_size={size:.6f}, feature={feature_label}")
                                        last_trade_time = current_time
                                        logger.info(f"Updated last_trade_time to {last_trade_time:.0f} after sell execution")

                                    orders_to_send.append(
                                        {
                                            "type": "order",
                                            "id": order["id"],
                                            "status": order_update["status"],
                                            "side": order_type,
                                            "price": float(order_price),
                                            "timestamp": int(current_time * 1000),
                                        }
                                    )
                                    logger.info(f"Added executed {order_type} order to WebSocket: id={order['id']}, "
                                                f"status={order_update['status']}, feature={feature_label}")

                            except ccxt.RateLimitExceeded:
                                logger.warning(f"Rate limit exceeded checking {order_type} order {order['id']}, "
                                            f"feature={feature_label}, waiting 5 seconds")
                                time.sleep(5)
                            except Exception as order_error:
                                logger.error(f"Error processing {order_type} order {order['id']} at grid_level {grid_level}, "
                                            f"feature={feature_label}: {order_error}")
                                if "401" in str(order_error):
                                    logger.error(f"Authentication error (401) for {order_type} order {order['id']}, "
                                                f"feature={feature_label}, waiting 5 seconds")
                                    time.sleep(5)

                        # --- Clean up orders (symmetrical for buy/sell) ---
                        for order in orders_to_remove:
                            if order in order_list:
                                order_list.remove(order)
                                feature_label = order.get("feature", "base")
                                feature_order_id = order.get("feature_order_id", None)
                                if feature_label == "base":
                                    logger.info(f"[BASE ORDER] Removed invalid or failed {order_type} order from list: id={order.get('id', 'unknown')}")
                                else:
                                    logger.info(f"[FEATURE ORDER] Removed invalid or failed {order_type} order from list: id={order.get('id', 'unknown')}, feature={feature_label}, feature_order_id={feature_order_id}")
                        # Remove closed orders from both buy and sell lists, preserving feature key
                        bot_state["buy_orders"] = [
                            order for order in bot_state["buy_orders"] if order.get("id") not in closed_order_ids
                        ]
                        bot_state["sell_orders"] = [
                            order for order in bot_state["sell_orders"] if order.get("id") not in closed_order_ids
                        ]
                        logger.info(f"[ORDER MGMT] Post-check {order_type} orders: {len(order_list)} remaining after removing {len(orders_to_remove)} orders")

                        # --- Log summary of open orders by type (symmetrical for buy/sell) ---
                        base_open = sum(1 for o in order_list if o.get("feature", "base") == "base" and o.get("status", "open") == "open")
                        feature_names = ["rsi", "bollinger", "macd", "ema", "atr", "volatility", "vwap"]
                        feature_open = {f: sum(1 for o in order_list if o.get("feature") == f and o.get("status", "open") == "open") for f in feature_names}
                        # Log all open feature order IDs for traceability
                        for f in feature_names:
                            open_ids = [o.get("id") for o in order_list if o.get("feature") == f and o.get("status", "open") == "open"]
                            open_feature_ids = [o.get("feature_order_id") for o in order_list if o.get("feature") == f and o.get("status", "open") == "open"]
                            if open_ids:
                                logger.info(f"[ORDER MGMT] {order_type.upper()} open {f.upper()} orders: ids={open_ids}, feature_order_ids={open_feature_ids}")
                        logger.info(f"[ORDER MGMT] {order_type.upper()} open base orders: {base_open}, feature orders: {feature_open}")
                        logger.info(f"[ORDER MGMT] {order_type.capitalize()} order check completed in {time.time() - check_order_type_start_time:.2f} seconds")

                        # --- Log absence of feature-based orders (symmetrical for buy/sell) ---
                        for feature_name in expanded_feature_list:
                            has_feature_order = any(order.get("feature") == feature_name and order.get("status", "open") == "open" for order in order_list if isinstance(order, dict))
                            if not has_feature_order:
                                logger.info(f"[ORDER MGMT] No open {order_type} orders for feature: {feature_name.upper()}")

                        # --- Ensure all order placement functions set 'feature' and 'feature_order_id' key for both buy and sell ---
                        # (This is a reminder for the rest of the code: when placing new orders after DCA, always set 'feature' and 'feature_order_id' key)

                        # --- DEBUG: Warn if no feature orders are present after placement attempts ---
                        if all(feature_open[f] == 0 for f in feature_names):
                            logger.warning(f"[ORDER MGMT] No open feature orders detected for {order_type.upper()} after order check. If you expect feature orders, check placement logic and feature tagging.")

                        # Example for place_buy_after_sell and place_sell_after_buy (should be defined elsewhere):
                        # def place_buy_after_sell(..., feature=None, feature_order_id=None):
                        #     ...
                        #     order = ...
                        #     if feature:
                        #         order["feature"] = feature
                        #     if feature_order_id:
                        #         order["feature_order_id"] = feature_order_id
                        #     ...
                        # def place_sell_after_buy(..., feature=None, feature_order_id=None):
                        #     ...
                        #     order = ...
                        #     if feature:
                        #         order["feature"] = feature
                        #     if feature_order_id:
                        #         order["feature_order_id"] = feature_order_id
                        #     ...

                        # (If these functions are not yet symmetrical, update them to always set the 'feature' and 'feature_order_id' key for both buy and sell orders.)

                    # Check and cancel excess orders if total exceeds MAX_NUM_GRID_LINES
                    total_orders = len(bot_state["buy_orders"]) + len(bot_state["sell_orders"])
                    max_grid_lines = config.MAX_NUM_GRID_LINES  # 52
                    logger.info(f"Total orders: {total_orders}, MAX_NUM_GRID_LINES: {max_grid_lines}")
                    if total_orders > max_grid_lines:
                        logger.info(f"Total orders ({total_orders}) exceeds MAX_NUM_GRID_LINES ({max_grid_lines}), initiating cancellation of excess orders")
                        excess_count = total_orders - max_grid_lines
                        logger.info(f"Calculated excess orders to cancel: {excess_count}")

                        # Combine buy and sell orders, sort by timestamp (newest first)
                        all_orders = [
                            {**order, "side": "buy"} for order in bot_state["buy_orders"]
                        ] + [
                            {**order, "side": "sell"} for order in bot_state["sell_orders"]
                        ]
                        all_orders.sort(key=lambda x: x.get("timestamp", 0), reverse=True)
                        logger.info(f"Combined and sorted {len(all_orders)} orders by timestamp (newest first)")

                        # Calculate target buy/sell counts for 50/50 balance
                        buy_count = len(bot_state["buy_orders"])
                        sell_count = len(bot_state["sell_orders"])
                        logger.info(f"Current order distribution: Buy orders={buy_count}, Sell orders={sell_count}")
                        target_each = max_grid_lines // 2  # Aim for equal buy/sell
                        logger.info(f"Target for each side (buy/sell): {target_each}")
                        buy_to_cancel = max(0, buy_count - target_each)
                        sell_to_cancel = max(0, sell_count - target_each)
                        logger.info(f"Initial cancellation plan: Cancel {buy_to_cancel} buy orders, {sell_to_cancel} sell orders")

                        # Adjust if total cancellations are less than excess_count
                        if buy_to_cancel + sell_to_cancel < excess_count:
                            remaining = excess_count - (buy_to_cancel + sell_to_cancel)
                            logger.info(f"Additional cancellations needed: {remaining}")
                            if buy_count > sell_count:
                                buy_to_cancel += remaining
                                logger.info(f"Assigning {remaining} additional cancellations to buy orders (buy_count={buy_count} > sell_count={sell_count})")
                            else:
                                sell_to_cancel += remaining
                                logger.info(f"Assigning {remaining} additional cancellations to sell orders (sell_count={sell_count} >= buy_count={buy_count})")
                        logger.info(f"Final cancellation plan: Cancel {buy_to_cancel} buy orders, {sell_to_cancel} sell orders")

                        # Select orders to cancel, prioritizing newest and balancing buy/sell
                        orders_to_cancel = []
                        buy_cancelled = 0
                        sell_cancelled = 0
                        for order in all_orders:
                            if order["side"] == "buy" and buy_cancelled < buy_to_cancel:
                                orders_to_cancel.append(order)
                                buy_cancelled += 1
                            elif order["side"] == "sell" and sell_cancelled < sell_to_cancel:
                                orders_to_cancel.append(order)
                                sell_cancelled += 1
                            if len(orders_to_cancel) >= excess_count:
                                break
                        logger.info(f"Selected {len(orders_to_cancel)} orders for cancellation: {buy_cancelled} buy, {sell_cancelled} sell")

                        # Cancel selected orders
                        for order in orders_to_cancel:
                            try:
                                cancelled = cancel_orders(exchange, [order], config.SYMBOL)
                                if cancelled:
                                    logger.info(f"Cancelled excess {order['side']} order: id={order['id']}, "
                                                f"price={order.get('price', 0):.2f}, feature={order.get('feature', 'base')}")
                                    orders_to_remove.append(order)
                                    orders_to_send.append(
                                        {
                                            "type": "order",
                                            "id": order["id"],
                                            "status": "canceled",
                                            "side": order["side"],
                                            "timestamp": int(current_time * 1000),
                                            "price": float(order.get("price", 0)),
                                        }
                                    )
                                    logger.info(f"Added canceled excess {order['side']} order to WebSocket: id={order['id']}, "
                                                f"status=canceled, feature={order.get('feature', 'base')}")
                                    if order["side"] == "buy":
                                        bot_state["buy_orders"] = [o for o in bot_state["buy_orders"] if o["id"] != order["id"]]
                                        logger.info(f"Removed cancelled buy order id={order['id']} from bot_state['buy_orders']")
                                    else:
                                        bot_state["sell_orders"] = [o for o in bot_state["sell_orders"] if o["id"] != order["id"]]
                                        logger.info(f"Removed cancelled sell order id={order['id']} from bot_state['sell_orders']")
                                else:
                                    logger.warning(f"Failed to cancel excess {order['side']} order: id={order['id']}, "
                                                f"feature={order.get('feature', 'base')}")
                            except Exception as e:
                                logger.error(f"Failed to cancel excess {order['side']} order id={order['id']}: {e}")

                    logger.info(f"Active grid after checks: Buy orders={len(bot_state['buy_orders'])}, Sell orders={len(bot_state['sell_orders'])}")

                    # Log data structures
                    logger.info(f"recent_trade_data columns: {recent_trade_data.columns.tolist()}, rows: {len(recent_trade_data)}")
                    logger.info(f"feature_cache columns: {bot_state['feature_cache'].columns.tolist()}, rows={len(bot_state['feature_cache'])}")

                    logger.info(f"Total order checking time: {time.time() - check_orders_start_time:.2f} seconds")
                except Exception as e:
                    logger.error(f"Error in order checking process: {e}")

                # 6 ML Predictions
                try:
                    # Update live data from WebSocket
                    ohlcv_df, raw_trades_for_ohlcv_df, updated = _update_live_data_from_websocket(
                        websocket_manager, ohlcv_df, raw_trades_for_ohlcv_df
                    )
                    logger.info(f"Updated WebSocket data: ohlcv_df={len(ohlcv_df)} rows, feature_cache={len(bot_state['feature_cache'])} rows")

                    # Sync recent_trade_data
                    recent_trade_data = bot_state["feature_cache"].tail(config.SKLEARN_LOOKBACK).copy()
                    logger.info(f"recent_trade_data columns: {recent_trade_data.columns.tolist()}, rows: {len(recent_trade_data)}")

                    # Define dynamic defaults
                    close_mean = ohlcv_df["close"].mean() if not ohlcv_df.empty else current_price
                    defaults = {
                        "rsi": ohlcv_df["rsi"].mean() if not ohlcv_df.empty and "rsi" in ohlcv_df else 50.0,
                        "ema": close_mean,
                        "volatility": ohlcv_df["volatility"].mean() if not ohlcv_df.empty and "volatility" in ohlcv_df else 0.1,
                        "macd": 0.0,
                        "macd_signal": 0.0,
                        "bollinger_upper": close_mean,
                        "bollinger_lower": close_mean,
                        "momentum": 0.0,
                        "volume_trend": 0.0,
                        "atr": close_mean * 0.01,
                        "vwap": close_mean,
                        "trades": int(ohlcv_df["trades"].mean()) if not ohlcv_df.empty and "trades" in ohlcv_df else 0,
                        "close": close_mean,
                        "volume": ohlcv_df["volume"].mean() if not ohlcv_df.empty and "volume" in ohlcv_df else 0.0,
                        "price_spread": 0.0,
                        "returns": 0.0,
                        "volume_change": 0.0,
                        "trade_intensity": 0.0
                    }
                    logger.info("Defined dynamic defaults using WebSocket data")

                    # Ensure required features
                    required_features = [
                        "close", "volume", "trades", "rsi", "ema", "volatility", "macd", "macd_signal",
                        "bollinger_upper", "bollinger_lower", "momentum", "volume_trend", "atr", "vwap",
                        "price_spread", "returns", "volume_change", "trade_intensity"
                    ]
                    for feature in required_features:
                        if feature not in recent_trade_data:
                            logger.warning(f"Adding missing feature {feature} with default")
                            recent_trade_data[feature] = defaults.get(feature, 0.0)
                        if recent_trade_data[feature].isna().any():
                            logger.info(f"Filling {recent_trade_data[feature].isna().sum()} NaN values in {feature}")
                            recent_trade_data[feature] = recent_trade_data[feature].ffill().bfill().fillna(defaults.get(feature, 0.0))

                    # Filter to required features
                    recent_trade_data = recent_trade_data[required_features].copy()
                    logger.debug(f"Filtered recent_trade_data to {len(recent_trade_data)} rows with features: {recent_trade_data.columns.tolist()}")

                    # Validate timestamps
                    if "timestamp" not in recent_trade_data:
                        logger.warning("Adding timestamp column")
                        recent_trade_data["timestamp"] = ohlcv_df["timestamp"].iloc[-len(recent_trade_data):].reindex(recent_trade_data.index).fillna(pd.Timestamp.now(tz="UTC"))
                    if recent_trade_data["timestamp"].isna().any():
                        logger.warning("Fixing NaN timestamps")
                        recent_trade_data["timestamp"] = recent_trade_data["timestamp"].fillna(pd.Timestamp.now(tz="UTC"))
                    recent_trade_data["timestamp"] = pd.to_datetime(recent_trade_data["timestamp"], errors="coerce").dt.tz_convert("UTC")

                    # Pad recent_trade_data if necessary
                    if len(recent_trade_data) < config.SKLEARN_LOOKBACK:
                        logger.warning(f"Padding recent_trade_data: {len(recent_trade_data)} rows, need {config.SKLEARN_LOOKBACK}")
                        padding_rows = config.SKLEARN_LOOKBACK - len(recent_trade_data)
                        padding_data = bot_state["feature_cache"][required_features].tail(padding_rows + len(recent_trade_data)).head(padding_rows).copy()
                        for feature in required_features:
                            if feature not in padding_data:
                                padding_data[feature] = defaults.get(feature, 0.0)
                            padding_data[feature] = padding_data[feature].ffill().bfill().fillna(defaults.get(feature, 0.0))
                        recent_trade_data = pd.concat([padding_data, recent_trade_data], ignore_index=True).tail(config.SKLEARN_LOOKBACK)

                    logger.info(f"Prepared ML data: rows={len(recent_trade_data)}, features={required_features}")

                    # Run ML predictions
                    log_ref_timestamp = recent_trade_data['timestamp'].iloc[-1] if not recent_trade_data.empty else pd.Timestamp.now(tz="UTC")
                    log_ref_current_close = recent_trade_data['close'].iloc[-1] if not recent_trade_data.empty else current_price

                    sklearn_predicted = last_sklearn_prediction
                    if bot_state.get("sklearn_rf_model"):
                        try:
                            sklearn_predicted = predict_sklearn_price(
                                bot_state["sklearn_rf_model"],
                                bot_state["scaler_sklearn_rf"],
                                config.SKLEARN_LOOKBACK,
                                recent_trade_data,
                                current_price,
                                last_sklearn_prediction,
                            )
                            logger.info(
                                f"Sklearn Prediction: timestamp={log_ref_timestamp}, "
                                f"predicted_price={sklearn_predicted:.2f}, current_close={log_ref_current_close:.2f}"
                            )
                        except Exception as e:
                            logger.error(f"Sklearn prediction failed: {e}")
                            sklearn_predicted = last_sklearn_prediction or current_price

                    pytorch_predicted = last_pytorch_prediction
                    if bot_state.get("pytorch_model"):
                        try:
                            pytorch_predicted = predict_pytorch_price(
                                bot_state["pytorch_model"],
                                bot_state["pytorch_scaler"],
                                bot_state["pytorch_target_scaler"],
                                config.PYTORCH_LOOKBACK,
                                recent_trade_data,
                                current_price,
                                last_pytorch_prediction,
                            )
                            logger.info(
                                f"Pytorch Prediction: timestamp={log_ref_timestamp}, "
                                f"predicted_price={pytorch_predicted:.2f}, current_close={log_ref_current_close:.2f}"
                            )
                        except Exception as e:
                            logger.error(f"Pytorch prediction failed: {e}")
                            pytorch_predicted = last_pytorch_prediction or current_price

                    xgb_predicted = last_xgb_prediction
                    if bot_state.get("xgb_model"):
                        try:
                            xgb_predicted = predict_xgboost_price(
                                bot_state["xgb_model"],
                                bot_state["xgb_scaler"],
                                60,  # Fixed lookback to match training
                                recent_trade_data,
                                current_price,
                                last_xgb_prediction,
                            )
                            logger.info(
                                f"XGBoost Prediction: timestamp={log_ref_timestamp}, "
                                f"predicted_price={xgb_predicted:.2f}, current_close={log_ref_current_close:.2f}"
                            )
                        except Exception as e:
                            logger.error(f"XGBoost prediction failed: {e}")
                            xgb_predicted = last_xgb_prediction or current_price

                    meta_predicted = log_ref_current_close
                    if bot_state.get("meta_model") and bot_state.get("meta_scaler") and all(np.isfinite([sklearn_predicted, pytorch_predicted, xgb_predicted])):
                        volatility = (
                            recent_trade_data["volatility"].iloc[-1]
                            if "volatility" in recent_trade_data and not recent_trade_data.empty and not pd.isna(recent_trade_data["volatility"].iloc[-1])
                            else defaults["volatility"]
                        )
                        try:
                            meta_predicted = predict_meta_model(
                                bot_state["meta_model"],
                                bot_state["meta_scaler"],
                                sklearn_predicted,
                                pytorch_predicted,
                                xgb_predicted,
                                volatility,
                                current_close=log_ref_current_close
                            )
                            logger.info(
                                f"Meta-Model Prediction: timestamp={log_ref_timestamp}, "
                                f"predicted_price={meta_predicted:.2f}, current_close={log_ref_current_close:.2f}"
                            )
                        except Exception as e:
                            logger.error(f"Meta-model prediction failed: {e}")
                            valid_preds = [p for p in [sklearn_predicted, pytorch_predicted, xgb_predicted] if np.isfinite(p)]
                            meta_predicted = np.mean(valid_preds) if valid_preds else log_ref_current_close

                    # Validate predictions
                    valid_predictions = [
                        p for p in [sklearn_predicted, pytorch_predicted, xgb_predicted, meta_predicted]
                        if np.isfinite(p) and 1000 < p < 10000
                    ]
                    if not valid_predictions:
                        logger.warning("No valid ML predictions, using current price")
                        sklearn_predicted = pytorch_predicted = xgb_predicted = meta_predicted = current_price
                    else:
                        avg_prediction = sum(valid_predictions) / len(valid_predictions)
                        sklearn_predicted = sklearn_predicted if np.isfinite(sklearn_predicted) and 1000 < sklearn_predicted < 10000 else avg_prediction
                        pytorch_predicted = pytorch_predicted if np.isfinite(pytorch_predicted) and 1000 < pytorch_predicted < 10000 else avg_prediction
                        xgb_predicted = xgb_predicted if np.isfinite(xgb_predicted) and 1000 < xgb_predicted < 10000 else avg_prediction
                        meta_predicted = meta_predicted if np.isfinite(meta_predicted) and 1000 < meta_predicted < 10000 else avg_prediction

                    # Update prediction history
                    prediction_history = bot_state.get(
                        "prediction_history",
                        pd.DataFrame(
                            columns=[
                                "sklearn_pred", "pytorch_pred", "xgb_pred",
                                "current_price", "volatility", "actual_price"
                            ]
                        ),
                    )
                    new_entry = pd.DataFrame(
                        {
                            "sklearn_pred": [sklearn_predicted],
                            "pytorch_pred": [pytorch_predicted],
                            "xgb_pred": [xgb_predicted],
                            "current_price": [current_price],
                            "volatility": [
                                recent_trade_data["volatility"].iloc[-1]
                                if "volatility" in recent_trade_data and not recent_trade_data.empty and not pd.isna(recent_trade_data["volatility"].iloc[-1])
                                else defaults["volatility"]
                            ],
                            "actual_price": [
                                recent_trade_data["close"].iloc[-1]
                                if not recent_trade_data.empty
                                else current_price
                            ],
                        }
                    )
                    prediction_history = pd.concat([prediction_history, new_entry], ignore_index=True).tail(config.PREDICTIONS_LIMIT)
                    bot_state["prediction_history"] = prediction_history
                    logger.debug(f"Updated prediction_history: {len(prediction_history)} rows")
                    last_sklearn_prediction = sklearn_predicted
                    last_pytorch_prediction = pytorch_predicted
                    last_xgb_prediction = xgb_predicted
                except Exception as ml_error:
                    logger.error(f"Error in ML predictions: {ml_error}")
                    sklearn_predicted = last_sklearn_prediction or current_price
                    pytorch_predicted = last_pytorch_prediction or current_price
                    xgb_predicted = last_xgb_prediction or current_price
                    meta_predicted = current_price

                # Online Updates (separate block)
                if len(recent_trade_data) >= config.PYTORCH_LOOKBACK:
                    try:
                        # Sklearn SGD update
                        X_sgd = recent_trade_data.tail(config.SKLEARN_LOOKBACK)[required_features]
                        y_sgd = recent_trade_data["close"].tail(config.SKLEARN_LOOKBACK).values
                        logger.debug(f"Sklearn SGD update: X shape={X_sgd.shape}, y shape={y_sgd.shape}")
                        if bot_state.get("sklearn_sgd_model"):
                            bot_state["sklearn_sgd_model"] = online_update_sklearn(
                                bot_state["sklearn_sgd_model"],
                                bot_state["scaler_sklearn_sgd"],
                                X_sgd,
                                y_sgd,
                                lookback=config.SKLEARN_LOOKBACK
                            )
                            logger.info("Sklearn SGD model updated online")
                        else:
                            logger.error("SGDRegressor model not initialized")

                        # PyTorch update
                        X_pytorch = recent_trade_data.tail(config.PYTORCH_LOOKBACK)[required_features]
                        y_pytorch = recent_trade_data["close"].tail(config.PYTORCH_LOOKBACK)
                        logger.debug(f"PyTorch update: X shape={X_pytorch.shape}, y shape={y_pytorch.shape}")
                        if bot_state.get("pytorch_model"):
                            bot_state["pytorch_model"] = online_update_pytorch(
                                bot_state["pytorch_model"],
                                bot_state["pytorch_scaler"],
                                bot_state["pytorch_target_scaler"],
                                X_pytorch,
                                y_pytorch,
                                config.DEVICE
                            )
                            logger.info("PyTorch model updated online")
                        else:
                            logger.error("PyTorch model not initialized")

                        logger.info("Performed online model updates")
                    except Exception as update_error:
                        logger.error(f"Error in online model updates: {update_error}")

                # 7. Adjust Parameters
                old_grid_size = config.GRID_SIZE
                old_position_size = config.POSITION_SIZE
                try:
                    if sklearn_predicted and pytorch_predicted and xgb_predicted and meta_predicted:
                        logger.info(
                            f"ML Predictions: Sklearn={sklearn_predicted:.2f}, PyTorch={pytorch_predicted:.2f}, "
                            f"XGBoost={xgb_predicted:.2f}, Meta={meta_predicted:.2f}, Current Price={current_price:.2f}"
                        )
                        logger.info(
                            f"Config Parameters: ML_CONFIDENCE_THRESHOLD={config.ML_CONFIDENCE_THRESHOLD:.2f}, "
                            f"VOLATILITY_THRESHOLD={config.VOLATILITY_THRESHOLD:.2f}, ML_TREND_WEIGHT={config.ML_TREND_WEIGHT:.2f}, "
                            f"ML_GRID_ADJUST_FACTOR={config.ML_GRID_ADJUST_FACTOR:.3f}, ML_POSITION_ADJUST_FACTOR={config.ML_POSITION_ADJUST_FACTOR:.3f}"
                        )
                        logger.info(
                            f"Initial Parameters: GRID_SIZE={initial_grid_size:.2f}, POSITION_SIZE={initial_position_size:.6f}"
                        )

                        # Accumulate MSEs
                        meta_metrics = bot_state.get("meta_metrics", {
                            "sklearn_mse": [],
                            "pytorch_mse": [],
                            "xgb_mse": [],
                            "meta_mse": []
                        })
                        actual_price = recent_data["close"].iloc[-1] if not recent_data.empty else current_price
                        meta_metrics["sklearn_mse"].append((sklearn_predicted - actual_price) ** 2)
                        meta_metrics["pytorch_mse"].append((pytorch_predicted - actual_price) ** 2)
                        meta_metrics["xgb_mse"].append((xgb_predicted - actual_price) ** 2)
                        meta_metrics["meta_mse"].append((meta_predicted - actual_price) ** 2)
                        # Limit size to prevent memory issues
                        for key in meta_metrics:
                            meta_metrics[key] = meta_metrics[key][-config.PREDICTION_HISTORY_MAX_ROWS:]
                        bot_state["meta_metrics"] = meta_metrics

                        # Validate and initialize meta_metrics
                        if not isinstance(meta_metrics, dict) or not all(
                            key in meta_metrics for key in ["sklearn_mse", "pytorch_mse", "xgb_mse", "meta_mse"]
                        ):
                            logger.warning(f"Invalid meta_metrics structure: {meta_metrics}, initializing default")
                            meta_metrics = {
                                "sklearn_mse": [],
                                "pytorch_mse": [],
                                "xgb_mse": [],
                                "meta_mse": []
                            }
                            bot_state["meta_metrics"] = meta_metrics

                        # Format Meta Metrics
                        model_rows = {
                            "Sklearn": len(meta_metrics["sklearn_mse"]),
                            "PyTorch": len(meta_metrics["pytorch_mse"]),
                            "XGBoost": len(meta_metrics["xgb_mse"]),
                            "Meta": len(meta_metrics["meta_mse"])
                        }
                        model_values = {
                            "Sklearn": {
                                "Recent": meta_metrics["sklearn_mse"][-1] if meta_metrics["sklearn_mse"] else float("inf"),
                                "Average": np.mean(meta_metrics["sklearn_mse"]) if meta_metrics["sklearn_mse"] else float("inf")
                            },
                            "PyTorch": {
                                "Recent": meta_metrics["pytorch_mse"][-1] if meta_metrics["pytorch_mse"] else float("inf"),
                                "Average": np.mean(meta_metrics["pytorch_mse"]) if meta_metrics["pytorch_mse"] else float("inf")
                            },
                            "XGBoost": {
                                "Recent": meta_metrics["xgb_mse"][-1] if meta_metrics["xgb_mse"] else float("inf"),
                                "Average": np.mean(meta_metrics["xgb_mse"]) if meta_metrics["xgb_mse"] else float("inf")
                            },
                            "Meta": {
                                "Recent": meta_metrics["meta_mse"][-1] if meta_metrics["meta_mse"] else float("inf"),
                                "Average": np.mean(meta_metrics["meta_mse"]) if meta_metrics["meta_mse"] else float("inf")
                            }
                        }
                        logger.info(
                            f"Meta Metrics - Model Rows: Sklearn={model_rows['Sklearn']} rows, PyTorch={model_rows['PyTorch']} rows, "
                            f"XGBoost={model_rows['XGBoost']} rows, Meta={model_rows['Meta']} rows"
                        )
                        logger.info(
                            f"Meta Metrics - Model Values: "
                            f"Sklearn(Recent={model_values['Sklearn']['Recent']:.6f}, Avg={model_values['Sklearn']['Average']:.6f}), "
                            f"PyTorch(Recent={model_values['PyTorch']['Recent']:.6f}, Avg={model_values['PyTorch']['Average']:.6f}), "
                            f"XGBoost(Recent={model_values['XGBoost']['Recent']:.6f}, Avg={model_values['XGBoost']['Average']:.6f}), "
                            f"Meta(Recent={model_values['Meta']['Recent']:.6f}, Avg={model_values['Meta']['Average']:.6f})"
                        )

                        mse_values = {
                            "sklearn": model_values["Sklearn"]["Average"],
                            "pytorch": model_values["PyTorch"]["Average"],
                            "xgboost": model_values["XGBoost"]["Average"],
                            "meta": model_values["Meta"]["Average"]
                        }

                        best_mse = min([v for v in mse_values.values() if v != float("inf")] or [1.0])
                        inverse_mse = {}
                        for k, v in mse_values.items():
                            if v != float("inf"):
                                inverse_mse[k] = 1.0 / max(v, 1e-6)
                                if k == "meta" and v <= 2.0 * best_mse:
                                    inverse_mse[k] = max(inverse_mse[k], 0.1 * sum(inverse_mse.values()))
                            else:
                                inverse_mse[k] = 0.0
                        total_inverse = sum(inverse_mse.values())
                        mse_weights = (
                            {k: v / total_inverse for k, v in inverse_mse.items()}
                            if total_inverse > 0
                            else {
                                "sklearn": config.SKLEARN_WEIGHT,
                                "pytorch": config.PYTORCH_WEIGHT,
                                "xgboost": config.XGB_WEIGHT,
                                "meta": config.META_WEIGHT,
                            }
                        )

                        config_weights = {
                            "sklearn": config.SKLEARN_WEIGHT,
                            "pytorch": config.PYTORCH_WEIGHT,
                            "xgboost": config.XGB_WEIGHT,
                            "meta": config.META_WEIGHT,
                        }
                        logger.info(
                            f"Config Weights: Sklearn={config_weights['sklearn']:.3f}, PyTorch={config_weights['pytorch']:.3f}, "
                            f"XGBoost={config_weights['xgboost']:.3f}, Meta={config_weights['meta']:.3f}"
                        )
                        model_weights = {
                            k: 0.5 * config_weights[k] + 0.5 * mse_weights[k]
                            for k in ["sklearn", "pytorch", "xgboost", "meta"]
                        }
                        total_weight = sum(model_weights.values())
                        if total_weight > 0:
                            model_weights = {k: v / total_weight for k, v in model_weights.items()}
                        else:
                            model_weights = {
                                "sklearn": config.SKLEARN_WEIGHT,
                                "pytorch": config.PYTORCH_WEIGHT,
                                "xgboost": config.XGB_WEIGHT,
                                "meta": config.META_WEIGHT,
                            }
                        logger.info(
                            f"Final Model Weights: Sklearn={model_weights['sklearn']:.3f}, PyTorch={model_weights['pytorch']:.3f}, "
                            f"XGBoost={model_weights['xgboost']:.3f}, Meta={model_weights['meta']:.3f}"
                        )

                        # Check for static meta-model predictions
                        if meta_predicted is not None and np.isfinite(meta_predicted):
                            bot_state["recent_meta_outputs"].append(meta_predicted)
                        
                        meta_predictions_list = list(bot_state["recent_meta_outputs"])
                        if not meta_predictions_list:
                            meta_predictions_list = [meta_predicted if meta_predicted is not None and np.isfinite(meta_predicted) else current_price]

                        meta_variance = np.var(meta_predictions_list) if len(meta_predictions_list) > 1 else 0.0
                        logger.info(f"Meta-model prediction variance: {meta_variance:.6f}")
                        if meta_variance < 0.01:
                            logger.warning(f"Static meta-model predictions detected: variance={meta_variance:.6f}, reducing meta weight")
                            model_weights['meta'] = model_weights['meta'] * 0.5
                            total_other = sum([v for k, v in model_weights.items() if k != 'meta'])
                            for k in ['sklearn', 'pytorch', 'xgboost']:
                                model_weights[k] = model_weights[k] * (1.0 / total_other) if total_other > 0 else model_weights[k]
                            logger.info(
                                f"Adjusted Model Weights: Sklearn={model_weights['sklearn']:.3f}, PyTorch={model_weights['pytorch']:.3f}, "
                                f"XGBoost={model_weights['xgboost']:.3f}, Meta={model_weights['meta']:.3f}"
                            )

                        sklearn_trend = (
                            (sklearn_predicted - current_price) * config.ML_TREND_WEIGHT * model_weights["sklearn"]
                        )
                        pytorch_trend = (
                            (pytorch_predicted - current_price) * config.ML_TREND_WEIGHT * model_weights["pytorch"]
                        )
                        xgb_trend = (
                            (xgb_predicted - current_price) * config.ML_TREND_WEIGHT * model_weights["xgboost"]
                        )
                        meta_trend = (
                            (meta_predicted - current_price) * config.ML_TREND_WEIGHT * model_weights["meta"]
                        )
                        current_trend = current_price - previous_price if previous_price else 0.0
                        logger.debug(
                            f"Weighted Trend Calculations: Sklearn=({sklearn_predicted:.2f}-{current_price:.2f})*{config.ML_TREND_WEIGHT}*{model_weights['sklearn']:.3f}={sklearn_trend:.2f}, "
                            f"PyTorch=({pytorch_predicted:.2f}-{current_price:.2f})*{config.ML_TREND_WEIGHT}*{model_weights['pytorch']:.3f}={pytorch_trend:.2f}, "
                            f"XGBoost=({xgb_predicted:.2f}-{current_price:.2f})*{config.ML_TREND_WEIGHT}*{model_weights['xgboost']:.3f}={xgb_trend:.2f}, "
                            f"Meta=({meta_predicted:.2f}-{current_price:.2f})*{config.ML_TREND_WEIGHT}*{model_weights['meta']:.3f}={meta_trend:.2f}"
                        )
                        logger.info(
                            f"Trend Contributions: Sklearn={sklearn_trend:.2f}, PyTorch={pytorch_trend:.2f}, "
                            f"XGBoost={xgb_trend:.2f}, Meta={meta_trend:.2f}, Current={current_trend:.2f}"
                        )

                        volatility = (
                            ohlcv_df["volatility"].iloc[-1]
                            if not ohlcv_df.empty
                            and "volatility" in ohlcv_df.columns
                            and not pd.isna(ohlcv_df["volatility"].iloc[-1])
                            and ohlcv_df["volatility"].iloc[-1] >= config.VOLATILITY_THRESHOLD
                            else config.LOG_DEFAULT_VOLATILITY
                        )
                        logger.info(f"Volatility: {volatility:.2f}% (threshold={config.VOLATILITY_THRESHOLD:.2f})")

                        historical_mean = historical_data['volume'].mean() if not historical_data.empty else 0.0
                        volume_factor = (
                            min(
                                config.VOLUME_FACTOR_MAX,
                                max(
                                    config.VOLUME_FACTOR_MIN,
                                    current_volume / historical_mean,
                                ),
                            )
                            if not historical_data.empty and historical_mean > 0
                            else 1.0
                        )
                        logger.info(
                            f"Volume Factor: {volume_factor:.2f} (current_volume={current_volume:.2f}, "
                            f"historical_mean={historical_mean:.2f})"
                        )

                        up_votes = (
                            (1 if meta_trend > 0 else 0) * abs(meta_trend)
                            + (1 if sklearn_trend > 0 else 0) * abs(sklearn_trend)
                            + (1 if pytorch_trend > 0 else 0) * abs(pytorch_trend)
                            + (1 if xgb_trend > 0 else 0) * abs(xgb_trend)
                            + (1 if current_trend > 0 else 0) * abs(current_trend)
                        )
                        down_votes = (
                            (1 if meta_trend < 0 else 0) * abs(meta_trend)
                            + (1 if sklearn_trend < 0 else 0) * abs(sklearn_trend)
                            + (1 if pytorch_trend < 0 else 0) * abs(pytorch_trend)
                            + (1 if xgb_trend < 0 else 0) * abs(xgb_trend)
                            + (1 if current_trend < 0 else 0) * abs(current_trend)
                        )
                        total_confidence = up_votes + down_votes
                        up_confidence = up_votes / total_confidence if total_confidence > 0 else 0.5
                        down_confidence = down_votes / total_confidence if total_confidence > 0 else 0.5
                        logger.info(
                            f"Trend Voting: Up Votes={up_votes:.2f}, Down Votes={down_votes:.2f}, "
                            f"Up Confidence={up_confidence:.2f}, Down Confidence={down_confidence:.2f}"
                        )

                        grid_adjust_factor = 1.0 + (volatility / 100) * config.VOLATILITY_GRID_FACTOR
                        position_adjust_factor = 1.0 + (volatility / 100) * config.VOLATILITY_POSITION_FACTOR
                        logger.info(
                            f"Adjustment Factors: Grid={grid_adjust_factor:.3f}, Position={position_adjust_factor:.3f}"
                        )

                        max_usd_position = (
                            bot_state["initial_usd"] / (config.NUM_BUY_GRID_LINES * current_price)
                            if bot_state.get("initial_usd", 0) > 0
                            else config.MAX_POSITION_SIZE
                        )
                        max_eth_position = (
                            (bot_state["initial_usd"] / config.ETH_BALANCE_DIVISOR)
                            / current_price
                            / config.NUM_SELL_GRID_LINES
                            if bot_state.get("initial_usd", 0) > 0
                            else config.MAX_POSITION_SIZE
                        )
                        logger.info(
                            f"Position Limits: Max USD={max_usd_position:.6f}, Max ETH={max_eth_position:.6f}"
                        )

                        if up_confidence > config.ML_CONFIDENCE_THRESHOLD:
                            new_grid_size = old_grid_size * (
                                1.0 + config.ML_GRID_ADJUST_FACTOR * grid_adjust_factor * volume_factor
                            )
                            config.GRID_SIZE = min(
                                config.MAX_GRID_SIZE,
                                max(config.MIN_GRID_SIZE, new_grid_size),
                            )
                            new_position_size = initial_position_size * (
                                1.0 + config.ML_POSITION_ADJUST_FACTOR * position_adjust_factor * up_confidence
                            )
                            # Clamp new_position_size to MIN and MAX bounds
                            new_position_size = max(
                                config.MIN_POSITION_SIZE,
                                min(config.MAX_POSITION_SIZE, new_position_size)
                            )
                            config.POSITION_SIZE = min(
                                max_usd_position,
                                max_eth_position,
                                new_position_size,
                            )
                            # Ensure final POSITION_SIZE respects MIN_POSITION_SIZE
                            config.POSITION_SIZE = max(config.MIN_POSITION_SIZE, config.POSITION_SIZE)
                            logger.info(
                                f"ML Uptrend (confidence: {up_confidence:.2f}), GRID_SIZE: {old_grid_size:.2f} -> {config.GRID_SIZE:.2f}, "
                                f"POSITION_SIZE: {old_position_size:.6f} -> {config.POSITION_SIZE:.6f}"
                            )
                        elif down_confidence > config.ML_CONFIDENCE_THRESHOLD:
                            new_grid_size = old_grid_size * (
                                1.0 - config.ML_GRID_ADJUST_FACTOR * grid_adjust_factor * volume_factor
                            )
                            config.GRID_SIZE = max(config.MIN_GRID_SIZE, new_grid_size)
                            new_position_size = initial_position_size * (
                                1.0 - config.ML_POSITION_ADJUST_FACTOR * position_adjust_factor * down_confidence
                            )
                            # Clamp new_position_size to MIN and MAX bounds
                            new_position_size = max(
                                config.MIN_POSITION_SIZE,
                                min(config.MAX_POSITION_SIZE, new_position_size)
                            )
                            config.POSITION_SIZE = max(
                                config.MIN_POSITION_SIZE,
                                min(
                                    max_usd_position,
                                    max_eth_position,
                                    new_position_size,
                                ),
                            )
                            logger.info(
                                f"ML Downtrend (confidence: {down_confidence:.2f}), GRID_SIZE: {old_grid_size:.2f} -> {config.GRID_SIZE:.2f}, "
                                f"POSITION_SIZE: {old_position_size:.6f} -> {config.POSITION_SIZE:.6f}"
                            )

                        if "last_reset_grid_size" not in bot_state:
                            bot_state["last_reset_grid_size"] = initial_grid_size

                        last_reset_grid_size = bot_state["last_reset_grid_size"]
                        grid_change = (
                            abs(config.GRID_SIZE - last_reset_grid_size) / last_reset_grid_size
                            if last_reset_grid_size > 0
                            else 0
                        )
                        position_change = (
                            abs(config.POSITION_SIZE - initial_position_size) / initial_position_size
                            if initial_position_size > 0
                            else 0
                        )

                        if config.GRID_SIZE != old_grid_size:
                            current_max_order_range = config.MAX_ORDER_RANGE * (config.GRID_SIZE / config.MIN_GRID_SIZE)
                            current_stagnation_timeout = config.STAGNATION_TIMEOUT * (
                                config.MIN_GRID_SIZE / config.GRID_SIZE
                            )
                            logger.info(
                                f"ML Grid adjusted - GRID_SIZE: {old_grid_size:.2f} -> {config.GRID_SIZE:.2f}, "
                                f"MAX_ORDER_RANGE: {current_max_order_range:.2f}, STAGNATION_TIMEOUT: {current_stagnation_timeout:.0f}s, "
                                f"Volatility: {volatility:.2f}%"
                            )
                            bot_state["last_reset_grid_size"] = config.GRID_SIZE
                    else:
                        logger.warning(
                            f"Missing ML predictions: Sklearn={'set' if sklearn_predicted else 'missing'}, "
                            f"PyTorch={'set' if pytorch_predicted else 'missing'}, "
                            f"XGBoost={'set' if xgb_predicted else 'missing'}, "
                            f"Meta={'set' if meta_predicted else 'missing'}, skipping parameter adjustment"
                        )
                except Exception as param_error:
                    logger.error(f"Error adjusting parameters: {param_error}")
                    config.GRID_SIZE = old_grid_size
                    config.POSITION_SIZE = old_position_size

                # 8. Send WebSocket Updates
                try:
                    valid_orders_to_send = []
                    for order in orders_to_send:
                        if not order or not isinstance(order, dict) or "id" not in order:
                            logger.warning(f"Skipping invalid order for WebSocket send: {order}")
                            continue
                        valid_orders_to_send.append(order)

                    timestamp = (
                        recent_data["timestamp"].iloc[-1]
                        if "timestamp" in recent_data and pd.notna(recent_data["timestamp"].iloc[-1])
                        else pd.Timestamp.now(tz="UTC")
                    )
                    orders_to_send.extend(
                        [
                            {
                                "type": "predictions",
                                "timestamp": int(current_time * 1000),
                                "sklearn_predicted": float(last_sklearn_prediction),
                                "pytorch_predicted": float(last_pytorch_prediction),
                                "xgb_predicted": float(last_xgb_prediction),
                            },
                            {
                                "type": "pl_update",
                                "timestamp": int(current_time * 1000),
                                "total_pl": float(config.TOTAL_PL),
                            },
                            {
                                "type": "status",
                                "paused": bot_state["paused"],
                                "current_price": float(current_price),
                                "buy_orders": len(bot_state["buy_orders"]),
                                "sell_orders": len(bot_state["sell_orders"]),
                                "total_pl": float(config.TOTAL_PL),
                                "sklearn_prediction": float(last_sklearn_prediction),
                                "pytorch_prediction": float(last_pytorch_prediction),
                                "xgb_prediction": float(last_xgb_prediction),
                                "eth_balance": float(eth_balance),
                                "usd_balance": float(usd_balance),
                                "grid_size": float(config.GRID_SIZE),
                                "position_size": float(config.POSITION_SIZE),
                                "num_buy_grid_lines": int(config.NUM_BUY_GRID_LINES),
                                "num_sell_grid_lines": int(config.NUM_SELL_GRID_LINES),
                                "min_grid_size": float(config.MIN_GRID_SIZE),
                                "max_grid_size": float(config.MAX_GRID_SIZE),
                                "check_order_frequency": float(config.CHECK_ORDER_FREQUENCY),
                                "stagnation_timeout": int(current_stagnation_timeout),
                                "max_order_range": float(current_max_order_range),
                                "sklearn_accuracy": float(
                                    sum(prediction_accuracy["sklearn"]) / max(len(prediction_accuracy["sklearn"]), 1)
                                ),
                                "pytorch_accuracy": float(
                                    sum(prediction_accuracy["pytorch"]) / max(len(prediction_accuracy["pytorch"]), 1)
                                ),
                                "xgboost_accuracy": float(
                                    sum(prediction_accuracy["xgboost"]) / max(len(prediction_accuracy["xgboost"]), 1)
                                ),
                            },
                            {
                                "type": "grid_trade",
                                "timestamp": timestamp.isoformat(),
                                "price": float(ohlcv_df["close"].iloc[-1] if not ohlcv_df.empty else current_price),
                                "volume": float(
                                    ohlcv_df["volume"].iloc[-1]
                                    if not ohlcv_df.empty and "volume" in ohlcv_df.columns
                                    else 0.0
                                ),
                                "trades": int(
                                    ohlcv_df["trades"].iloc[-1]
                                    if not ohlcv_df.empty and "trades" in ohlcv_df.columns
                                    else 0
                                ),
                                "rsi": float(
                                    ohlcv_df["rsi"].iloc[-1]
                                    if not ohlcv_df.empty and "rsi" in ohlcv_df.columns
                                    else 50.0
                                ),
                                "ema": float(
                                    ohlcv_df["ema"].iloc[-1]
                                    if not ohlcv_df.empty and "ema" in ohlcv_df.columns
                                    else current_price
                                ),
                                "volatility": float(
                                    ohlcv_df["volatility"].iloc[-1]
                                    if not ohlcv_df.empty and "volatility" in ohlcv_df.columns
                                    else 0.1
                                ),
                                "macd": float(
                                    ohlcv_df["macd"].iloc[-1]
                                    if not ohlcv_df.empty and "macd" in ohlcv_df.columns
                                    else 0.0
                                ),
                                "macd_signal": float(
                                    ohlcv_df["macd_signal"].iloc[-1]
                                    if not ohlcv_df.empty and "macd_signal" in ohlcv_df.columns
                                    else 0.0
                                ),
                                "bollinger_upper": float(
                                    ohlcv_df["bollinger_upper"].iloc[-1]
                                    if not ohlcv_df.empty and "bollinger_upper" in ohlcv_df.columns
                                    else current_price
                                ),
                                "bollinger_lower": float(
                                    ohlcv_df["bollinger_lower"].iloc[-1]
                                    if not ohlcv_df.empty and "bollinger_lower" in ohlcv_df.columns
                                    else current_price
                                ),
                                "momentum": float(
                                    ohlcv_df["momentum"].iloc[-1]
                                    if not ohlcv_df.empty and "momentum" in ohlcv_df.columns
                                    else 0.0
                                ),
                                "volume_trend": float(
                                    ohlcv_df["volume_trend"].iloc[-1]
                                    if not ohlcv_df.empty and "volume_trend" in ohlcv_df.columns
                                    else 0.0
                                ),
                                "predicted_price": float(
                                    ohlcv_df["predicted_price"].iloc[-1]
                                    if not ohlcv_df.empty and "predicted_price" in ohlcv_df.columns
                                    else current_price
                                ),
                                "grid_level": int(
                                    ohlcv_df["grid_level"].iloc[-1]
                                    if not ohlcv_df.empty and "grid_level" in ohlcv_df.columns
                                    else 0
                                ),
                                "source": "gridbot",
                            },
                        ]
                    )
                    if closed_orders:
                        closed_orders_dict = []
                        for order in closed_orders:
                            if order and isinstance(order, dict):
                                closed_orders_dict.append(order_to_dict(order))
                        orders_to_send.append({"type": "closed_orders", "orders": closed_orders_dict})
                    for order in bot_state["buy_orders"] + bot_state["sell_orders"]:
                        if order and isinstance(order, dict) and "id" in order:
                            orders_to_send.append(order_to_dict(order))
                        else:
                            logger.warning(f"Skipping invalid order for WebSocket send: {order}")
                    if current_time - last_heartbeat > config.HEARTBEAT_INTERVAL:
                        orders_to_send.append({"type": "heartbeat", "timestamp": int(current_time * 1000)})
                        last_heartbeat = current_time
                    logger.info(
                        f"Sending WebSocket messages: predictions={last_sklearn_prediction:.2f}/{last_pytorch_prediction:.2f}/{last_xgb_prediction:.2f}, pl={config.TOTAL_PL:.4f}, orders={len(bot_state['buy_orders']) + len(bot_state['sell_orders'])}, closed_orders={len(closed_orders)}"
                    )
                    websocket_manager.send(json.dumps(orders_to_send))
                except Exception as websocket_error:
                    logger.error(f"Error sending WebSocket updates: {websocket_error}")

                # 9. Process WebSocket Commands
                try:
                    command_processed = websocket_manager.process_command()
                    if command_processed:
                        logger.info("Processed one WebSocket command, checking grid reset")
                    else:
                        logger.debug("No WebSocket commands to process")
                except Exception as command_error:
                    logger.error(f"Error processing WebSocket command: {command_error}")

                """ # 10. Check Grid Stagnation and Rebalance
                try:
                    rebalance_start_time = time.time()
                    logger.info("Starting grid stagnation check and rebalancing process")

                    current_price = get_current_price()
                    time_since_last_trade = current_time - last_trade_time
                    volatility = (
                        ohlcv_df["volatility"].iloc[-1]
                        if not ohlcv_df.empty and "volatility" in ohlcv_df.columns
                        else config.DEFAULT_VOLATILITY
                    )
                    logger.info(f"Volatility calculated: {volatility:.4f}")

                    dynamic_stagnation_timeout = 10800  # Temporary: 30 minutes (1800s) for inactivity trigger
                    total_orders = len(bot_state["buy_orders"]) + len(bot_state["sell_orders"])
                    grid_base_price = bot_state.get("grid_base_price", current_price)
                    price_drift = abs(current_price - grid_base_price)
                    eth_threshold = config.POSITION_SIZE * config.NUM_SELL_GRID_LINES * config.REBALANCE_ETH_THRESHOLD
                    dynamic_price_drift_threshold = (
                        config.GRID_SIZE * config.NUM_BUY_GRID_LINES * config.PRICE_DRIFT_THRESHOLD
                    )
                    min_reset_interval = max(config.MIN_RESET_INTERVAL, dynamic_stagnation_timeout * 0.5)  # Temporary: 900s (1800 * 0.5)

                    logger.info(
                        f"Grid metrics: time_since_last_trade={time_since_last_trade:.0f}s, "
                        f"dynamic_stagnation_timeout={dynamic_stagnation_timeout:.0f}s, "
                        f"total_orders={total_orders}, price_drift={price_drift:.2f}, "
                        f"eth_threshold={eth_threshold:.6f}, price_drift_threshold={dynamic_price_drift_threshold:.2f}"
                    )

                    # Initialize last_reset_grid_size and initial_grid_settings if not set
                    if "last_reset_grid_size" not in bot_state:
                        bot_state["last_reset_grid_size"] = config.GRID_SIZE
                        logger.info(f"Initialized last_reset_grid_size: {config.GRID_SIZE:.2f}")
                    if "initial_grid_settings" not in bot_state or not bot_state["initial_grid_settings"].get("grid_base_price"):
                        bot_state["initial_grid_settings"] = {
                            "grid_base_price": current_price,
                            "grid_size": config.GRID_SIZE,
                            "position_size": config.POSITION_SIZE,
                            "num_buy_grid_lines": config.NUM_BUY_GRID_LINES,
                            "num_sell_grid_lines": config.NUM_SELL_GRID_LINES,
                            "start_time": current_time,
                        }
                        logger.info(
                            f"Stored initial grid settings: base_price={current_price:.2f}, "
                            f"grid_size={config.GRID_SIZE:.2f}, position_size={config.POSITION_SIZE:.6f}, "
                            f"buy_lines={config.NUM_BUY_GRID_LINES}, sell_lines={config.NUM_SELL_GRID_LINES}"
                        )

                    # Compute feature signals for rebalancing
                    logger.info("Computing feature signals for rebalancing")
                    close_prices = ohlcv_df["close"].tail(100)
                    high_prices = ohlcv_df["high"].tail(100)
                    low_prices = ohlcv_df["low"].tail(100)
                    volume = ohlcv_df["volume"].tail(100)
                    feature_data = ohlcv_df.tail(100)

                    macd, macd_signal = compute_macd(close_prices)
                    upper_bb, lower_bb = compute_bollinger(close_prices)
                    momentum = compute_momentum(close_prices)
                    volume_trend = compute_volume_trend(volume)
                    rsi = compute_rsi(close_prices)
                    ema = compute_ema(close_prices)
                    atr = compute_atr(high_prices, low_prices, close_prices)
                    vwap = compute_vwap(feature_data)

                    # Normalize feature signals
                    features = [
                        (macd.iloc[-1] - macd_signal.iloc[-1]) / (macd.std() + 1e-6),  # MACD histogram
                        (close_prices.iloc[-1] - lower_bb.iloc[-1]) / (upper_bb.iloc[-1] - lower_bb.iloc[-1] + 1e-6),  # Bollinger position
                        momentum.iloc[-1] / (momentum.std() + 1e-6),  # Momentum
                        volume_trend.iloc[-1] / (volume_trend.std() + 1e-6),  # Volume Trend
                        (rsi.iloc[-1] - 50) / (rsi.std() + 1e-6),  # RSI centered
                        (close_prices.iloc[-1] - ema.iloc[-1]) / (ema.std() + 1e-6),  # EMA deviation
                        atr.iloc[-1] / (atr.std() + 1e-6),  # ATR
                        (close_prices.iloc[-1] - vwap.iloc[-1]) / (vwap.std() + 1e-6),  # VWAP deviation
                    ]
                    feature_weights = np.array(features) / (np.abs(features).sum() + 1e-6)
                    logger.info(f"Feature weights: {', '.join([f'{f:.4f}' for f in feature_weights])}")

                    # Determine grid lines for feature-based rebalancing
                    base_buy_lines = 20
                    base_sell_lines = 20
                    feature_buy_lines = max(24, (config.NUM_BUY_GRID_LINES - base_buy_lines) // 8)
                    feature_sell_lines = max(24, (config.NUM_SELL_GRID_LINES - base_sell_lines) // 8)
                    num_features = len(features)
                    lines_per_feature = max(8, (feature_buy_lines // num_features) // 2 * 2)

                    logger.info(
                        f"Grid configuration: base_buy={base_buy_lines}, base_sell={base_sell_lines}, "
                        f"feature_buy={feature_buy_lines}, feature_sell={feature_sell_lines}, "
                        f"lines_per_feature={lines_per_feature}"
                    )

                    # Stabilization logic based on initial grid
                    initial_grid = bot_state["initial_grid_settings"]
                    time_since_start = current_time - initial_grid["start_time"]
                    price_drift_from_initial = abs(current_price - initial_grid["grid_base_price"])
                    stabilization_threshold = 350.0  # Temporary: $250 price swing threshold

                    logger.info(
                        f"Stabilization metrics: time_since_start={time_since_start:.0f}s, "
                        f"initial_price_drift={price_drift_from_initial:.2f}, stabilization_threshold={stabilization_threshold:.2f}, "
                        f"initial_grid_base_price={initial_grid['grid_base_price']:.2f}"
                    )

                    reset_triggers = []
                    if (
                        bot_state["needs_reset"]
                        and (current_time - last_reset_time) > min_reset_interval
                        and time_since_start > 10800  # Temporary: 3 hour (10800s)
                    ):
                        reset_triggers.append("needs_reset=True")
                    if (
                        time_since_last_trade > dynamic_stagnation_timeout
                        and (current_time - last_reset_time) > min_reset_interval
                        and time_since_start > 10800  # Temporary: 2 hour (7200s)
                    ):
                        reset_triggers.append(
                            f"stagnation={time_since_last_trade/60:.1f}m > timeout={dynamic_stagnation_timeout/60:.1f}m"
                        )
                    if (
                        total_orders > config.MAX_NUM_GRID_LINES
                        and eth_balance >= eth_threshold
                        and time_since_start > config.TIMESTAMP_VALIDATION
                    ):
                        reset_triggers.append(f"total_orders={total_orders} > {config.MAX_NUM_GRID_LINES}")
                    if (
                        eth_balance < eth_threshold
                        and time_since_start > config.TIMESTAMP_VALIDATION
                    ):
                        reset_triggers.append(f"eth_balance={eth_balance:.6f} < eth_threshold={eth_threshold:.6f}")
                    if (
                        price_drift_from_initial > stabilization_threshold
                        and time_since_start > 7200  # Temporary: 2 hour (7200s)
                        and total_orders > config.MAX_NUM_GRID_LINES
                    ):
                        reset_triggers.append(
                            f"initial_price_drift={price_drift_from_initial:.2f} > stabilization_threshold={stabilization_threshold:.2f}"
                        )

                    if reset_triggers:
                        logger.info(
                            f"Resetting grid: triggers=[{', '.join(reset_triggers)}], "
                            f"stagnation={time_since_last_trade / 60:.1f} minutes, "
                            f"dynamic_timeout={dynamic_stagnation_timeout / 60:.1f} minutes, "
                            f"total_orders={total_orders}, price_drift={price_drift:.2f}, "
                            f"eth_balance={eth_balance:.6f}, eth_threshold={eth_threshold:.6f}"
                        )

                        eth_balance, usd_balance = sync_balances(exchange)
                        logger.info(
                            f"Pre-reset balances: ETH={eth_balance:.6f}, USD={usd_balance:.2f}, "
                            f"Locked ETH={bot_state.get('locked_eth', 0.0):.6f}, Locked USD={bot_state.get('locked_usd', 0.0):.2f}"
                        )

                        # Cancel existing orders
                        cancel_start_time = time.time()
                        all_orders = bot_state["buy_orders"] + bot_state["sell_orders"]
                        cancelled = []
                        if all_orders:
                            logger.info(f"Attempting to cancel {len(all_orders)} orders")
                            for attempt in range(config.CANCEL_ORDER_RETRIES):
                                try:
                                    if hasattr(exchange, "cancel_orders"):
                                        cancelled = exchange.cancel_orders(
                                            [order["id"] for order in all_orders if order and "id" in order],
                                            config.SYMBOL,
                                        )
                                        for order_id in cancelled:
                                            logger.info(f"Successfully cancelled order {order_id}")
                                        break
                                    else:
                                        cancelled = cancel_orders(exchange, all_orders, config.SYMBOL)
                                        for cancelled_order in cancelled:
                                            logger.info(f"Successfully cancelled order {cancelled_order['id']}")
                                        break
                                except Exception as cancel_error:
                                    logger.error(
                                        f"Error cancelling orders (attempt {attempt + 1}/{config.CANCEL_ORDER_RETRIES}): {cancel_error}"
                                    )
                                    if attempt + 1 == config.CANCEL_ORDER_RETRIES:
                                        logger.error("Max cancellation retries reached, proceeding with reset")
                                    time.sleep(exchange.rateLimit / 1000)
                        else:
                            logger.info("No active orders to cancel, skipping batch cancellation")
                        logger.info(f"Order cancellation completed in {time.time() - cancel_start_time:.2f} seconds")

                        bot_state["buy_orders"].clear()
                        bot_state["sell_orders"].clear()
                        buy_prices.clear()
                        logger.info("Cleared buy and sell order lists and buy_prices")

                        eth_balance, usd_balance = sync_balances(exchange)
                        locked_eth = bot_state.get("locked_eth", 0.0)
                        locked_usd = bot_state.get("locked_usd", 0.0)
                        if locked_eth > config.REBALANCE_ETH_THRESHOLD or locked_usd > config.MIN_USD_BALANCE:
                            logger.warning(
                                f"Locked funds remain after cancellation: ETH={locked_eth:.6f}, USD={locked_usd:.2f}"
                            )
                        logger.info(f"Post-cancellation balances: ETH={eth_balance:.6f}, USD={usd_balance:.2f}")

                        # Update grid base price
                        try:
                            ticker = exchange.fetch_ticker(config.SYMBOL)
                            grid_base_price = float(ticker["last"])
                            logger.info(f"Grid base price updated to {grid_base_price:.2f} during reset")
                        except Exception as price_error:
                            logger.error(f"Error updating grid base price: {price_error}")
                            grid_base_price = current_price
                            logger.info(f"Fallback to current_price: {grid_base_price:.2f}")

                        # Adjust grid parameters
                        config.GRID_SIZE = max(
                            config.MIN_GRID_SIZE,
                            min(config.MAX_GRID_SIZE, config.GRID_SIZE),
                        )
                        config.POSITION_SIZE = max(
                            config.MIN_POSITION_SIZE,
                            min(config.MAX_POSITION_SIZE, config.POSITION_SIZE),
                        )
                        config.NUM_BUY_GRID_LINES = max(
                            config.MIN_NUM_GRID_LINES,
                            min(config.MAX_NUM_GRID_LINES, config.NUM_BUY_GRID_LINES),
                        )
                        config.NUM_SELL_GRID_LINES = max(
                            config.MIN_NUM_GRID_LINES,
                            min(config.MAX_NUM_GRID_LINES, config.NUM_SELL_GRID_LINES),
                        )
                        logger.info(
                            f"Grid parameters adjusted: GRID_SIZE={config.GRID_SIZE:.2f}, "
                            f"POSITION_SIZE={config.POSITION_SIZE:.6f}, "
                            f"NUM_BUY_GRID_LINES={config.NUM_BUY_GRID_LINES}, "
                            f"NUM_SELL_GRID_LINES={config.NUM_SELL_GRID_LINES}"
                        )

                        # Replenish ETH if needed
                        replenish_start_time = time.time()
                        success = replenish_eth(
                            exchange,
                            config.TARGET_ETH_BUFFER * config.POSITION_SIZE * config.NUM_SELL_GRID_LINES,
                        )
                        if success:
                            eth_balance, usd_balance = sync_balances(exchange)
                            logger.info(
                                f"ETH replenishment successful: ETH={eth_balance:.6f}, USD={usd_balance:.2f}, "
                                f"Target ETH={config.TARGET_ETH_BUFFER * config.POSITION_SIZE * config.NUM_SELL_GRID_LINES:.6f}"
                            )
                        else:
                            logger.info(
                                f"ETH replenishment not needed or failed: ETH={eth_balance:.6f}, Threshold={config.REPLENISH_ETH_THRESHOLD:.6f}"
                            )
                        logger.info(f"ETH replenishment completed in {time.time() - replenish_start_time:.2f} seconds")

                        # Check available funds
                        required_usd = config.POSITION_SIZE * config.NUM_BUY_GRID_LINES * grid_base_price
                        required_eth = config.POSITION_SIZE * config.NUM_SELL_GRID_LINES
                        if usd_balance < required_usd * 0.8 or eth_balance < required_eth * 0.8:
                            logger.warning(
                                f"Insufficient funds for full grid: USD={usd_balance:.2f} < {required_usd:.2f}, ETH={eth_balance:.6f} < {required_eth:.6f}"
                            )
                            max_buy_lines = max(
                                config.MIN_NUM_GRID_LINES,
                                int(usd_balance / (config.POSITION_SIZE * grid_base_price)),
                            )
                            max_sell_lines = max(
                                config.MIN_NUM_GRID_LINES,
                                int(eth_balance / config.POSITION_SIZE),
                            )
                            config.NUM_BUY_GRID_LINES = min(max_buy_lines, config.NUM_BUY_GRID_LINES)
                            config.NUM_SELL_GRID_LINES = min(max_sell_lines, config.NUM_SELL_GRID_LINES)
                            logger.info(
                                f"Adjusted grid due to insufficient funds: NUM_BUY_GRID_LINES={config.NUM_BUY_GRID_LINES}, "
                                f"NUM_SELL_GRID_LINES={config.NUM_SELL_GRID_LINES}"
                            )
                            if (
                                config.NUM_BUY_GRID_LINES < config.MIN_NUM_GRID_LINES
                                or config.NUM_SELL_GRID_LINES < config.MIN_NUM_GRID_LINES
                            ):
                                logger.error("Grid lines too low, pausing bot")
                            #    bot_state["paused"] = True
                                return

                        # Place orders
                        order_placement_start_time = time.time()
                        orders_to_send = []
                        buy_order_ids = []
                        logger.info("Placing base buy orders")
                        for grid_index in range(base_buy_lines):
                            price = grid_base_price - (config.GRID_SIZE * (grid_index + 1))
                            price = float(exchange.price_to_precision(config.SYMBOL, price))
                            if (grid_base_price - price) <= config.MAX_ORDER_RANGE:
                                amount = exchange.amount_to_precision(config.SYMBOL, config.POSITION_SIZE)
                                cost = float(amount) * price
                                if usd_balance >= cost:
                                    try:
                                        order = exchange.create_limit_buy_order(
                                            config.SYMBOL,
                                            amount,
                                            price,
                                            params={"post_only": True},
                                        )
                                        fetched_order = exchange.fetch_order(order["id"], config.SYMBOL)
                                        order_price = float(fetched_order["price"]) if fetched_order["price"] else price
                                        if fetched_order["status"] == "open":
                                            bot_state["buy_orders"].append(fetched_order)
                                            orders_to_send.append(
                                                {
                                                    "type": "order",
                                                    "id": fetched_order["id"],
                                                    "status": fetched_order["status"],
                                                    "side": fetched_order["side"],
                                                    "price": float(order_price),
                                                    "timestamp": int(time.time() * 1000),
                                                }
                                            )
                                            buy_order_ids.append(fetched_order["id"])
                                            logger.info(
                                                f"Base buy order placed: id={fetched_order['id']}, price={price:.2f}, amount={amount}, cost={cost:.2f}"
                                            )
                                        else:
                                            logger.warning(
                                                f"Base buy order failed: id={fetched_order['id']}, status={fetched_order['status']}, price={price:.2f}"
                                            )
                                    except Exception as buy_order_error:
                                        logger.error(f"Failed to place base buy order at {price:.2f}: {buy_order_error}")
                                else:
                                    logger.warning(
                                        f"Insufficient USD for base buy order: available={usd_balance:.2f}, required={cost:.2f}, price={price:.2f}"
                                    )
                            time.sleep(exchange.rateLimit / 1000)
                        logger.info(f"Placed {len(bot_state['buy_orders'])}/{base_buy_lines} base buy orders")

                        # Place feature-based buy orders with post-only compliance and cap of 8 per feature
                        logger.info("Placing feature-based buy orders (strategy-driven, capped at 8 per feature, post-only compliant)")
                        feature_buy_caps = {"rsi": 8, "bollinger": 8, "macd": 8}
                        open_feature_buy_counts = {k: 0 for k in feature_buy_caps}
                        for order in bot_state["buy_orders"]:
                            feature = order.get("feature")
                            if feature in open_feature_buy_counts:
                                open_feature_buy_counts[feature] += 1

                        # Fetch order book for post-only compliance
                        try:
                            orderbook = exchange.fetch_order_book(config.SYMBOL)
                            best_bid = float(orderbook['bids'][0][0]) if orderbook['bids'] else None
                            best_ask = float(orderbook['asks'][0][0]) if orderbook['asks'] else None
                            min_tick = getattr(exchange, 'markets', {}).get(config.SYMBOL, {}).get('precision', {}).get('price', 0.01)
                            min_tick = float(min_tick) if min_tick else 0.01
                        except Exception as ob_err:
                            logger.warning(f"Could not fetch order book for post-only compliance: {ob_err}")
                            best_bid = best_ask = None
                            min_tick = 0.01

                        # RSI
                        logger.info(f"[RSI CHECK] Current RSI: {rsi.iloc[-1]:.2f}, Buy if < 30")
                        if rsi.iloc[-1] < 30 and open_feature_buy_counts["rsi"] < feature_buy_caps["rsi"] and best_ask:
                            retry_count = 0
                            max_retries = 5
                            while retry_count < max_retries:
                                try:
                                    orderbook = exchange.fetch_order_book(config.SYMBOL)
                                    best_bid = float(orderbook['bids'][0][0]) if orderbook['bids'] else None
                                    best_ask = float(orderbook['asks'][0][0]) if orderbook['asks'] else None
                                    if best_bid is not None and best_ask is not None:
                                        mid_price = (best_bid + best_ask) / 2
                                        price = float(exchange.price_to_precision(config.SYMBOL, min(mid_price - min_tick, best_ask - min_tick)))
                                    amount = exchange.amount_to_precision(config.SYMBOL, config.POSITION_SIZE)
                                    cost = float(amount) * price
                                    if best_ask is not None and price < best_ask and usd_balance >= cost:
                                        order = exchange.create_limit_buy_order(
                                            config.SYMBOL, amount, price, params={"post_only": True}
                                        )
                                        fetched_order = exchange.fetch_order(order["id"], config.SYMBOL)
                                        if fetched_order["status"] == "open":
                                            fetched_order["feature"] = "rsi"
                                            bot_state["buy_orders"].append(fetched_order)
                                            orders_to_send.append({
                                                "type": "order",
                                                "id": fetched_order["id"],
                                                "status": fetched_order["status"],
                                                "side": fetched_order["side"],
                                                "price": float(fetched_order["price"]),
                                                "timestamp": int(time.time() * 1000),
                                                "feature": "rsi"
                                            })
                                            buy_order_ids.append(fetched_order["id"])
                                            logger.info(f"RSI buy order placed: id={fetched_order['id']}, price={price:.2f}, amount={amount}")
                                            break
                                    else:
                                        logger.warning(f"[RSI] Buy price {price} not below best ask {best_ask}, retrying...")
                                        time.sleep(5)
                                except Exception as e:
                                    if "INVALID_LIMIT_PRICE_POST_ONLY" in str(e):
                                        retry_count += 1
                                        logger.warning(f"[RSI] Buy order post-only error, retrying with lower price: {price:.2f} (attempt {retry_count}/{max_retries})")
                                        time.sleep(5)
                                    else:
                                        logger.error(f"Failed to place RSI buy order: {e}")
                                        break
                            else:
                                logger.warning(f"[RSI] Skipped buy: could not place post-only order after {max_retries} retries. Last attempted price: {price}")

                        # Bollinger
                        logger.info(f"[BOLLINGER CHECK] Current Price: {close_prices.iloc[-1]:.2f}, Lower Band: {lower_bb.iloc[-1]:.2f}, Upper Band: {upper_bb.iloc[-1]:.2f}")
                        if close_prices.iloc[-1] <= lower_bb.iloc[-1] and open_feature_buy_counts["bollinger"] < feature_buy_caps["bollinger"] and best_ask:
                            retry_count = 0
                            max_retries = 5
                            while retry_count < max_retries:
                                try:
                                    orderbook = exchange.fetch_order_book(config.SYMBOL)
                                    best_bid = float(orderbook['bids'][0][0]) if orderbook['bids'] else None
                                    best_ask = float(orderbook['asks'][0][0]) if orderbook['asks'] else None
                                    if best_bid is not None and best_ask is not None:
                                        mid_price = (best_bid + best_ask) / 2
                                        price = float(exchange.price_to_precision(config.SYMBOL, min(mid_price - min_tick, best_ask - min_tick, lower_bb.iloc[-1])))
                                    amount = exchange.amount_to_precision(config.SYMBOL, config.POSITION_SIZE)
                                    cost = float(amount) * price
                                    if best_ask is not None and price < best_ask and usd_balance >= cost:
                                        order = exchange.create_limit_buy_order(
                                            config.SYMBOL, amount, price, params={"post_only": True}
                                        )
                                        fetched_order = exchange.fetch_order(order["id"], config.SYMBOL)
                                        if fetched_order["status"] == "open":
                                            fetched_order["feature"] = "bollinger"
                                            bot_state["buy_orders"].append(fetched_order)
                                            orders_to_send.append({
                                                "type": "order",
                                                "id": fetched_order["id"],
                                                "status": fetched_order["status"],
                                                "side": fetched_order["side"],
                                                "price": float(fetched_order["price"]),
                                                "timestamp": int(time.time() * 1000),
                                                "feature": "bollinger"
                                            })
                                            buy_order_ids.append(fetched_order["id"])
                                            logger.info(f"Bollinger Band buy order placed: id={fetched_order['id']}, price={price:.2f}, amount={amount}")
                                            break
                                    else:
                                        logger.warning(f"[BOLLINGER] Buy price {price} not below best ask {best_ask}, retrying...")
                                        time.sleep(5)
                                except Exception as e:
                                    if "INVALID_LIMIT_PRICE_POST_ONLY" in str(e):
                                        retry_count += 1
                                        logger.warning(f"[BOLLINGER] Buy order post-only error, retrying with lower price: {price:.2f} (attempt {retry_count}/{max_retries})")
                                        time.sleep(5)
                                    else:
                                        logger.error(f"Failed to place Bollinger Band buy order: {e}")
                                        break
                            else:
                                logger.warning(f"[BOLLINGER] Skipped buy: could not place post-only order after {max_retries} retries. Last attempted price: {price}")

                        # MACD
                        logger.info(f"[MACD CHECK] Current MACD: {macd.iloc[-1]:.4f}, Signal: {macd_signal.iloc[-1]:.4f}, Buy if MACD > Signal")
                        if macd.iloc[-2] < macd_signal.iloc[-2] and macd.iloc[-1] > macd_signal.iloc[-1] and open_feature_buy_counts["macd"] < feature_buy_caps["macd"] and best_ask:
                            retry_count = 0
                            max_retries = 5
                            while retry_count < max_retries:
                                try:
                                    orderbook = exchange.fetch_order_book(config.SYMBOL)
                                    best_bid = float(orderbook['bids'][0][0]) if orderbook['bids'] else None
                                    best_ask = float(orderbook['asks'][0][0]) if orderbook['asks'] else None
                                    if best_bid is not None and best_ask is not None:
                                        mid_price = (best_bid + best_ask) / 2
                                        price = float(exchange.price_to_precision(config.SYMBOL, min(mid_price - min_tick, best_ask - min_tick, grid_base_price - config.GRID_SIZE * 0.5)))
                                    amount = exchange.amount_to_precision(config.SYMBOL, config.POSITION_SIZE)
                                    cost = float(amount) * price
                                    if best_ask is not None and price < best_ask and usd_balance >= cost:
                                        order = exchange.create_limit_buy_order(
                                            config.SYMBOL, amount, price, params={"post_only": True}
                                        )
                                        fetched_order = exchange.fetch_order(order["id"], config.SYMBOL)
                                        if fetched_order["status"] == "open":
                                            fetched_order["feature"] = "macd"
                                            bot_state["buy_orders"].append(fetched_order)
                                            orders_to_send.append({
                                                "type": "order",
                                                "id": fetched_order["id"],
                                                "status": fetched_order["status"],
                                                "side": fetched_order["side"],
                                                "price": float(fetched_order["price"]),
                                                "timestamp": int(time.time() * 1000),
                                                "feature": "macd"
                                            })
                                            buy_order_ids.append(fetched_order["id"])
                                            logger.info(f"MACD buy order placed: id={fetched_order['id']}, price={price:.2f}, amount={amount}")
                                            break
                                    else:
                                        logger.warning(f"[MACD] Buy price {price} not below best ask {best_ask}, retrying...")
                                        time.sleep(5)
                                except Exception as e:
                                    if "INVALID_LIMIT_PRICE_POST_ONLY" in str(e):
                                        retry_count += 1
                                        logger.warning(f"[MACD] Buy order post-only error, retrying with lower price: {price:.2f} (attempt {retry_count}/{max_retries})")
                                        time.sleep(5)
                                    else:
                                        logger.error(f"Failed to place MACD buy order: {e}")
                                        break
                            else:
                                logger.warning(f"[MACD] Skipped buy: could not place post-only order after {max_retries} retries. Last attempted price: {price}")

                        eth_balance, _ = sync_balances(exchange)
                        logger.info(f"ETH balance before sell orders: {eth_balance:.6f}")

                        sell_order_ids = []
                        logger.info("Placing base sell orders")
                        for grid_index in range(base_sell_lines):
                            price = grid_base_price + (config.GRID_SIZE * (grid_index + 1))
                            price = float(exchange.price_to_precision(config.SYMBOL, price))
                            if (price - grid_base_price) <= config.MAX_ORDER_RANGE:
                                amount = min(
                                    eth_balance / max(1, base_sell_lines - grid_index),
                                    config.POSITION_SIZE * config.SELL_SIZE_MULTIPLIER,
                                )
                                amount = float(exchange.amount_to_precision(config.SYMBOL, amount))
                                if eth_balance >= amount and amount >= config.MIN_POSITION_SIZE:
                                    try:
                                        order = exchange.create_limit_sell_order(
                                            config.SYMBOL,
                                            amount,
                                            price,
                                            params={"post_only": True},
                                        )
                                        fetched_order = exchange.fetch_order(order["id"], config.SYMBOL)
                                        order_price = float(fetched_order["price"]) if fetched_order["price"] else price
                                        if fetched_order["status"] == "open":
                                            bot_state["sell_orders"].append(fetched_order)
                                            orders_to_send.append(
                                                {
                                                    "type": "order",
                                                    "id": fetched_order["id"],
                                                    "status": fetched_order["status"],
                                                    "side": fetched_order["side"],
                                                    "price": float(order_price),
                                                    "timestamp": int(time.time() * 1000),
                                                }
                                            )
                                            sell_order_ids.append(fetched_order["id"])
                                            eth_balance -= amount
                                            buy_prices[fetched_order["id"]] = grid_base_price
                                            logger.info(
                                                f"Base sell order placed: id={fetched_order['id']}, price={price:.2f}, amount={amount}"
                                            )
                                        else:
                                            logger.warning(
                                                f"Base sell order failed: id={fetched_order['id']}, status={fetched_order['status']}, price={price:.2f}"
                                            )
                                    except Exception as sell_order_error:
                                        logger.error(f"Failed to place base sell order at {price:.2f}: {sell_order_error}")
                                else:
                                    logger.warning(
                                        f"Insufficient ETH for base sell order: available={eth_balance:.6f}, required={amount:.6f}, price={price:.2f}"
                                    )
                            time.sleep(exchange.rateLimit / 1000)
                        logger.info(f"Placed {len(bot_state['sell_orders'])}/{base_sell_lines} base sell orders")

                        # Place feature-based sell orders with cap of 8 per feature
                        logger.info("Placing feature-based sell orders (strategy-driven, capped at 2 per feature)")
                        feature_sell_caps = {"rsi": 8, "bollinger": 8, "macd": 8}
                        # Count current open sell orders per feature
                        open_feature_sell_counts = {k: 0 for k in feature_sell_caps}
                        for order in bot_state["sell_orders"]:
                            feature = order.get("feature")
                            if feature in open_feature_sell_counts:
                                open_feature_sell_counts[feature] += 1

                        # RSI
                        logger.info(f"[RSI CHECK] Current RSI: {rsi.iloc[-1]:.2f}, Sell if > 70")
                        if rsi.iloc[-1] > 70 and open_feature_sell_counts["rsi"] < feature_sell_caps["rsi"] and best_bid:
                            retry_count = 0
                            max_retries = 5
                            while retry_count < max_retries:
                                price = float(exchange.price_to_precision(config.SYMBOL, max(grid_base_price + config.GRID_SIZE, best_bid + min_tick)))
                                amount = min(
                                    eth_balance,
                                    config.POSITION_SIZE * config.SELL_SIZE_MULTIPLIER,
                                )
                                amount = float(exchange.amount_to_precision(config.SYMBOL, amount))
                                if eth_balance >= amount and amount >= config.MIN_POSITION_SIZE and price >= best_bid + min_tick:
                                    try:
                                        order = exchange.create_limit_sell_order(
                                            config.SYMBOL, amount, price, params={"post_only": True}
                                        )
                                        fetched_order = exchange.fetch_order(order["id"], config.SYMBOL)
                                        if fetched_order["status"] == "open":
                                            fetched_order["feature"] = "rsi"
                                            bot_state["sell_orders"].append(fetched_order)
                                            orders_to_send.append({
                                                "type": "order",
                                                "id": fetched_order["id"],
                                                "status": fetched_order["status"],
                                                "side": fetched_order["side"],
                                                "price": float(fetched_order["price"]),
                                                "timestamp": int(time.time() * 1000),
                                                "feature": "rsi"
                                            })
                                            sell_order_ids.append(fetched_order["id"])
                                            eth_balance -= amount
                                            buy_prices[fetched_order["id"]] = grid_base_price
                                            logger.info(f"RSI sell order placed: id={fetched_order['id']}, price={price:.2f}, amount={amount}")
                                            break
                                    except Exception as e:
                                        if "INVALID_LIMIT_PRICE_POST_ONLY" in str(e):
                                            retry_count += 1
                                            price = float(exchange.price_to_precision(config.SYMBOL, price + min_tick))
                                            logger.warning(f"[RSI] Sell order post-only error, retrying with higher price: {price:.2f} (attempt {retry_count}/{max_retries})")
                                            time.sleep(5)
                                        else:
                                            logger.error(f"Failed to place RSI sell order: {e}")
                                            break
                                else:
                                    logger.warning(f"[RSI] Skipped sell: insufficient ETH or below min size or price not above best bid.")
                                    time.sleep(5)
                                    break
                            else:
                                logger.warning(f"[RSI] Skipped sell: could not place post-only order after {max_retries} retries. Last attempted price: {price}")

                        # Bollinger
                        logger.info(f"[BOLLINGER CHECK] Current Price: {close_prices.iloc[-1]:.2f}, Lower Band: {lower_bb.iloc[-1]:.2f}, Upper Band: {upper_bb.iloc[-1]:.2f}")
                        if close_prices.iloc[-1] >= upper_bb.iloc[-1] and open_feature_sell_counts["bollinger"] < feature_sell_caps["bollinger"] and best_bid:
                            retry_count = 0
                            max_retries = 5
                            while retry_count < max_retries:
                                price = float(exchange.price_to_precision(config.SYMBOL, max(upper_bb.iloc[-1], best_bid + min_tick)))
                                amount = min(
                                    eth_balance,
                                    config.POSITION_SIZE * config.SELL_SIZE_MULTIPLIER,
                                )
                                amount = float(exchange.amount_to_precision(config.SYMBOL, amount))
                                if eth_balance >= amount and amount >= config.MIN_POSITION_SIZE and price >= best_bid + min_tick:
                                    try:
                                        order = exchange.create_limit_sell_order(
                                            config.SYMBOL, amount, price, params={"post_only": True}
                                        )
                                        fetched_order = exchange.fetch_order(order["id"], config.SYMBOL)
                                        if fetched_order["status"] == "open":
                                            fetched_order["feature"] = "bollinger"
                                            bot_state["sell_orders"].append(fetched_order)
                                            orders_to_send.append({
                                                "type": "order",
                                                "id": fetched_order["id"],
                                                "status": fetched_order["status"],
                                                "side": fetched_order["side"],
                                                "price": float(fetched_order["price"]),
                                                "timestamp": int(time.time() * 1000),
                                                "feature": "bollinger"
                                            })
                                            sell_order_ids.append(fetched_order["id"])
                                            eth_balance -= amount
                                            buy_prices[fetched_order["id"]] = grid_base_price
                                            logger.info(f"Bollinger Band sell order placed: id={fetched_order['id']}, price={price:.2f}, amount={amount}")
                                            break
                                    except Exception as e:
                                        if "INVALID_LIMIT_PRICE_POST_ONLY" in str(e):
                                            retry_count += 1
                                            price = float(exchange.price_to_precision(config.SYMBOL, price + min_tick))
                                            logger.warning(f"[BOLLINGER] Sell order post-only error, retrying with higher price: {price:.2f} (attempt {retry_count}/{max_retries})")
                                            time.sleep(5)
                                        else:
                                            logger.error(f"Failed to place Bollinger Band sell order: {e}")
                                            break
                                else:
                                    logger.warning(f"[BOLLINGER] Skipped sell: insufficient ETH or below min size or price not above best bid.")
                                    time.sleep(5)
                                    break
                            else:
                                logger.warning(f"[BOLLINGER] Skipped sell: could not place post-only order after {max_retries} retries. Last attempted price: {price}")

                        # MACD
                        logger.info(f"[MACD CHECK] Current MACD: {macd.iloc[-1]:.4f}, Signal: {macd_signal.iloc[-1]:.4f}, Sell if MACD < Signal")
                        if macd.iloc[-2] > macd_signal.iloc[-2] and macd.iloc[-1] < macd_signal.iloc[-1] and open_feature_sell_counts["macd"] < feature_sell_caps["macd"] and best_bid:
                            retry_count = 0
                            max_retries = 5
                            while retry_count < max_retries:
                                price = float(exchange.price_to_precision(config.SYMBOL, max(grid_base_price + config.GRID_SIZE * 0.5, best_bid + min_tick)))
                                amount = min(
                                    eth_balance,
                                    config.POSITION_SIZE * config.SELL_SIZE_MULTIPLIER,
                                )
                                amount = float(exchange.amount_to_precision(config.SYMBOL, amount))
                                if eth_balance >= amount and amount >= config.MIN_POSITION_SIZE and price >= best_bid + min_tick:
                                    try:
                                        order = exchange.create_limit_sell_order(
                                            config.SYMBOL, amount, price, params={"post_only": True}
                                        )
                                        fetched_order = exchange.fetch_order(order["id"], config.SYMBOL)
                                        if fetched_order["status"] == "open":
                                            fetched_order["feature"] = "macd"
                                            bot_state["sell_orders"].append(fetched_order)
                                            orders_to_send.append({
                                                "type": "order",
                                                "id": fetched_order["id"],
                                                "status": fetched_order["status"],
                                                "side": fetched_order["side"],
                                                "price": float(fetched_order["price"]),
                                                "timestamp": int(time.time() * 1000),
                                                "feature": "macd"
                                            })
                                            sell_order_ids.append(fetched_order["id"])
                                            eth_balance -= amount
                                            buy_prices[fetched_order["id"]] = grid_base_price
                                            logger.info(f"MACD sell order placed: id={fetched_order['id']}, price={price:.2f}, amount={amount}")
                                            break
                                    except Exception as e:
                                        if "INVALID_LIMIT_PRICE_POST_ONLY" in str(e):
                                            retry_count += 1
                                            price = float(exchange.price_to_precision(config.SYMBOL, price + min_tick))
                                            logger.warning(f"[MACD] Sell order post-only error, retrying with higher price: {price:.2f} (attempt {retry_count}/{max_retries})")
                                            time.sleep(5)
                                        else:
                                            logger.error(f"Failed to place MACD sell order: {e}")
                                            break
                                else:
                                    logger.warning(f"[MACD] Skipped sell: insufficient ETH or below min size or price not above best bid.")
                                    time.sleep(5)
                                    break
                            else:
                                logger.warning(f"[MACD] Skipped sell: could not place post-only order after {max_retries} retries. Last attempted price: {price}")

                        logger.info(f"Order placement completed in {time.time() - order_placement_start_time:.2f} seconds")

                        # Send orders to WebSocket
                        try:
                            websocket_manager.send(json.dumps(orders_to_send))
                            logger.info(f"Sent {len(orders_to_send)} orders to WebSocket during rebalance")
                            logger.info(
                                f"Grid rebalanced: {len(bot_state['buy_orders'])} buy orders, {len(bot_state['sell_orders'])} sell orders"
                            )
                            logger.info(
                                f"Post-reset balances: ETH={eth_balance:.6f}, USD={usd_balance:.2f}, "
                                f"Active buy orders={len(buy_order_ids)}, Active sell orders={len(sell_order_ids)}"
                            )
                        except Exception as websocket_error:
                            logger.error(f"Error sending WebSocket orders during rebalance: {websocket_error}")

                        expected_orders = config.NUM_BUY_GRID_LINES + config.NUM_SELL_GRID_LINES
                        actual_orders = len(bot_state["buy_orders"]) + len(bot_state["sell_orders"])
                        if actual_orders < expected_orders * 0.8:
                            logger.error(
                                f"Incomplete rebalance: {actual_orders}/{expected_orders} orders placed, pausing bot"
                            )
                        #    bot_state["paused"] = True
                            return

                        # Update initial_grid_settings post-reset
                        bot_state["initial_grid_settings"] = {
                            "grid_base_price": current_price,
                            "grid_size": config.GRID_SIZE,
                            "position_size": config.POSITION_SIZE,
                            "num_buy_grid_lines": config.NUM_BUY_GRID_LINES,
                            "num_sell_grid_lines": config.NUM_SELL_GRID_LINES,
                            "start_time": current_time,
                        }
                        logger.info(
                            f"Updated grid settings: base_price={current_price:.2f}, "
                            f"grid_size={config.GRID_SIZE:.2f}, position_size={config.POSITION_SIZE:.6f}, "
                            f"buy_lines={config.NUM_BUY_GRID_LINES}, sell_lines={config.NUM_SELL_GRID_LINES}"
                        )

                        bot_state["needs_reset"] = False
                        last_reset_time = current_time
                        last_trade_time = current_time
                        bot_state["last_reset_grid_size"] = config.GRID_SIZE
                        logger.info(
                            f"Rebalance completed: last_reset_time={last_reset_time:.0f}, "
                            f"last_reset_grid_size={bot_state['last_reset_grid_size']:.2f}"
                        )
                    else:
                        logger.info("No reset needed at this time")

                    logger.info(f"Total rebalancing time: {time.time() - rebalance_start_time:.2f} seconds")
                except Exception as rebalance_error:
                    logger.error(f"Error checking grid stagnation: {rebalance_error}") """

                # 11. Update Bot State
                bot_state.update(
                    {
                        "current_price": current_price,
                        "total_pl": bot_state["total_pl"],
                        "last_sklearn_prediction": last_sklearn_prediction,
                        "last_pytorch_prediction": last_pytorch_prediction,
                        "last_xgb_prediction": last_xgb_prediction,
                        "eth_balance": eth_balance,
                        "usd_balance": usd_balance,
                    }
                )

                # Clean up trade_counts with type conversion
                cutoff = pd.Timestamp.now(tz="UTC") - pd.Timedelta(minutes=lookback)
                trade_counts = {
                    k: v for k, v in trade_counts.items()
                    if pd.to_datetime(k, errors='coerce', utc=True) > cutoff and not pd.isna(pd.to_datetime(k, errors='coerce', utc=True))
                }

                logger.info(
                    f"Iteration {iteration_count}: Price={bot_state['current_price']:.2f}, "
                    f"Orders={len(bot_state['buy_orders'])} buy/{len(bot_state['sell_orders'])} sell, "
                    f"Feature Cache={len(bot_state['feature_cache'])} rows, "
                    f"Last Retrain={datetime.fromtimestamp(bot_state['last_retrain_time']).strftime('%Y-%m-%d %H:%M:%S') if bot_state['last_retrain_time'] else 'Never'}"
                )

                iteration_count += 1
                time.sleep(config.CHECK_ORDER_FREQUENCY)

            except Exception as loop_error:
                logger.error(f"Main loop error: {loop_error}", exc_info=True)
                time.sleep(config.CHECK_ORDER_FREQUENCY)

    except Exception as main_error:
        logger.error(f"Fatal error: {main_error}", exc_info=True)
        shutdown_event.set()
        raise
    finally:
        logger.info("Shutting down")
        cancel_orders(exchange, bot_state["buy_orders"] + bot_state["sell_orders"], config.SYMBOL)
        websocket_manager.stop()
        coinbase_thread.join()


def safe_request(fn, retries=3, delay=2):
    import time
    for i in range(retries):
        try:
            return fn()
        except Exception as e:
            if i == retries-1: raise
            time.sleep(delay)


if __name__ == "__main__":
    import sys
    # Redirect all stdout and stderr to gridbot_ml.log
    sys.stdout = open("gridbot_ml.log", "a", buffering=1)
    sys.stderr = sys.stdout
    while True:
        try:
            print("Starting bot")
            run_bot()
        except Exception as crash_error:
            print(f"Bot crashed: {crash_error}")
            time.sleep(10)